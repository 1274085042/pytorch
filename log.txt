

>>> Lint for torch/_decomp/decompositions.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int]";
    expected "list[int]"

         908  |
         909  |    # Note that F.pad takes (padding_left, padding_right, padding_top, padding_bottom)
         910  |    # ugh
    >>>  911  |    padded_input = F.pad(input, (padding_w, padding_w, padding_h, padding_h))
         912  |
         913  |    blocks_row_indices = blocks_row_indices.unsqueeze(-1).unsqueeze(-1)
         914  |    output = padded_input[:, :, blocks_row_indices, blocks_col_indices]

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int]";
    expected "list[int]"

        1016  |    )
        1017  |    idx = (None, None, indices_row, indices_col)
        1018  |    output = aten._unsafe_index_put(output, idx, input, accumulate=True)
    >>> 1019  |    output = F.pad(output, (-padding_w, -padding_w, -padding_h, -padding_h))
        1020  |
        1021  |    if not batched_input:
        1022  |        output = output.squeeze(0)

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3828  |    grid_one = torch.ones((1, 1, 1), dtype=dtype, device=device)
        3829  |
        3830  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
    >>> 3831  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 2), mode="constant", value=0)
        3832  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 1), mode="constant", value=0)
        3833  |    grid_one = torch.nn.functional.pad(grid_one, pad=(2, 0), mode="constant", value=0)
        3834  |    return grid_x + grid_y + grid_one

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3829  |
        3830  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3831  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 2), mode="constant", value=0)
    >>> 3832  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 1), mode="constant", value=0)
        3833  |    grid_one = torch.nn.functional.pad(grid_one, pad=(2, 0), mode="constant", value=0)
        3834  |    return grid_x + grid_y + grid_one
        3835  |

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3830  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3831  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 2), mode="constant", value=0)
        3832  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 1), mode="constant", value=0)
    >>> 3833  |    grid_one = torch.nn.functional.pad(grid_one, pad=(2, 0), mode="constant", value=0)
        3834  |    return grid_x + grid_y + grid_one
        3835  |
        3836  |

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3844  |    grid_one = torch.ones((1, 1, 1, 1), dtype=dtype, device=device)
        3845  |
        3846  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
    >>> 3847  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
        3848  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
        3849  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
        3850  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3845  |
        3846  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3847  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
    >>> 3848  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
        3849  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
        3850  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)
        3851  |    return grid_x + grid_y + grid_z + grid_one

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3846  |    # this is just a temporary hack and we should use torch.stack here once #104480 is merged
        3847  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
        3848  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
    >>> 3849  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
        3850  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)
        3851  |    return grid_x + grid_y + grid_z + grid_one
        3852  |

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        3847  |    grid_x = torch.nn.functional.pad(grid_x, pad=(0, 3), mode="constant", value=0)
        3848  |    grid_y = torch.nn.functional.pad(grid_y, pad=(1, 2), mode="constant", value=0)
        3849  |    grid_z = torch.nn.functional.pad(grid_z, pad=(2, 1), mode="constant", value=0)
    >>> 3850  |    grid_one = torch.nn.functional.pad(grid_one, pad=(3, 0), mode="constant", value=0)
        3851  |    return grid_x + grid_y + grid_z + grid_one
        3852  |
        3853  |



>>> Lint for torch/_dynamo/guards.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "RootGuardManager"

         101  |check_type_id = torch._C._dynamo.guards.check_type_id
         102  |dict_version = torch._C._dynamo.guards.dict_version
         103  |
    >>>  104  |RootGuardManager = torch._C._dynamo.guards.RootGuardManager
         105  |DictGuardManager = torch._C._dynamo.guards.DictGuardManager
         106  |install_tensor_aliasing_guard = torch._C._dynamo.guards.install_tensor_aliasing_guard
         107  |install_no_tensor_aliasing_guard = (

  Error (MYPY) [attr-defined]
    Module has no attribute "DictGuardManager"

         102  |dict_version = torch._C._dynamo.guards.dict_version
         103  |
         104  |RootGuardManager = torch._C._dynamo.guards.RootGuardManager
    >>>  105  |DictGuardManager = torch._C._dynamo.guards.DictGuardManager
         106  |install_tensor_aliasing_guard = torch._C._dynamo.guards.install_tensor_aliasing_guard
         107  |install_no_tensor_aliasing_guard = (
         108  |    torch._C._dynamo.guards.install_no_tensor_aliasing_guard

  Error (MYPY) [attr-defined]
    Module has no attribute "install_tensor_aliasing_guard"

         103  |
         104  |RootGuardManager = torch._C._dynamo.guards.RootGuardManager
         105  |DictGuardManager = torch._C._dynamo.guards.DictGuardManager
    >>>  106  |install_tensor_aliasing_guard = torch._C._dynamo.guards.install_tensor_aliasing_guard
         107  |install_no_tensor_aliasing_guard = (
         108  |    torch._C._dynamo.guards.install_no_tensor_aliasing_guard
         109  |)

  Error (MYPY) [attr-defined]
    Module has no attribute "install_no_tensor_aliasing_guard"

         105  |DictGuardManager = torch._C._dynamo.guards.DictGuardManager
         106  |install_tensor_aliasing_guard = torch._C._dynamo.guards.install_tensor_aliasing_guard
         107  |install_no_tensor_aliasing_guard = (
    >>>  108  |    torch._C._dynamo.guards.install_no_tensor_aliasing_guard
         109  |)
         110  |
         111  |



>>> Lint for torch/_inductor/fx_passes/fb/fuse_split_ops.py:

  Error (MYPY) [no-redef]
    Name "total_node_dims" already defined on line 718

        720  |            arg_dim: int = node.kwargs["dim"]  # type: ignore[assignment]
        721  |            return (total_node_dims + arg_dim) % total_node_dims
        722  |        elif is_op(node, torch.ops.aten.chunk):
    >>> 723  |            total_node_dims: int = len(
        724  |                node.all_input_nodes[0].meta["tensor_meta"].shape
        725  |            )
        726  |            # pyre-ignore

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Any, ...] | list[Any] |
    dict[str, Any] | slice | range | Node | str | int | float | bool | complex
    | dtype | Tensor | device | memory_format | layout | OpOverload | None",
    expected "int | None")

        727  |            arg_dim: int = node.kwargs["dim"]  # type: ignore[no-redef]
        728  |            return (total_node_dims + arg_dim) % total_node_dims
        729  |        elif is_op(node, torch.ops.aten.split_with_sizes):
    >>> 730  |            return node.kwargs["dim"]
        731  |    return None
        732  |
        733  |

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "int | None",
    variable has type "int")

        776  |                break
        777  |
        778  |            # pyre-ignore
    >>> 779  |            normalized_real_user_dim: int = get_normalized_dim_kwarg(real_user)
        780  |            if normalized_real_user_dim != normalized_node_dim:
        781  |                can_remove = False
        782  |                break

  Error (MYPY) [union-attr]
    Item "None" of "Node | None" has no attribute "kwargs"

        793  |
        794  |        # chunk has to be in exact order as concat.
        795  |        pos: Optional[int] = None
    >>> 796  |        org_inputs: List[fx.Node] = cat_node.kwargs["tensors"]
        797  |        for i, _ in enumerate(org_inputs):
        798  |            if split_op.getitems == org_inputs[i : i + len(split_op.getitems)]:
        799  |                pos = i

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "tuple[Any, ...] |
    list[Any] | dict[str, Any] | slice | range | Node | str | int | float |
    complex | dtype | Tensor | device | memory_format | layout | OpOverload |
    Any | None", variable has type "list[Node]")

        793  |
        794  |        # chunk has to be in exact order as concat.
        795  |        pos: Optional[int] = None
    >>> 796  |        org_inputs: List[fx.Node] = cat_node.kwargs["tensors"]
        797  |        for i, _ in enumerate(org_inputs):
        798  |            if split_op.getitems == org_inputs[i : i + len(split_op.getitems)]:
        799  |                pos = i

  Error (MYPY) [union-attr]
    Item "None" of "Node | None" has no attribute "kwargs"

        809  |                + [split_op.pre_split_header]
        810  |                + org_inputs[pos + len(split_op.getitems) :]
        811  |            ),
    >>> 812  |            "dim": cat_node.kwargs["dim"],
        813  |        }
        814  |        cat_node.kwargs = kwargs
        815  |        removed = True

  Error (MYPY) [union-attr]
    Item "None" of "Node | None" has no attribute "kwargs"

        811  |            ),
        812  |            "dim": cat_node.kwargs["dim"],
        813  |        }
    >>> 814  |        cat_node.kwargs = kwargs
        815  |        removed = True
        816  |
        817  |    if removed:



>>> Lint for torch/_inductor/fx_passes/fb/remove_reshape.py:

  Error (MYPY) [union-attr]
    Item "tuple[Any, ...]" of "tuple[Any, ...] | Any | list[Any] | dict[str,
    Any] | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "list[Any]" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any]
    | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "dict[str, Any]" of "tuple[Any, ...] | Any | list[Any] | dict[str,
    Any] | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "slice" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "range" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "str" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] | slice
    | range | Node | str | int | float | complex | dtype | Tensor | device |
    memory_format | layout | OpOverload | None" has no attribute "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "int" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] | slice
    | range | Node | str | int | float | complex | dtype | Tensor | device |
    memory_format | layout | OpOverload | None" has no attribute "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "float" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "complex" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any]
    | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "dtype" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "Tensor" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "device" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "memory_format" of "tuple[Any, ...] | Any | list[Any] | dict[str,
    Any] | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "layout" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] |
    slice | range | Node | str | int | float | complex | dtype | Tensor |
    device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "OpOverload" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any]
    | slice | range | Node | str | int | float | complex | dtype | Tensor
    | device | memory_format | layout | OpOverload | None" has no attribute
    "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()

  Error (MYPY) [union-attr]
    Item "None" of "tuple[Any, ...] | Any | list[Any] | dict[str, Any] | slice
    | range | Node | str | int | float | complex | dtype | Tensor | device |
    memory_format | layout | OpOverload | None" has no attribute "format_node"

        131  |                input_node, torch.ops.aten.clone
        132  |            ):
        133  |                input_node = input_node.kwargs["self"]
    >>> 134  |            _LOGGER.info(f"replacing {n.format_node()} with {input_node.format_node()}")
        135  |            n.replace_all_uses_with(input_node)
        136  |
        137  |    graph.eliminate_dead_code()



>>> Lint for torch/_inductor/fx_passes/fb/split_cat_aten.py:

  Error (MYPY) [union-attr]
    Item "str" of "Any | str | int" has no attribute "args"

        1245  |            indices, idx_to_getitem = [], {}
        1246  |            for getitem in cat_user.args[0]:  # type: ignore[union-attr]
        1247  |                indices.append(getitem.args[1])  # type: ignore[union-attr]
    >>> 1248  |                idx_to_getitem[getitem.args[1]] = getitem  # type: ignore[index]
        1249  |            # the gettitems to be merged must be consecutive, otherwise
        1250  |            # returned sliced tensor could be wrong
        1251  |            if not is_sorted_and_consecutive(indices):

  Error (MYPY) [union-attr]
    Item "int" of "Any | str | int" has no attribute "args"

        1245  |            indices, idx_to_getitem = [], {}
        1246  |            for getitem in cat_user.args[0]:  # type: ignore[union-attr]
        1247  |                indices.append(getitem.args[1])  # type: ignore[union-attr]
    >>> 1248  |                idx_to_getitem[getitem.args[1]] = getitem  # type: ignore[index]
        1249  |            # the gettitems to be merged must be consecutive, otherwise
        1250  |            # returned sliced tensor could be wrong
        1251  |            if not is_sorted_and_consecutive(indices):

  Error (MYPY) [arg-type]
    Argument 1 to "len" has incompatible type "tuple[Any, ...] | list[Any] |
    dict[str, Any] | slice | range | Node | str | int | float | bool | complex
    | dtype | Tensor | device | memory_format | layout | OpOverload | None";
    expected "Sized"

        1251  |            if not is_sorted_and_consecutive(indices):
        1252  |                continue
        1253  |            # case 1: the cat uses all getitems from the split
    >>> 1254  |            if len(split_sections) == len(cat_user.args[0]):  # type: ignore[union-attr]
        1255  |                # replace the users of the cat node to be the input of the split node
        1256  |                cat_user.replace_all_uses_with(split_node.args[0])
        1257  |                # remove the cat node

  Error (MYPY) [arg-type]
    Argument 1 to "is_node_meta_valid" has incompatible type "tuple[Any, ...]
    | list[Any] | dict[str, Any] | slice | range | Node | str | int | float
    | bool | complex | dtype | Tensor | device | memory_format | layout |
    OpOverload | None"; expected "Node | None"

        1258  |                graph.erase_node(cat_user)
        1259  |                counters["inductor"]["cat_mutated"] += 1
        1260  |            # case 2: the cat uses some getitems from the split
    >>> 1261  |            elif is_node_meta_valid(split_node.args[0]):  # type: ignore[union-attr]
        1262  |                # check the split dim, and construct the slice tuple
        1263  |                start_fused_size = calculate_fused_tensor_size(
        1264  |                    split_node, list(range(indices[0]))



>>> Lint for torch/_refs/__init__.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "frexp"

        1371  |@register_decomposition(aten.frexp)
        1372  |@out_wrapper("mantissa", "exponent")
        1373  |def frexp(self: TensorLikeType) -> Tuple[TensorLikeType, TensorLikeType]:
    >>> 1374  |    return torch.return_types.frexp(prims.frexp(self))
        1375  |
        1376  |
        1377  |@_make_elementwise_binary_reference(



>>> Lint for torch/_tensor.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "split_with_sizes"

         918  |        if isinstance(split_size, (int, torch.SymInt)):
         919  |            return torch._VF.split(self, split_size, dim)  # type: ignore[attr-defined]
         920  |        else:
    >>>  921  |            return torch._VF.split_with_sizes(self, split_size, dim)
         922  |
         923  |    def unique(self, sorted=True, return_inverse=False, return_counts=False, dim=None):
         924  |        r"""Returns the unique elements of the input tensor.



>>> Lint for torch/ao/nn/quantizable/modules/activation.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        369  |                k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
        370  |                v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
        371  |                if attn_mask is not None:
    >>> 372  |                    attn_mask = nnF.pad(attn_mask, (0, 1))
        373  |                if key_padding_mask is not None:
        374  |                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        375  |            else:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        371  |                if attn_mask is not None:
        372  |                    attn_mask = nnF.pad(attn_mask, (0, 1))
        373  |                if key_padding_mask is not None:
    >>> 374  |                    key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        375  |            else:
        376  |                assert static_k is None, "bias cannot be added to static key."
        377  |                assert static_v is None, "bias cannot be added to static value."

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        413  |            v = torch.cat([v, v_zeros], dim=1)
        414  |
        415  |            if attn_mask is not None:
    >>> 416  |                attn_mask = nnF.pad(attn_mask, (0, 1))
        417  |            if key_padding_mask is not None:
        418  |                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        419  |

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        415  |            if attn_mask is not None:
        416  |                attn_mask = nnF.pad(attn_mask, (0, 1))
        417  |            if key_padding_mask is not None:
    >>> 418  |                key_padding_mask = nnF.pad(key_padding_mask, (0, 1))
        419  |
        420  |        # Leaving the quantized zone here
        421  |        q = self.dequant_q(q)



>>> Lint for torch/ao/nn/quantized/reference/modules/rnn.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh_cell"

        138  |            hx = hx.unsqueeze(0) if not is_batched else hx
        139  |
        140  |        if self.nonlinearity == "tanh":
    >>> 141  |            ret = _VF.rnn_tanh_cell(
        142  |                input, hx,
        143  |                self.get_weight_ih(), self.get_weight_hh(),
        144  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu_cell"

        144  |                self.bias_ih, self.bias_hh,
        145  |            )
        146  |        elif self.nonlinearity == "relu":
    >>> 147  |            ret = _VF.rnn_relu_cell(
        148  |                input, hx,
        149  |                self.get_weight_ih(), self.get_weight_hh(),
        150  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm_cell"

        202  |        else:
        203  |            hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx
        204  |
    >>> 205  |        ret = _VF.lstm_cell(
        206  |            input, hx,
        207  |            self.get_weight_ih(), self.get_weight_hh(),
        208  |            self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru_cell"

        253  |        else:
        254  |            hx = hx.unsqueeze(0) if not is_batched else hx
        255  |
    >>> 256  |        ret = _VF.gru_cell(
        257  |            input, hx,
        258  |            self.get_weight_ih(), self.get_weight_hh(),
        259  |            self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

        453  |
        454  |        self.check_forward_args(input, hx, batch_sizes)
        455  |        if batch_sizes is None:
    >>> 456  |            result = _VF.lstm(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        457  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
        458  |        else:
        459  |            result = _VF.lstm(input, batch_sizes, hx, self.get_flat_weights(), self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

        456  |            result = _VF.lstm(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        457  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
        458  |        else:
    >>> 459  |            result = _VF.lstm(input, batch_sizes, hx, self.get_flat_weights(), self.bias,
        460  |                              self.num_layers, self.dropout, self.training, self.bidirectional)
        461  |        output = result[0]
        462  |        hidden = result[1:]

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        576  |
        577  |        self.check_forward_args(input, hx, batch_sizes)
        578  |        if batch_sizes is None:
    >>> 579  |            result = _VF.gru(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        580  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        581  |        else:
        582  |            result = _VF.gru(input, batch_sizes, hx, self.get_flat_weights(), self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        579  |            result = _VF.gru(input, hx, self.get_flat_weights(), self.bias, self.num_layers,
        580  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        581  |        else:
    >>> 582  |            result = _VF.gru(input, batch_sizes, hx, self.get_flat_weights(), self.bias,
        583  |                             self.num_layers, self.dropout, self.training, self.bidirectional)
        584  |        output = result[0]
        585  |        hidden = result[1]



>>> Lint for torch/autograd/forward_ad.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "_make_dual"

        120  |            f"Expected tangent to be floating point or complex, but got: {tangent.dtype}"
        121  |        )
        122  |
    >>> 123  |    return torch._VF._make_dual(tensor, tangent, level=level)
        124  |
        125  |
        126  |_UnpackedDualTensor = namedtuple("_UnpackedDualTensor", ["primal", "tangent"])

  Error (MYPY) [attr-defined]
    Module has no attribute "_unpack_dual"

        163  |    if level < 0:
        164  |        return UnpackedDualTensor(tensor, None)
        165  |
    >>> 166  |    primal, dual = torch._VF._unpack_dual(tensor, level=level)
        167  |
        168  |    return UnpackedDualTensor(primal, dual)
        169  |



>>> Lint for torch/autograd/profiler.py:

  Error (MYPY) [attr-defined]
    "_KinetoEvent" has no attribute "device_resource_id"

         469  |                sequence_nr=kineto_event.sequence_nr(),
         470  |                device_type=kineto_event.device_type(),
         471  |                device_index=kineto_event.device_index(),
    >>>  472  |                device_resource_id=kineto_event.device_resource_id(),
         473  |                flops=kineto_event.flops(),
         474  |            )
         475  |            max_evt_id = max(max_evt_id, fe.id)



>>> Lint for torch/distributed/_tensor/tp_conv.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, Any]"; expected
    "list[int]"

        194  |        padding_w = padding[1]
        195  |        if rank == 0:
        196  |            grad_out_tensor = torch.nn.functional.pad(
    >>> 197  |                grad_out_tensor, (0, padding_w), "constant", 0
        198  |            )
        199  |        elif rank == size - 1:
        200  |            grad_out_tensor = torch.nn.functional.pad(

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[Any, int]"; expected
    "list[int]"

        198  |            )
        199  |        elif rank == size - 1:
        200  |            grad_out_tensor = torch.nn.functional.pad(
    >>> 201  |                grad_out_tensor, (padding_w, 0), "constant", 0
        202  |            )
        203  |        else:
        204  |            grad_out_tensor = torch.nn.functional.pad(

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[Any, Any]"; expected
    "list[int]"

        202  |            )
        203  |        else:
        204  |            grad_out_tensor = torch.nn.functional.pad(
    >>> 205  |                grad_out_tensor, (padding_w, padding_w), "constant", 0
        206  |            )
        207  |
        208  |        # step3 feed local input tensor to op_call



>>> Lint for torch/distributed/algorithms/ddp_comm_hooks/quantization_hooks.py:

  Error (MYPY) [arg-type]
    Argument "pad" to "pad" has incompatible type "tuple[int, Any]"; expected
    "list[int]"

        153  |    tensor_in_channels = (
        154  |        nn.functional.pad(
        155  |            input=tensor,
    >>> 156  |            pad=(0, bucket_size - len(tensor) % bucket_size),
        157  |            mode="constant",
        158  |            value=0,
        159  |        )



>>> Lint for torch/functional.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "unique_dim"

         899  |            return_counts=return_counts, dim=dim)
         900  |
         901  |    if dim is not None:
    >>>  902  |        output, inverse_indices, counts = _VF.unique_dim(
         903  |            input,
         904  |            dim,
         905  |            sorted=sorted,

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        1638  |    if dim is None and out is None and dtype is None and p is not None:
        1639  |        if isinstance(p, str):
        1640  |            if p == "fro":
    >>> 1641  |                return _VF.frobenius_norm(input, dim=(), keepdim=keepdim)
        1642  |        if not isinstance(p, str):
        1643  |            _dim = [i for i in range(ndim)]  # noqa: C416 TODO: rewrite as list(range(m))
        1644  |            return _VF.norm(input, p, dim=_dim, keepdim=keepdim)  # type: ignore[attr-defined]

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        1662  |            if _dim is None:
        1663  |                _dim = list(range(ndim))
        1664  |            if out is None:
    >>> 1665  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1666  |            else:
        1667  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1668  |        elif p == "nuc":

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        1664  |            if out is None:
        1665  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1666  |            else:
    >>> 1667  |                return _VF.frobenius_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1668  |        elif p == "nuc":
        1669  |            if dtype is not None:
        1670  |                raise ValueError("dtype argument is not supported in nuclear norm")

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1670  |                raise ValueError("dtype argument is not supported in nuclear norm")
        1671  |            if _dim is None:
        1672  |                if out is None:
    >>> 1673  |                    return _VF.nuclear_norm(input, keepdim=keepdim)  # type: ignore[arg-type]
        1674  |                else:
        1675  |                    return _VF.nuclear_norm(input, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1676  |            else:

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1672  |                if out is None:
        1673  |                    return _VF.nuclear_norm(input, keepdim=keepdim)  # type: ignore[arg-type]
        1674  |                else:
    >>> 1675  |                    return _VF.nuclear_norm(input, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1676  |            else:
        1677  |                if out is None:
        1678  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1675  |                    return _VF.nuclear_norm(input, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1676  |            else:
        1677  |                if out is None:
    >>> 1678  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1679  |                else:
        1680  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1681  |        raise RuntimeError(f"only valid string values are 'fro' and 'nuc', found {p}")

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        1677  |                if out is None:
        1678  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim)  # type: ignore[arg-type]
        1679  |                else:
    >>> 1680  |                    return _VF.nuclear_norm(input, _dim, keepdim=keepdim, out=out)  # type: ignore[arg-type]
        1681  |        raise RuntimeError(f"only valid string values are 'fro' and 'nuc', found {p}")
        1682  |    else:
        1683  |        if _dim is None:



>>> Lint for torch/jit/_builtins.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "unique_dim"

        101  |    (torch._VF.istft, "aten::istft"),  # type: ignore[attr-defined]
        102  |    (torch._VF.cdist, "aten::cdist"),  # type: ignore[attr-defined]
        103  |    (torch._VF.norm, "aten::norm"),  # type: ignore[attr-defined]
    >>> 104  |    (torch._VF.unique_dim, "aten::unique_dim"),
        105  |    (torch._VF.unique_consecutive, "aten::unique_consecutive"),  # type: ignore[attr-defined]
        106  |    (torch._VF.nuclear_norm, "aten::nuclear_norm"),
        107  |    (torch._VF.frobenius_norm, "aten::frobenius_norm"),

  Error (MYPY) [attr-defined]
    Module has no attribute "nuclear_norm"

        103  |    (torch._VF.norm, "aten::norm"),  # type: ignore[attr-defined]
        104  |    (torch._VF.unique_dim, "aten::unique_dim"),
        105  |    (torch._VF.unique_consecutive, "aten::unique_consecutive"),  # type: ignore[attr-defined]
    >>> 106  |    (torch._VF.nuclear_norm, "aten::nuclear_norm"),
        107  |    (torch._VF.frobenius_norm, "aten::frobenius_norm"),
        108  |    (torch._VF.tensordot, "aten::tensordot"),  # type: ignore[attr-defined]
        109  |]

  Error (MYPY) [attr-defined]
    Module has no attribute "frobenius_norm"

        104  |    (torch._VF.unique_dim, "aten::unique_dim"),
        105  |    (torch._VF.unique_consecutive, "aten::unique_consecutive"),  # type: ignore[attr-defined]
        106  |    (torch._VF.nuclear_norm, "aten::nuclear_norm"),
    >>> 107  |    (torch._VF.frobenius_norm, "aten::frobenius_norm"),
        108  |    (torch._VF.tensordot, "aten::tensordot"),  # type: ignore[attr-defined]
        109  |]
        110  |



>>> Lint for torch/nested/_internal/ops.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

         564  |        lengths = inp._offsets.diff()
         565  |        chunked_lengths = lengths.chunk(chunks)
         566  |        chunked_offsets = [torch.cumsum(x, dim=0) for x in chunked_lengths]
    >>>  567  |        chunked_offsets = [F.pad(x, (1, 0), value=0) for x in chunked_offsets]
         568  |        nested_kwargs = [
         569  |            {"offsets": per_offsets, "_ragged_idx": inp._ragged_idx}
         570  |            for per_offsets in chunked_offsets



>>> Lint for torch/nn/functional.py:

  Error (MYPY) [assignment]
    Incompatible types in assignment (expression has type "None", variable has
    type Module)

           7  |try:
           8  |    import numpy as np
           9  |except ModuleNotFoundError:
    >>>   10  |    np = None
          11  |
          12  |import torch
          13  |from torch import _VF

  Error (MYPY) [attr-defined]
    Module "torch._jit_internal" has no attribute "BroadcastingList2"; maybe
    "BroadcastingList1"?

          22  |    # The JIT doesn't understand Union, nor torch.dtype here
          23  |    DType = int
          24  |
    >>>   25  |from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
          26  |from ..overrides import (
          27  |    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
          28  |    handle_torch_function)

  Error (MYPY) [attr-defined]
    Module "torch._jit_internal" has no attribute "BroadcastingList3"; maybe
    "BroadcastingList1"?

          22  |    # The JIT doesn't understand Union, nor torch.dtype here
          23  |    DType = int
          24  |
    >>>   25  |from .._jit_internal import boolean_dispatch, _overload, BroadcastingList1, BroadcastingList2, BroadcastingList3
          26  |from ..overrides import (
          27  |    has_torch_function, has_torch_function_unary, has_torch_function_variadic,
          28  |    handle_torch_function)

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         636  |
         637  |
         638  |def max_pool1d_with_indices(
    >>>  639  |    input: Tensor, kernel_size: BroadcastingList1[int],
         640  |    stride: Optional[BroadcastingList1[int]] = None,
         641  |    padding: BroadcastingList1[int] = 0,
         642  |    dilation: BroadcastingList1[int] = 1,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         637  |
         638  |def max_pool1d_with_indices(
         639  |    input: Tensor, kernel_size: BroadcastingList1[int],
    >>>  640  |    stride: Optional[BroadcastingList1[int]] = None,
         641  |    padding: BroadcastingList1[int] = 0,
         642  |    dilation: BroadcastingList1[int] = 1,
         643  |    ceil_mode: bool = False,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         638  |def max_pool1d_with_indices(
         639  |    input: Tensor, kernel_size: BroadcastingList1[int],
         640  |    stride: Optional[BroadcastingList1[int]] = None,
    >>>  641  |    padding: BroadcastingList1[int] = 0,
         642  |    dilation: BroadcastingList1[int] = 1,
         643  |    ceil_mode: bool = False,
         644  |    return_indices: bool = False

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         639  |    input: Tensor, kernel_size: BroadcastingList1[int],
         640  |    stride: Optional[BroadcastingList1[int]] = None,
         641  |    padding: BroadcastingList1[int] = 0,
    >>>  642  |    dilation: BroadcastingList1[int] = 1,
         643  |    ceil_mode: bool = False,
         644  |    return_indices: bool = False
         645  |) -> Tuple[Tensor, Tensor]:  # noqa: D400

  Error (MYPY) [arg-type]
    Argument 3 to "max_pool1d_with_indices" has incompatible type
    "BroadcastingList1?[builtins.int] | None"; expected "int | Size |
    list[int] | tuple[int, ...]"

         682  |        )
         683  |    if stride is None:
         684  |        stride = torch.jit.annotate(List[int], [])
    >>>  685  |    return torch.max_pool1d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
         686  |
         687  |
         688  |def _max_pool1d(

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         686  |
         687  |
         688  |def _max_pool1d(
    >>>  689  |    input: Tensor, kernel_size: BroadcastingList1[int],
         690  |    stride: Optional[BroadcastingList1[int]] = None,
         691  |    padding: BroadcastingList1[int] = 0,
         692  |    dilation: BroadcastingList1[int] = 1,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         687  |
         688  |def _max_pool1d(
         689  |    input: Tensor, kernel_size: BroadcastingList1[int],
    >>>  690  |    stride: Optional[BroadcastingList1[int]] = None,
         691  |    padding: BroadcastingList1[int] = 0,
         692  |    dilation: BroadcastingList1[int] = 1,
         693  |    ceil_mode: bool = False,

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         688  |def _max_pool1d(
         689  |    input: Tensor, kernel_size: BroadcastingList1[int],
         690  |    stride: Optional[BroadcastingList1[int]] = None,
    >>>  691  |    padding: BroadcastingList1[int] = 0,
         692  |    dilation: BroadcastingList1[int] = 1,
         693  |    ceil_mode: bool = False,
         694  |    return_indices: bool = False

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         689  |    input: Tensor, kernel_size: BroadcastingList1[int],
         690  |    stride: Optional[BroadcastingList1[int]] = None,
         691  |    padding: BroadcastingList1[int] = 0,
    >>>  692  |    dilation: BroadcastingList1[int] = 1,
         693  |    ceil_mode: bool = False,
         694  |    return_indices: bool = False
         695  |) -> Tensor:

  Error (MYPY) [arg-type]
    Argument 3 to "max_pool1d" has incompatible type "BroadcastingList1?
    [builtins.int] | None"; expected "int | Size | list[int] |
    tuple[int, ...]"

         707  |        )
         708  |    if stride is None:
         709  |        stride = torch.jit.annotate(List[int], [])
    >>>  710  |    return torch.max_pool1d(input, kernel_size, stride, padding, dilation, ceil_mode)
         711  |
         712  |
         713  |max_pool1d = boolean_dispatch(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_pool2d_with_indices"

         768  |        )
         769  |    if stride is None:
         770  |        stride = torch.jit.annotate(List[int], [])
    >>>  771  |    return torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
         772  |
         773  |
         774  |def _max_pool2d(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_pool3d_with_indices"

         854  |        )
         855  |    if stride is None:
         856  |        stride = torch.jit.annotate(List[int], [])
    >>>  857  |    return torch._C._nn.max_pool3d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)
         858  |
         859  |
         860  |def _max_pool3d(

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         924  |
         925  |def max_unpool1d(
         926  |    input: Tensor, indices: Tensor,
    >>>  927  |    kernel_size: BroadcastingList1[int],
         928  |    stride: Optional[BroadcastingList1[int]] = None,
         929  |    padding: BroadcastingList1[int] = 0,
         930  |    output_size: Optional[BroadcastingList1[int]] = None

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         925  |def max_unpool1d(
         926  |    input: Tensor, indices: Tensor,
         927  |    kernel_size: BroadcastingList1[int],
    >>>  928  |    stride: Optional[BroadcastingList1[int]] = None,
         929  |    padding: BroadcastingList1[int] = 0,
         930  |    output_size: Optional[BroadcastingList1[int]] = None
         931  |) -> Tensor:

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         926  |    input: Tensor, indices: Tensor,
         927  |    kernel_size: BroadcastingList1[int],
         928  |    stride: Optional[BroadcastingList1[int]] = None,
    >>>  929  |    padding: BroadcastingList1[int] = 0,
         930  |    output_size: Optional[BroadcastingList1[int]] = None
         931  |) -> Tensor:
         932  |    r"""Compute a partial inverse of :class:`MaxPool1d`.

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

         927  |    kernel_size: BroadcastingList1[int],
         928  |    stride: Optional[BroadcastingList1[int]] = None,
         929  |    padding: BroadcastingList1[int] = 0,
    >>>  930  |    output_size: Optional[BroadcastingList1[int]] = None
         931  |) -> Tensor:
         932  |    r"""Compute a partial inverse of :class:`MaxPool1d`.
         933  |

  Error (MYPY) [attr-defined]
    Module has no attribute "max_unpool2d"

         955  |        output_size = output_size + [1]
         956  |    else:
         957  |        output_size = output_size + (1,)
    >>>  958  |    return torch._C._nn.max_unpool2d(input.unsqueeze(-1), indices.unsqueeze(-1), output_size).squeeze(-1)
         959  |
         960  |
         961  |def max_unpool2d(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_unpool2d"

         987  |        _stride = kernel_size
         988  |    padding = _pair(padding)
         989  |    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)
    >>>  990  |    return torch._C._nn.max_unpool2d(input, indices, output_size)
         991  |
         992  |
         993  |def max_unpool3d(

  Error (MYPY) [attr-defined]
    Module has no attribute "max_unpool3d"

        1019  |        _stride = kernel_size
        1020  |    padding = _triple(padding)
        1021  |    output_size = _unpool_output_size(input, kernel_size, _stride, padding, output_size)
    >>> 1022  |    return torch._C._nn.max_unpool3d(input, indices, output_size, _stride, padding)
        1023  |
        1024  |
        1025  |def lp_pool3d(

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

        1079  |def lp_pool1d(
        1080  |    input: Tensor, norm_type: Union[int, float],
        1081  |    kernel_size: int,
    >>> 1082  |    stride: Optional[BroadcastingList1[int]] = None,
        1083  |    ceil_mode: bool = False
        1084  |) -> Tensor:
        1085  |    r"""Apply a 1D power-average pooling over an input signal composed of several input planes.

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

        1102  |
        1103  |
        1104  |def adaptive_max_pool1d_with_indices(
    >>> 1105  |    input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False
        1106  |) -> Tuple[Tensor, Tensor]:  # noqa: D400
        1107  |    r"""
        1108  |    adaptive_max_pool1d(input, output_size, return_indices=False)

  Error (MYPY) [valid-type]
    Variable "torch._jit_internal.BroadcastingList1" is not valid as a type

        1123  |    return torch.adaptive_max_pool1d(input, output_size)
        1124  |
        1125  |
    >>> 1126  |def _adaptive_max_pool1d(input: Tensor, output_size: BroadcastingList1[int], return_indices: bool = False) -> Tensor:
        1127  |    if has_torch_function_unary(input):
        1128  |        return handle_torch_function(
        1129  |            adaptive_max_pool1d, (input,), input, output_size, return_indices=return_indices

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1162  |        return handle_torch_function(
        1163  |            adaptive_max_pool2d_with_indices, (input,), input, output_size, return_indices=return_indices
        1164  |        )
    >>> 1165  |    output_size = _list_with_default(output_size, input.size())
        1166  |    return torch._C._nn.adaptive_max_pool2d(input, output_size)
        1167  |
        1168  |

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1206  |        return handle_torch_function(
        1207  |            adaptive_max_pool3d_with_indices, (input,), input, output_size, return_indices=return_indices
        1208  |        )
    >>> 1209  |    output_size = _list_with_default(output_size, input.size())
        1210  |    return torch._C._nn.adaptive_max_pool3d(input, output_size)
        1211  |
        1212  |

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1256  |    """
        1257  |    if has_torch_function_unary(input):
        1258  |        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)
    >>> 1259  |    _output_size = _list_with_default(output_size, input.size())
        1260  |    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
        1261  |
        1262  |

  Error (MYPY) [attr-defined]
    Module has no attribute "adaptive_avg_pool2d"; maybe "adaptive_max_pool2d"
    or "adaptive_max_pool3d"?

        1257  |    if has_torch_function_unary(input):
        1258  |        return handle_torch_function(adaptive_avg_pool2d, (input,), input, output_size)
        1259  |    _output_size = _list_with_default(output_size, input.size())
    >>> 1260  |    return torch._C._nn.adaptive_avg_pool2d(input, _output_size)
        1261  |
        1262  |
        1263  |def adaptive_avg_pool3d(input: Tensor, output_size: BroadcastingList3[int]) -> Tensor:

  Error (MYPY) [arg-type]
    Argument 2 to "_list_with_default" has incompatible type "Size"; expected
    "list[int]"

        1271  |    """
        1272  |    if has_torch_function_unary(input):
        1273  |        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)
    >>> 1274  |    _output_size = _list_with_default(output_size, input.size())
        1275  |    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)
        1276  |
        1277  |

  Error (MYPY) [attr-defined]
    Module has no attribute "adaptive_avg_pool3d"; maybe "adaptive_max_pool3d"
    or "adaptive_max_pool2d"?

        1272  |    if has_torch_function_unary(input):
        1273  |        return handle_torch_function(adaptive_avg_pool3d, (input,), input, output_size)
        1274  |    _output_size = _list_with_default(output_size, input.size())
    >>> 1275  |    return torch._C._nn.adaptive_avg_pool3d(input, _output_size)
        1276  |
        1277  |
        1278  |# Activation functions

  Error (MYPY) [attr-defined]
    Module has no attribute "dropout_"

        1292  |        return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)
        1293  |    if p < 0.0 or p > 1.0:
        1294  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1295  |    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
        1296  |
        1297  |
        1298  |def alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "dropout"

        1292  |        return handle_torch_function(dropout, (input,), input, p=p, training=training, inplace=inplace)
        1293  |    if p < 0.0 or p > 1.0:
        1294  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1295  |    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
        1296  |
        1297  |
        1298  |def alpha_dropout(input: Tensor, p: float = 0.5, training: bool = False, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "alpha_dropout_"

        1304  |        return handle_torch_function(alpha_dropout, (input,), input, p=p, training=training, inplace=inplace)
        1305  |    if p < 0.0 or p > 1.0:
        1306  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1307  |    return _VF.alpha_dropout_(input, p, training) if inplace else _VF.alpha_dropout(input, p, training)
        1308  |
        1309  |
        1310  |def dropout1d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "alpha_dropout"

        1304  |        return handle_torch_function(alpha_dropout, (input,), input, p=p, training=training, inplace=inplace)
        1305  |    if p < 0.0 or p > 1.0:
        1306  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1307  |    return _VF.alpha_dropout_(input, p, training) if inplace else _VF.alpha_dropout(input, p, training)
        1308  |
        1309  |
        1310  |def dropout1d(input: Tensor, p: float = 0.5, training: bool = True, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout_"

        1337  |    if not is_batched:
        1338  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1339  |
    >>> 1340  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1341  |
        1342  |    if not is_batched:
        1343  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout"

        1337  |    if not is_batched:
        1338  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1339  |
    >>> 1340  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1341  |
        1342  |    if not is_batched:
        1343  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout_"

        1384  |                      "input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D "
        1385  |                      "channel-wise dropout behavior, please switch to using dropout1d instead.")
        1386  |
    >>> 1387  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1388  |
        1389  |    return result
        1390  |

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout"

        1384  |                      "input as one without a batch dimension, i.e. shape (C, H, W). To maintain the 1D "
        1385  |                      "channel-wise dropout behavior, please switch to using dropout1d instead.")
        1386  |
    >>> 1387  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1388  |
        1389  |    return result
        1390  |

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout_"

        1421  |    if not is_batched:
        1422  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1423  |
    >>> 1424  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1425  |
        1426  |    if not is_batched:
        1427  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_dropout"

        1421  |    if not is_batched:
        1422  |        input = input.unsqueeze_(0) if inplace else input.unsqueeze(0)
        1423  |
    >>> 1424  |    result = _VF.feature_dropout_(input, p, training) if inplace else _VF.feature_dropout(input, p, training)
        1425  |
        1426  |    if not is_batched:
        1427  |        result = result.squeeze_(0) if inplace else result.squeeze(0)

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_alpha_dropout_"

        1454  |        )
        1455  |    if p < 0.0 or p > 1.0:
        1456  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1457  |    return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)
        1458  |
        1459  |
        1460  |def _threshold(input: Tensor, threshold: float, value: float, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "feature_alpha_dropout"

        1454  |        )
        1455  |    if p < 0.0 or p > 1.0:
        1456  |        raise ValueError(f"dropout probability has to be between 0 and 1, but got {p}")
    >>> 1457  |    return _VF.feature_alpha_dropout_(input, p, training) if inplace else _VF.feature_alpha_dropout(input, p, training)
        1458  |
        1459  |
        1460  |def _threshold(input: Tensor, threshold: float, value: float, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "threshold_"

        1465  |    if has_torch_function_unary(input):
        1466  |        return handle_torch_function(_threshold, (input,), input, threshold, value, inplace=inplace)
        1467  |    if inplace:
    >>> 1468  |        result = _VF.threshold_(input, threshold, value)
        1469  |    else:
        1470  |        result = _VF.threshold(input, threshold, value)
        1471  |    return result

  Error (MYPY) [attr-defined]
    Module has no attribute "threshold"

        1467  |    if inplace:
        1468  |        result = _VF.threshold_(input, threshold, value)
        1469  |    else:
    >>> 1470  |        result = _VF.threshold(input, threshold, value)
        1471  |    return result
        1472  |
        1473  |

  Error (MYPY) [attr-defined]
    Module has no attribute "threshold_"

        1477  |threshold = _threshold
        1478  |
        1479  |threshold_ = _add_docstr(
    >>> 1480  |    _VF.threshold_,
        1481  |    r"""
        1482  |threshold_(input, threshold, value) -> Tensor
        1483  |

  Error (MYPY) [attr-defined]
    Module has no attribute "glu"; maybe "gelu"?

        1533  |        return handle_torch_function(glu, (input,), input, dim=dim)
        1534  |    if input.dim() == 0:
        1535  |        raise RuntimeError("glu does not support scalars because halving size must be even")
    >>> 1536  |    return torch._C._nn.glu(input, dim)
        1537  |
        1538  |
        1539  |def hardtanh(input: Tensor, min_val: float = -1., max_val: float = 1., inplace: bool = False) -> Tensor:  # noqa: D400,D402

  Error (MYPY) [attr-defined]
    Module has no attribute "relu6_"; maybe "elu_"?

        1574  |    if has_torch_function_unary(input):
        1575  |        return handle_torch_function(relu6, (input,), input, inplace=inplace)
        1576  |    if inplace:
    >>> 1577  |        result = torch._C._nn.relu6_(input)
        1578  |    else:
        1579  |        result = torch._C._nn.relu6(input)
        1580  |    return result

  Error (MYPY) [attr-defined]
    Module has no attribute "relu6"

        1576  |    if inplace:
        1577  |        result = torch._C._nn.relu6_(input)
        1578  |    else:
    >>> 1579  |        result = torch._C._nn.relu6(input)
        1580  |    return result
        1581  |
        1582  |

  Error (MYPY) [attr-defined]
    Module has no attribute "elu"; maybe "elu_" or "gelu"?

        1590  |    if inplace:
        1591  |        result = torch._C._nn.elu_(input, alpha)
        1592  |    else:
    >>> 1593  |        result = torch._C._nn.elu(input, alpha)
        1594  |    return result
        1595  |
        1596  |

  Error (MYPY) [attr-defined]
    Module has no attribute "hardsigmoid_"; maybe "hardsigmoid"?

        2030  |    if has_torch_function_unary(input):
        2031  |        return handle_torch_function(hardsigmoid, (input,), input, inplace=inplace)
        2032  |    if inplace:
    >>> 2033  |        return torch._C._nn.hardsigmoid_(input)
        2034  |    return torch._C._nn.hardsigmoid(input)
        2035  |
        2036  |

  Error (MYPY) [attr-defined]
    Module has no attribute "silu_"

        2100  |    if has_torch_function_unary(input):
        2101  |        return handle_torch_function(silu, (input,), input, inplace=inplace)
        2102  |    if inplace:
    >>> 2103  |        return torch._C._nn.silu_(input)
        2104  |    return torch._C._nn.silu(input)
        2105  |
        2106  |

  Error (MYPY) [attr-defined]
    Module has no attribute "silu"

        2101  |        return handle_torch_function(silu, (input,), input, inplace=inplace)
        2102  |    if inplace:
        2103  |        return torch._C._nn.silu_(input)
    >>> 2104  |    return torch._C._nn.silu(input)
        2105  |
        2106  |
        2107  |def mish(input: Tensor, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "mish_"

        2120  |    if has_torch_function_unary(input):
        2121  |        return handle_torch_function(mish, (input,), input, inplace=inplace)
        2122  |    if inplace:
    >>> 2123  |        return torch._C._nn.mish_(input)
        2124  |    return torch._C._nn.mish(input)
        2125  |
        2126  |

  Error (MYPY) [attr-defined]
    Module has no attribute "mish"

        2121  |        return handle_torch_function(mish, (input,), input, inplace=inplace)
        2122  |    if inplace:
        2123  |        return torch._C._nn.mish_(input)
    >>> 2124  |    return torch._C._nn.mish(input)
        2125  |
        2126  |
        2127  |def hardswish(input: Tensor, inplace: bool = False) -> Tensor:

  Error (MYPY) [attr-defined]
    Module has no attribute "hardswish_"

        2145  |    if has_torch_function_unary(input):
        2146  |        return handle_torch_function(hardswish, (input,), input, inplace=inplace)
        2147  |    if inplace:
    >>> 2148  |        return torch._C._nn.hardswish_(input)
        2149  |    return torch._C._nn.hardswish(input)
        2150  |
        2151  |

  Error (MYPY) [attr-defined]
    Module has no attribute "hardswish"

        2146  |        return handle_torch_function(hardswish, (input,), input, inplace=inplace)
        2147  |    if inplace:
        2148  |        return torch._C._nn.hardswish_(input)
    >>> 2149  |    return torch._C._nn.hardswish(input)
        2150  |
        2151  |
        2152  |def _no_grad_embedding_renorm_(weight: Tensor, input: Tensor, max_norm: float, norm_type: float) -> Tuple[Tensor, Tensor]:

  Error (MYPY) [return]
    Missing return statement

        2149  |    return torch._C._nn.hardswish(input)
        2150  |
        2151  |
    >>> 2152  |def _no_grad_embedding_renorm_(weight: Tensor, input: Tensor, max_norm: float, norm_type: float) -> Tuple[Tensor, Tensor]:
        2153  |    torch.embedding_renorm_(weight.detach(), input, max_norm, norm_type)
        2154  |
        2155  |

  Error (MYPY) [arg-type]
    Argument 1 to "_verify_batch_size" has incompatible type "Size"; expected
    "list[int]"

        2506  |            eps=eps,
        2507  |        )
        2508  |    if training:
    >>> 2509  |        _verify_batch_size(input.size())
        2510  |
        2511  |    return torch.batch_norm(
        2512  |        input, weight, bias, running_mean, running_var, training, momentum, eps, torch.backends.cudnn.enabled

  Error (MYPY) [arg-type]
    Argument 1 to "_verify_spatial_size" has incompatible type "Size";
    expected "list[int]"

        2551  |            eps=eps,
        2552  |        )
        2553  |    if use_input_stats:
    >>> 2554  |        _verify_spatial_size(input.size())
        2555  |    return torch.instance_norm(
        2556  |        input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, torch.backends.cudnn.enabled
        2557  |    )

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int]";
    expected "list[int]"

        2612  |    div = input.mul(input)
        2613  |    if dim == 3:
        2614  |        div = div.unsqueeze(1)
    >>> 2615  |        div = pad(div, (0, 0, size // 2, (size - 1) // 2))
        2616  |        div = avg_pool2d(div, (size, 1), stride=1).squeeze(1)
        2617  |    else:
        2618  |        sizes = input.size()

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int, int, int, int,
    int]"; expected "list[int]"

        2617  |    else:
        2618  |        sizes = input.size()
        2619  |        div = div.view(sizes[0], 1, sizes[1], sizes[2], -1)
    >>> 2620  |        div = pad(div, (0, 0, 0, 0, size // 2, (size - 1) // 2))
        2621  |        div = avg_pool3d(div, (size, 1, 1), stride=1).squeeze(1)
        2622  |        div = div.view(sizes)
        2623  |    div = div.mul(alpha).add(k).pow(beta)

  Error (MYPY) [attr-defined]
    Module has no attribute "nll_loss_nd"

        2759  |        )
        2760  |    if size_average is not None or reduce is not None:
        2761  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
    >>> 2762  |    return torch._C._nn.nll_loss_nd(input, target, weight, _Reduction.get_enum(reduction), ignore_index)
        2763  |
        2764  |
        2765  |def poisson_nll_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "cross_entropy_loss"

        3085  |        )
        3086  |    if size_average is not None or reduce is not None:
        3087  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
    >>> 3088  |    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
        3089  |
        3090  |
        3091  |def binary_cross_entropy(

  Error (MYPY) [attr-defined]
    Module has no attribute "binary_cross_entropy"

        3153  |        new_size = _infer_size(target.size(), weight.size())
        3154  |        weight = weight.expand(new_size)
        3155  |
    >>> 3156  |    return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)
        3157  |
        3158  |
        3159  |def binary_cross_entropy_with_logits(

  Error (MYPY) [attr-defined]
    Module has no attribute "l1_loss"

        3267  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
        3268  |
        3269  |    if beta == 0.0:
    >>> 3270  |        return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3271  |    else:
        3272  |        return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)
        3273  |

  Error (MYPY) [attr-defined]
    Module has no attribute "smooth_l1_loss"

        3269  |    if beta == 0.0:
        3270  |        return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3271  |    else:
    >>> 3272  |        return torch._C._nn.smooth_l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), beta)
        3273  |
        3274  |
        3275  |def huber_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "huber_loss"

        3304  |                      stacklevel=2)
        3305  |
        3306  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
    >>> 3307  |    return torch._C._nn.huber_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction), delta)
        3308  |
        3309  |
        3310  |def l1_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "l1_loss"

        3335  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
        3336  |
        3337  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
    >>> 3338  |    return torch._C._nn.l1_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3339  |
        3340  |
        3341  |def mse_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "mse_loss"

        3365  |        reduction = _Reduction.legacy_get_string(size_average, reduce)
        3366  |
        3367  |    expanded_input, expanded_target = torch.broadcast_tensors(input, target)
    >>> 3368  |    return torch._C._nn.mse_loss(expanded_input, expanded_target, _Reduction.get_enum(reduction))
        3369  |
        3370  |
        3371  |def margin_ranking_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "multilabel_margin_loss"

        3460  |        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
        3461  |    else:
        3462  |        reduction_enum = _Reduction.get_enum(reduction)
    >>> 3463  |    return torch._C._nn.multilabel_margin_loss(input, target, reduction_enum)
        3464  |
        3465  |
        3466  |def soft_margin_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "soft_margin_loss"

        3483  |        reduction_enum = _Reduction.legacy_get_enum(size_average, reduce)
        3484  |    else:
        3485  |        reduction_enum = _Reduction.get_enum(reduction)
    >>> 3486  |    return torch._C._nn.soft_margin_loss(input, target, reduction_enum)
        3487  |
        3488  |
        3489  |def multilabel_soft_margin_loss(

  Error (MYPY) [attr-defined]
    Module has no attribute "multi_margin_loss"

        3602  |        if weight.dim() != 1:
        3603  |            raise ValueError("weight must be one-dimensional")
        3604  |
    >>> 3605  |    return torch._C._nn.multi_margin_loss(input, target, p, margin, weight, reduction_enum)
        3606  |
        3607  |
        3608  |pixel_shuffle = _add_docstr(

  Error (MYPY) [no-redef]
    Name "upsample" already defined on line 3742

        3744  |    pass
        3745  |
        3746  |
    >>> 3747  |@_overload  # noqa: F811
        3748  |def upsample(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None, mode: str = "nearest", align_corners: Optional[bool] = None) -> Tensor:  # noqa: F811,B950
        3749  |    pass
        3750  |

  Error (MYPY) [no-redef]
    Name "upsample" already defined on line 3742

        3749  |    pass
        3750  |
        3751  |
    >>> 3752  |def upsample(input, size=None, scale_factor=None, mode="nearest", align_corners=None):  # noqa: F811
        3753  |    r"""Upsample input.
        3754  |
        3755  |    Provided tensor is upsampled to either the given :attr:`size` or the given

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3831

        3833  |    pass
        3834  |
        3835  |
    >>> 3836  |@_overload  # noqa: F811
        3837  |def interpolate(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950
        3838  |    pass
        3839  |

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3831

        3838  |    pass
        3839  |
        3840  |
    >>> 3841  |@_overload  # noqa: F811
        3842  |def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[float] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950
        3843  |    pass
        3844  |

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3831

        3843  |    pass
        3844  |
        3845  |
    >>> 3846  |@_overload  # noqa: F811
        3847  |def interpolate(  # noqa: F811
        3848  |    input: Tensor,
        3849  |    size: Optional[List[int]] = None,

  Error (MYPY) [no-redef]
    Name "interpolate" already defined on line 3831

        3855  |) -> Tensor:  # noqa: F811
        3856  |    pass
        3857  |
    >>> 3858  |def interpolate(input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None, mode: str = 'nearest', align_corners: Optional[bool] = None, recompute_scale_factor: Optional[bool] = None, antialias: bool = False) -> Tensor:  # noqa: F811,B950
        3859  |    r"""Down/up samples the input.
        3860  |
        3861  |    Tensor interpolated to either the given :attr:`size` or the given

  Error (MYPY) [no-redef]
    Name "upsample_nearest" already defined on line 4101

        4103  |    pass
        4104  |
        4105  |
    >>> 4106  |@_overload  # noqa: F811
        4107  |def upsample_nearest(input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None) -> Tensor:  # noqa: F811
        4108  |    pass
        4109  |

  Error (MYPY) [no-redef]
    Name "upsample_nearest" already defined on line 4101

        4108  |    pass
        4109  |
        4110  |
    >>> 4111  |def upsample_nearest(input, size=None, scale_factor=None):  # noqa: F811
        4112  |    r"""Upsamples the input, using nearest neighbours' pixel values.
        4113  |
        4114  |    .. warning::

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4139

        4143  |    pass
        4144  |
        4145  |
    >>> 4146  |@_overload  # noqa: F811
        4147  |def upsample_bilinear(  # noqa: F811
        4148  |    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[float] = None
        4149  |) -> Tensor:  # noqa: F811

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4139

        4150  |    pass
        4151  |
        4152  |
    >>> 4153  |@_overload  # noqa: F811
        4154  |def upsample_bilinear(  # noqa: F811
        4155  |    input: Tensor, size: Optional[int] = None, scale_factor: Optional[List[float]] = None
        4156  |) -> Tensor:  # noqa: F811

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4139

        4157  |    pass
        4158  |
        4159  |
    >>> 4160  |@_overload  # noqa: F811
        4161  |def upsample_bilinear(  # noqa: F811
        4162  |    input: Tensor, size: Optional[List[int]] = None, scale_factor: Optional[List[float]] = None
        4163  |) -> Tensor:  # noqa: F811

  Error (MYPY) [no-redef]
    Name "upsample_bilinear" already defined on line 4139

        4164  |    pass
        4165  |
        4166  |
    >>> 4167  |def upsample_bilinear(input, size=None, scale_factor=None):  # noqa: F811
        4168  |    r"""Upsamples the input, using bilinear upsampling.
        4169  |
        4170  |    .. warning::

  Error (MYPY) [attr-defined]
    Module has no attribute "im2col"

        4822  |        return handle_torch_function(
        4823  |            unfold, (input,), input, kernel_size, dilation=dilation, padding=padding, stride=stride
        4824  |        )
    >>> 4825  |    return torch._C._nn.im2col(input, _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride))
        4826  |
        4827  |
        4828  |def fold(

  Error (MYPY) [attr-defined]
    Module has no attribute "col2im"

        4843  |        return handle_torch_function(
        4844  |            fold, (input,), input, output_size, kernel_size, dilation=dilation, padding=padding, stride=stride
        4845  |        )
    >>> 4846  |    return torch._C._nn.col2im(
        4847  |        input, _pair(output_size), _pair(kernel_size), _pair(dilation), _pair(padding), _pair(stride)
        4848  |    )
        4849  |

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Any, Any, Any]", expected
    "list[Tensor]")

        4892  |            proj = linear(q, w, b)
        4893  |            # reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()
        4894  |            proj = proj.unflatten(-1, (3, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
    >>> 4895  |            return proj[0], proj[1], proj[2]
        4896  |        else:
        4897  |            # encoder-decoder attention
        4898  |            w_q, w_kv = w.split([E, E * 2])

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Tensor, Any, Any]", expected
    "list[Tensor]")

        4904  |            kv_proj = linear(k, w_kv, b_kv)
        4905  |            # reshape to 2, E and not E, 2 is deliberate for better memory coalescing and keeping same order as chunk()
        4906  |            kv_proj = kv_proj.unflatten(-1, (2, E)).unsqueeze(0).transpose(0, -2).squeeze(-2).contiguous()
    >>> 4907  |            return (q_proj, kv_proj[0], kv_proj[1])
        4908  |    else:
        4909  |        w_q, w_k, w_v = w.chunk(3)
        4910  |        if b is None:

  Error (MYPY) [return-value]
    Incompatible return value type (got "tuple[Tensor, Tensor, Tensor]",
    expected "list[Tensor]")

        4911  |            b_q = b_k = b_v = None
        4912  |        else:
        4913  |            b_q, b_k, b_v = b.chunk(3)
    >>> 4914  |        return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
        4915  |
        4916  |
        4917  |def _in_projection(

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5403  |        k = torch.cat([k, bias_k.repeat(1, bsz, 1)])
        5404  |        v = torch.cat([v, bias_v.repeat(1, bsz, 1)])
        5405  |        if attn_mask is not None:
    >>> 5406  |            attn_mask = pad(attn_mask, (0, 1))
        5407  |        if key_padding_mask is not None:
        5408  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5409  |    else:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5405  |        if attn_mask is not None:
        5406  |            attn_mask = pad(attn_mask, (0, 1))
        5407  |        if key_padding_mask is not None:
    >>> 5408  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5409  |    else:
        5410  |        assert bias_k is None
        5411  |        assert bias_v is None

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5439  |        k = torch.cat([k, torch.zeros(zero_attn_shape, dtype=k.dtype, device=k.device)], dim=1)
        5440  |        v = torch.cat([v, torch.zeros(zero_attn_shape, dtype=v.dtype, device=v.device)], dim=1)
        5441  |        if attn_mask is not None:
    >>> 5442  |            attn_mask = pad(attn_mask, (0, 1))
        5443  |        if key_padding_mask is not None:
        5444  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5445  |

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        5441  |        if attn_mask is not None:
        5442  |            attn_mask = pad(attn_mask, (0, 1))
        5443  |        if key_padding_mask is not None:
    >>> 5444  |            key_padding_mask = pad(key_padding_mask, (0, 1))
        5445  |
        5446  |    # update source sequence length after adjustments
        5447  |    src_len = k.size(1)



>>> Lint for torch/nn/modules/normalization.py:

  Error (MYPY) [arg-type]
    Argument 2 to "layer_norm" has incompatible type "tuple[int, ...]";
    expected "list[int]"

        199  |
        200  |    def forward(self, input: Tensor) -> Tensor:
        201  |        return F.layer_norm(
    >>> 202  |            input, self.normalized_shape, self.weight, self.bias, self.eps)
        203  |
        204  |    def extra_repr(self) -> str:
        205  |        return '{normalized_shape}, eps={eps}, ' \



>>> Lint for torch/nn/modules/padding.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

         23  |
         24  |    def forward(self, input: Tensor) -> Tensor:
         25  |        self._check_input_dim(input)
    >>>  26  |        return F.pad(input, self.padding, 'circular')
         27  |
         28  |    def extra_repr(self) -> str:
         29  |        return f'{self.padding}'

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

        202  |        self.value = value
        203  |
        204  |    def forward(self, input: Tensor) -> Tensor:
    >>> 205  |        return F.pad(input, self.padding, 'constant', self.value)
        206  |
        207  |    def extra_repr(self) -> str:
        208  |        return f'padding={self.padding}, value={self.value}'

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

        355  |    padding: Sequence[int]
        356  |
        357  |    def forward(self, input: Tensor) -> Tensor:
    >>> 358  |        return F.pad(input, self.padding, 'reflect')
        359  |
        360  |    def extra_repr(self) -> str:
        361  |        return f'{self.padding}'

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "Sequence[int]"; expected
    "list[int]"

        514  |    padding: Sequence[int]
        515  |
        516  |    def forward(self, input: Tensor) -> Tensor:
    >>> 517  |        return F.pad(input, self.padding, 'replicate')
        518  |
        519  |    def extra_repr(self) -> str:
        520  |        return f'{self.padding}'



>>> Lint for torch/nn/modules/rnn.py:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh"

          15  |__all__ = ['RNNBase', 'RNN', 'LSTM', 'GRU', 'RNNCellBase', 'RNNCell', 'LSTMCell', 'GRUCell']
          16  |
          17  |_rnn_impls = {
    >>>   18  |    'RNN_TANH': _VF.rnn_tanh,
          19  |    'RNN_RELU': _VF.rnn_relu,
          20  |}
          21  |

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu"

          16  |
          17  |_rnn_impls = {
          18  |    'RNN_TANH': _VF.rnn_tanh,
    >>>   19  |    'RNN_RELU': _VF.rnn_relu,
          20  |}
          21  |
          22  |

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh"

         582  |        assert self.mode == 'RNN_TANH' or self.mode == 'RNN_RELU'
         583  |        if batch_sizes is None:
         584  |            if self.mode == 'RNN_TANH':
    >>>  585  |                result = _VF.rnn_tanh(input, hx, self._flat_weights, self.bias, self.num_layers,
         586  |                                      self.dropout, self.training, self.bidirectional,
         587  |                                      self.batch_first)
         588  |            else:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu"

         586  |                                      self.dropout, self.training, self.bidirectional,
         587  |                                      self.batch_first)
         588  |            else:
    >>>  589  |                result = _VF.rnn_relu(input, hx, self._flat_weights, self.bias, self.num_layers,
         590  |                                      self.dropout, self.training, self.bidirectional,
         591  |                                      self.batch_first)
         592  |        else:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh"

         591  |                                      self.batch_first)
         592  |        else:
         593  |            if self.mode == 'RNN_TANH':
    >>>  594  |                result = _VF.rnn_tanh(input, batch_sizes, hx, self._flat_weights, self.bias,
         595  |                                      self.num_layers, self.dropout, self.training,
         596  |                                      self.bidirectional)
         597  |            else:

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu"

         595  |                                      self.num_layers, self.dropout, self.training,
         596  |                                      self.bidirectional)
         597  |            else:
    >>>  598  |                result = _VF.rnn_relu(input, batch_sizes, hx, self._flat_weights, self.bias,
         599  |                                      self.num_layers, self.dropout, self.training,
         600  |                                      self.bidirectional)
         601  |

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

         907  |                hx = self.permute_hidden(hx, sorted_indices)
         908  |
         909  |        if batch_sizes is None:
    >>>  910  |            result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
         911  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
         912  |        else:
         913  |            result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm"

         910  |            result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,
         911  |                              self.dropout, self.training, self.bidirectional, self.batch_first)
         912  |        else:
    >>>  913  |            result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,
         914  |                              self.num_layers, self.dropout, self.training, self.bidirectional)
         915  |        output = result[0]
         916  |        hidden = result[1:]

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        1129  |
        1130  |        self.check_forward_args(input, hx, batch_sizes)
        1131  |        if batch_sizes is None:
    >>> 1132  |            result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,
        1133  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        1134  |        else:
        1135  |            result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru"

        1132  |            result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,
        1133  |                             self.dropout, self.training, self.bidirectional, self.batch_first)
        1134  |        else:
    >>> 1135  |            result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,
        1136  |                             self.num_layers, self.dropout, self.training, self.bidirectional)
        1137  |        output = result[0]
        1138  |        hidden = result[1]

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_tanh_cell"

        1271  |            hx = hx.unsqueeze(0) if not is_batched else hx
        1272  |
        1273  |        if self.nonlinearity == "tanh":
    >>> 1274  |            ret = _VF.rnn_tanh_cell(
        1275  |                input, hx,
        1276  |                self.weight_ih, self.weight_hh,
        1277  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "rnn_relu_cell"

        1277  |                self.bias_ih, self.bias_hh,
        1278  |            )
        1279  |        elif self.nonlinearity == "relu":
    >>> 1280  |            ret = _VF.rnn_relu_cell(
        1281  |                input, hx,
        1282  |                self.weight_ih, self.weight_hh,
        1283  |                self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "lstm_cell"

        1375  |        else:
        1376  |            hx = (hx[0].unsqueeze(0), hx[1].unsqueeze(0)) if not is_batched else hx
        1377  |
    >>> 1378  |        ret = _VF.lstm_cell(
        1379  |            input, hx,
        1380  |            self.weight_ih, self.weight_hh,
        1381  |            self.bias_ih, self.bias_hh,

  Error (MYPY) [attr-defined]
    Module has no attribute "gru_cell"

        1467  |        else:
        1468  |            hx = hx.unsqueeze(0) if not is_batched else hx
        1469  |
    >>> 1470  |        ret = _VF.gru_cell(
        1471  |            input, hx,
        1472  |            self.weight_ih, self.weight_hh,
        1473  |            self.bias_ih, self.bias_hh,



>>> Lint for torch/onnx/_internal/fx/passes/modularization.py:

  Error (MYPY) [misc]
    Final name must be initialized with a value

         54  |        _raw_meta: The raw meta '(module_name, node.meta["nn_module_stack"][module_name])'.
         55  |    """
         56  |
    >>>  57  |    _module_class: Final[Optional[type]]
         58  |    _module_name: Final[str]
         59  |    _raw_meta: Final[Tuple[Any, Any]]
         60  |

  Error (MYPY) [misc]
    Final name must be initialized with a value

         55  |    """
         56  |
         57  |    _module_class: Final[Optional[type]]
    >>>  58  |    _module_name: Final[str]
         59  |    _raw_meta: Final[Tuple[Any, Any]]
         60  |
         61  |    @_beartype.beartype

  Error (MYPY) [misc]
    Final name must be initialized with a value

         56  |
         57  |    _module_class: Final[Optional[type]]
         58  |    _module_name: Final[str]
    >>>  59  |    _raw_meta: Final[Tuple[Any, Any]]
         60  |
         61  |    @_beartype.beartype
         62  |    def __init__(

  Error (MYPY) [misc]
    Final name must be initialized with a value

        204  |            }
        205  |    """
        206  |
    >>> 207  |    _module_stack: Final[List[_ModuleMeta]]
        208  |
        209  |    @_beartype.beartype
        210  |    def __init__(



>>> Lint for torch/sparse/semi_structured.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, Any, int, Any]";
    expected "list[int]"

        282  |        to_pad_m = -m % min_rows if m < min_rows or m % min_rows else 0
        283  |        to_pad_n = -n % min_cols if n < min_cols or n % min_rows else 0
        284  |        if to_pad_m or to_pad_n:
    >>> 285  |            return torch.nn.functional.pad(dense_input, (0, to_pad_n, 0, to_pad_m))
        286  |        else:
        287  |            return dense_input
        288  |



>>> Lint for torch/utils/_content_store.py:

  Error (MYPY) [arg-type]
    Argument 2 to "pad" has incompatible type "tuple[int, int]"; expected
    "list[int]"

        127  |        # though it could be profitably fused
        128  |        pad = -x.numel() % 4
        129  |        if pad > 0:
    >>> 130  |            x = F.pad(x, (0, pad), "constant", 0)
        131  |        x = x.view(torch.int32)
        132  |        # We run the 32-bit hash five times with differing parameters to
        133  |        # reduce chance of collision

Successfully applied all patches.
