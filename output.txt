[2024-02-26 13:48:37,840] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo start tracing forward /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:47
[2024-02-26 13:48:37,841] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] create_env
[2024-02-26 13:48:37,843] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:47 in forward (TestModule.forward)
[2024-02-26 13:48:37,843] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         def forward(self, x3):
[2024-02-26 13:48:37,845] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RESUME 0 []
[2024-02-26 13:48:37,845] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:48 in forward (TestModule.forward)
[2024-02-26 13:48:37,845] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             unsafe_alloc_storage(self.x1)
[2024-02-26 13:48:37,845] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL unsafe_alloc_storage []
[2024-02-26 13:48:37,964] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [NullVariable, UserFunctionVariable()]
[2024-02-26 13:48:37,964] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR x1 [NullVariable, UserFunctionVariable(), LazyVariableTracker()]
[2024-02-26 13:48:37,966] [0/0] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['self'].x1 (4, 4) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], constraint_sizes=[None, None], tensor_source=NNModuleSource(base=AttrSource(base=LocalSource(local_name='self', cell_or_freevar=False), member='x1', get_static=False)), shape_env_to_source_to_symbol_cache={}) <class 'torch.nn.parameter.Parameter'>
[2024-02-26 13:48:37,967] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, UserFunctionVariable(), TensorVariable()]
[2024-02-26 13:48:37,967] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, UserFunctionVariable(), TensorVariable()]
[2024-02-26 13:48:37,967] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call unsafe_alloc_storage from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:48 in forward (TestModule.forward)
[2024-02-26 13:48:37,967] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         unsafe_alloc_storage(self.x1)
[2024-02-26 13:48:37,967] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object unsafe_alloc_storage at 0x7f5f06fb4df0, file "/data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py", line 23>
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]  23           0 RESUME                   0
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG] 
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]  24           2 LOAD_FAST                0 (tensor)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]               4 LOAD_METHOD              0 (untyped_storage)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              26 PRECALL                  0
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              30 CALL                     0
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              40 LOAD_METHOD              1 (resize_)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              62 LOAD_FAST                0 (tensor)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              64 LOAD_METHOD              2 (numel)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              86 PRECALL                  0
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              90 CALL                     0
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             100 LOAD_FAST                0 (tensor)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             102 LOAD_ATTR                3 (itemsize)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             112 BINARY_OP                5 (*)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             116 PRECALL                  1
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             120 CALL                     1
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             130 POP_TOP
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             132 LOAD_CONST               0 (None)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG]             134 RETURN_VALUE
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG] , inlined according trace_rules.lookup
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:23 in unsafe_alloc_storage (unsafe_alloc_storage) (inline depth: 1)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]     def unsafe_alloc_storage(tensor: torch.Tensor) -> None:
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RESUME 0 []
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:24 in unsafe_alloc_storage (unsafe_alloc_storage) (inline depth: 1)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         tensor.untyped_storage().resize_(tensor.numel() * tensor.itemsize)
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor []
[2024-02-26 13:48:37,968] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD untyped_storage [TensorVariable()]
[2024-02-26 13:48:37,969] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 0 [NullVariable, GetAttrVariable(TensorVariable(), untyped_storage)]
[2024-02-26 13:48:37,969] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 0 [NullVariable, GetAttrVariable(TensorVariable(), untyped_storage)]
[2024-02-26 13:48:37,969] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD resize_ [UntypedStorageVariable()]
[2024-02-26 13:48:37,969] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_)]
[2024-02-26 13:48:37,969] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD numel [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), TensorVariable()]
[2024-02-26 13:48:37,970] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 0 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), NullVariable, GetAttrVariable(TensorVariable(), numel)]
[2024-02-26 13:48:37,970] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 0 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), NullVariable, GetAttrVariable(TensorVariable(), numel)]
[2024-02-26 13:48:37,970] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), ConstantVariable(int: 16)]
[2024-02-26 13:48:37,970] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR itemsize [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), ConstantVariable(int: 16), TensorVariable()]
[2024-02-26 13:48:37,970] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call getattr_1 from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:24 in unsafe_alloc_storage (unsafe_alloc_storage) (inline depth: 1)
[2024-02-26 13:48:37,970] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     tensor.untyped_storage().resize_(tensor.numel() * tensor.itemsize)
[2024-02-26 13:48:37,970] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]                                                       ^^^^^^^^^^^^^^^
[2024-02-26 13:48:37,971] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_OP 5 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), ConstantVariable(int: 16), ConstantVariable(int: 4)]
[2024-02-26 13:48:37,971] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), ConstantVariable(int: 64)]
[2024-02-26 13:48:37,971] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), ConstantVariable(int: 64)]
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call resize_storage_bytes_ from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:24 in unsafe_alloc_storage (unsafe_alloc_storage) (inline depth: 1)
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     tensor.untyped_storage().resize_(tensor.numel() * tensor.itemsize)
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [UntypedStorageVariable()]
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None []
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert: [DEBUG] DONE INLINING <code object unsafe_alloc_storage at 0x7f5f06fb4df0, file "/data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py", line 23>
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward (TestModule.forward)
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             x5 = self.x1[:]
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self []
[2024-02-26 13:48:37,972] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR x1 [NNModuleVariable()]
[2024-02-26 13:48:37,973] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None [TensorVariable()]
[2024-02-26 13:48:37,973] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None [TensorVariable(), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,973] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BUILD_SLICE 2 [TensorVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,973] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_SUBSCR None [TensorVariable(), SliceVariable()]
[2024-02-26 13:48:37,973] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call getitem from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward (TestModule.forward)
[2024-02-26 13:48:37,973] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]         x5 = self.x1[:]
[2024-02-26 13:48:37,973] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]              ~~~~~~~^^^
[2024-02-26 13:48:37,975] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST x5 [TensorVariable()]
[2024-02-26 13:48:37,975] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:50 in forward (TestModule.forward)
[2024-02-26 13:48:37,975] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             with torch.no_grad():
[2024-02-26 13:48:37,976] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch []
[2024-02-26 13:48:37,976] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR no_grad [NullVariable, PythonModuleVariable(<module 'torch' from '/data/users/willfeng/pytorch_yf225/torch/__init__.py'>)]
[2024-02-26 13:48:37,976] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 0 [NullVariable, TorchCtxManagerClassVariable(<class 'torch.autograd.grad_mode.no_grad'>)]
[2024-02-26 13:48:37,976] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 0 [NullVariable, TorchCtxManagerClassVariable(<class 'torch.autograd.grad_mode.no_grad'>)]
[2024-02-26 13:48:37,977] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BEFORE_WITH None [GradModeVariable()]
[2024-02-26 13:48:37,977] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,977] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:51 in forward (TestModule.forward)
[2024-02-26 13:48:37,977] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]                 torch._foreach_copy_([x5], [x3])
[2024-02-26 13:48:37,977] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL torch [WithExitFunctionVariable()]
[2024-02-26 13:48:37,977] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR _foreach_copy_ [WithExitFunctionVariable(), NullVariable, PythonModuleVariable(<module 'torch' from '/data/users/willfeng/pytorch_yf225/torch/__init__.py'>)]
[2024-02-26 13:48:37,978] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x5 [WithExitFunctionVariable(), NullVariable, TorchInGraphFunctionVariable(<built-in method _foreach_copy_ of type object at 0x7f5f060dcb20>)]
[2024-02-26 13:48:37,978] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BUILD_LIST 1 [WithExitFunctionVariable(), NullVariable, TorchInGraphFunctionVariable(<built-in method _foreach_copy_ of type object at 0x7f5f060dcb20>), TensorVariable()]
[2024-02-26 13:48:37,978] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x3 [WithExitFunctionVariable(), NullVariable, TorchInGraphFunctionVariable(<built-in method _foreach_copy_ of type object at 0x7f5f060dcb20>), ListVariable()]
[2024-02-26 13:48:37,978] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BUILD_LIST 1 [WithExitFunctionVariable(), NullVariable, TorchInGraphFunctionVariable(<built-in method _foreach_copy_ of type object at 0x7f5f060dcb20>), ListVariable(), LazyVariableTracker()]
[2024-02-26 13:48:37,978] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 2 [WithExitFunctionVariable(), NullVariable, TorchInGraphFunctionVariable(<built-in method _foreach_copy_ of type object at 0x7f5f060dcb20>), ListVariable(), ListVariable()]
[2024-02-26 13:48:37,978] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 2 [WithExitFunctionVariable(), NullVariable, TorchInGraphFunctionVariable(<built-in method _foreach_copy_ of type object at 0x7f5f060dcb20>), ListVariable(), ListVariable()]
[2024-02-26 13:48:37,979] [0/0] torch._dynamo.output_graph: [DEBUG] create_graph_input L_x3_ L['x3']
[2024-02-26 13:48:37,979] [0/0] torch._dynamo.variables.builder: [DEBUG] wrap_to_fake L['x3'] (4, 4) StatefulSymbolicContext(dynamic_sizes=[<DimDynamic.STATIC: 2>, <DimDynamic.STATIC: 2>], constraint_sizes=[None, None], tensor_source=LocalSource(local_name='x3', cell_or_freevar=False), shape_env_to_source_to_symbol_cache={}) <class 'torch.Tensor'>
[2024-02-26 13:48:37,980] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call _foreach_copy_ from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:51 in forward (TestModule.forward)
[2024-02-26 13:48:37,980] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]             torch._foreach_copy_([x5], [x3])
[2024-02-26 13:48:37,980] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]             ~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
[2024-02-26 13:48:37,981] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,981] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:50 in forward (TestModule.forward)
[2024-02-26 13:48:37,981] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             with torch.no_grad():
[2024-02-26 13:48:37,981] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None [WithExitFunctionVariable()]
[2024-02-26 13:48:37,981] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None [WithExitFunctionVariable(), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None [WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 2 [WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 2 [WithExitFunctionVariable(), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None), ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE JUMP_FORWARD 204 []
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward (TestModule.forward)
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             out = x5 * x5
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x5 []
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST x5 [TensorVariable()]
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE BINARY_OP 5 [TensorVariable(), TensorVariable()]
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call mul from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward (TestModule.forward)
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]         out = x5 * x5
[2024-02-26 13:48:37,982] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]               ~~~^~~~
[2024-02-26 13:48:37,984] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE STORE_FAST out [TensorVariable()]
[2024-02-26 13:48:37,984] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:53 in forward (TestModule.forward)
[2024-02-26 13:48:37,984] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             unsafe_free_storage(self.x1)
[2024-02-26 13:48:37,984] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_GLOBAL unsafe_free_storage []
[2024-02-26 13:48:37,985] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST self [NullVariable, UserFunctionVariable()]
[2024-02-26 13:48:37,985] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_ATTR x1 [NullVariable, UserFunctionVariable(), NNModuleVariable()]
[2024-02-26 13:48:37,985] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, UserFunctionVariable(), TensorVariable()]
[2024-02-26 13:48:37,985] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, UserFunctionVariable(), TensorVariable()]
[2024-02-26 13:48:37,985] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG] TRACE inlined call unsafe_free_storage from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:53 in forward (TestModule.forward)
[2024-02-26 13:48:37,985] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         unsafe_free_storage(self.x1)
[2024-02-26 13:48:37,985] [0/0] torch._dynamo.symbolic_convert.__trace_call: [DEBUG]         ~~~~~~~~~~~~~~~~~~~^^^^^^^^^
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG] INLINING <code object unsafe_free_storage at 0x7f5f06f73220, file "/data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py", line 27>
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]  27           0 RESUME                   0
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG] 
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]  28           2 LOAD_FAST                0 (tensor)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]               4 LOAD_METHOD              0 (untyped_storage)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              26 PRECALL                  0
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              30 CALL                     0
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              40 LOAD_METHOD              1 (resize_)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              62 LOAD_CONST               1 (0)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              64 PRECALL                  1
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              68 CALL                     1
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              78 POP_TOP
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              80 LOAD_CONST               0 (None)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG]              82 RETURN_VALUE
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG] , inlined according trace_rules.lookup
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:27 in unsafe_free_storage (unsafe_free_storage) (inline depth: 1)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]     def unsafe_free_storage(tensor: torch.Tensor) -> None:
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RESUME 0 []
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:28 in unsafe_free_storage (unsafe_free_storage) (inline depth: 1)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]         tensor.untyped_storage().resize_(0)
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST tensor []
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD untyped_storage [TensorVariable()]
[2024-02-26 13:48:37,986] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 0 [NullVariable, GetAttrVariable(TensorVariable(), untyped_storage)]
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 0 [NullVariable, GetAttrVariable(TensorVariable(), untyped_storage)]
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_METHOD resize_ [UntypedStorageVariable()]
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST 0 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_)]
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE PRECALL 1 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), ConstantVariable(int: 0)]
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE CALL 1 [NullVariable, GetAttrVariable(UntypedStorageVariable(), resize_), ConstantVariable(int: 0)]
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG] TRACE FX call resize_storage_bytes__1 from /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:28 in unsafe_free_storage (unsafe_free_storage) (inline depth: 1)
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     tensor.untyped_storage().resize_(0)
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.output_graph.__trace_call: [DEBUG]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^
[2024-02-26 13:48:37,987] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [UntypedStorageVariable()]
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_CONST None []
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [DEBUG] DONE INLINING <code object unsafe_free_storage at 0x7f5f06f73220, file "/data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py", line 27>
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE POP_TOP None [ConstantVariable(NoneType: None)]
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG] TRACE starts_line /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:54 in forward (TestModule.forward)
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert.__trace_source: [DEBUG]             return out
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE LOAD_FAST out []
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [DEBUG] TRACE RETURN_VALUE None [TensorVariable()]
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [INFO] Step 1: torchdynamo done tracing forward (RETURN_VALUE)
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.symbolic_convert: [DEBUG] RETURN_VALUE triggered compile
[2024-02-26 13:48:37,988] [0/0] torch._dynamo.output_graph: [DEBUG] COMPILING GRAPH due to GraphCompileReason(reason='return_value', user_stack=[<FrameSummary file /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py, line 54 in forward>], graph_break=False)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG] TRACED GRAPH
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  ===== __compiled_fn_0 =====
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]  /data/users/willfeng/pytorch_yf225/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]     def forward(self, L_x3_ : torch.Tensor):
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l_x3_ = L_x3_
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:48 in forward, code: unsafe_alloc_storage(self.x1)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         l__self___x1 = self.L__self___x1
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:24 in unsafe_alloc_storage, code: tensor.untyped_storage().resize_(tensor.numel() * tensor.itemsize)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         getattr_1 = l__self___x1.itemsize
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         resize_storage_bytes_ = torch.ops.inductor.resize_storage_bytes_(l__self___x1, 64)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward, code: x5 = self.x1[:]
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         x5 = l__self___x1[slice(None, None, None)]
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # No stacktrace found for following nodes
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         _set_grad_enabled = torch._C._set_grad_enabled(False)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:51 in forward, code: torch._foreach_copy_([x5], [x3])
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         _foreach_copy_ = torch._foreach_copy_([x5], [l_x3_]);  l_x3_ = None
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # No stacktrace found for following nodes
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         _set_grad_enabled_1 = torch._C._set_grad_enabled(True)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward, code: out = x5 * x5
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         out = x5 * x5;  x5 = None
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:28 in unsafe_free_storage, code: tensor.untyped_storage().resize_(0)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         resize_storage_bytes__1 = torch.ops.inductor.resize_storage_bytes_(l__self___x1, 0);  l__self___x1 = None
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         return (out,)
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG]         
[2024-02-26 13:48:37,989] [0/0] torch._dynamo.output_graph.__graph_code: [DEBUG] 
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] TRACED GRAPH
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG]  __compiled_fn_0 /data/users/willfeng/pytorch_yf225/torch/fx/_lazy_graph_module.py opcode         name                     target                                                             args                                     kwargs
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] -------------  -----------------------  -----------------------------------------------------------------  ---------------------------------------  --------
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] placeholder    l_x3_                    L_x3_                                                              ()                                       {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] get_attr       l__self___x1             L__self___x1                                                       ()                                       {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  getattr_1                <built-in function getattr>                                        (l__self___x1, 'itemsize')               {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  resize_storage_bytes_    inductor.resize_storage_bytes_                                     (l__self___x1, 64)                       {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  x5                       <built-in function getitem>                                        (l__self___x1, slice(None, None, None))  {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  _set_grad_enabled        <built-in function _set_grad_enabled>                              (False,)                                 {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  _foreach_copy_           <built-in method _foreach_copy_ of type object at 0x7f5f060dcb20>  ([x5], [l_x3_])                          {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  _set_grad_enabled_1      <built-in function _set_grad_enabled>                              (True,)                                  {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  out                      <built-in function mul>                                            (x5, x5)                                 {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] call_function  resize_storage_bytes__1  inductor.resize_storage_bytes_                                     (l__self___x1, 0)                        {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] output         output                   output                                                             ((out,),)                                {}
[2024-02-26 13:48:37,990] [0/0] torch._dynamo.output_graph.__graph: [DEBUG] 
[2024-02-26 13:48:37,996] [0/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] TRACED GRAPH TENSOR SIZES
[2024-02-26 13:48:37,996] [0/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] ===== __compiled_fn_0 =====
[2024-02-26 13:48:37,996] [0/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l_x3_: (4, 4)
[2024-02-26 13:48:37,996] [0/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] l__self___x1: (4, 4)
[2024-02-26 13:48:37,996] [0/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] x5: (4, 4)
[2024-02-26 13:48:37,996] [0/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] out: (4, 4)
[2024-02-26 13:48:37,996] [0/0] torch._dynamo.output_graph.__graph_sizes: [DEBUG] 
[2024-02-26 13:48:37,997] [0/0] torch._dynamo.output_graph: [INFO] Step 2: calling compiler function aot_eager
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO] TRACED GRAPH
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]  ===== Joint graph 0 =====
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]  /data/users/willfeng/pytorch_yf225/torch/fx/_lazy_graph_module.py class joint_helper(torch.nn.Module):
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]     def forward(self, primals, tangents):
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         primals_1: "f32[4, 4]"; primals_2: "f32[4, 4]"; tangents_1: "f32[4, 4]"; tangents_2: "f32[4, 4]"; 
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]     
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         primals_1, primals_2, tangents_1, tangents_2, = fx_pytree.tree_flatten_spec([primals, tangents], self._in_spec)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # No stacktrace found for following nodes
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         clone: "f32[4, 4]" = torch.ops.aten.clone.default(primals_1);  primals_1 = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:24 in unsafe_alloc_storage, code: tensor.untyped_storage().resize_(tensor.numel() * tensor.itemsize)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         resize_storage_bytes_ = torch.ops.inductor.resize_storage_bytes_.default(clone, 64)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward, code: x5 = self.x1[:]
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         slice_1: "f32[4, 4]" = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:51 in forward, code: torch._foreach_copy_([x5], [x3])
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         _foreach_copy = torch.ops.aten._foreach_copy.default([slice_1], [primals_2]);  slice_1 = primals_2 = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         getitem: "f32[4, 4]" = _foreach_copy[0];  _foreach_copy = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         slice_scatter: "f32[4, 4]" = torch.ops.aten.slice_scatter.default(clone, getitem, 0, 0, 9223372036854775807);  clone = getitem = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         slice_2: "f32[4, 4]" = torch.ops.aten.slice.Tensor(slice_scatter, 0, 0, 9223372036854775807)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward, code: out = x5 * x5
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         mul: "f32[4, 4]" = torch.ops.aten.mul.Tensor(slice_2, slice_2)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:28 in unsafe_free_storage, code: tensor.untyped_storage().resize_(0)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         resize_storage_bytes__1 = torch.ops.inductor.resize_storage_bytes_.default(slice_scatter, 0)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward, code: out = x5 * x5
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         mul_1: "f32[4, 4]" = torch.ops.aten.mul.Tensor(tangents_2, slice_2)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         mul_2: "f32[4, 4]" = torch.ops.aten.mul.Tensor(tangents_2, slice_2);  tangents_2 = slice_2 = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward, code: out = x5 * x5
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         add: "f32[4, 4]" = torch.ops.aten.add.Tensor(mul_2, mul_1);  mul_2 = mul_1 = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward, code: x5 = self.x1[:]
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         slice_backward: "f32[4, 4]" = torch.ops.aten.slice_backward.default(add, [4, 4], 0, 0, 9223372036854775807, 1);  add = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward, code: x5 = self.x1[:]
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         add_1: "f32[4, 4]" = torch.ops.aten.add.Tensor(tangents_1, slice_backward);  tangents_1 = slice_backward = None
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         return pytree.tree_unflatten([slice_scatter, mul, add_1, None], self._out_spec)
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO]         
[2024-02-26 13:48:38,097] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_joint_graph: [INFO] 
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO] TRACED GRAPH
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]  ===== Forward graph 0 =====
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]  /data/users/willfeng/pytorch_yf225/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]     def forward(self, primals_1: "f32[4, 4]", primals_2: "f32[4, 4]"):
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # No stacktrace found for following nodes
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         clone: "f32[4, 4]" = torch.ops.aten.clone.default(primals_1);  primals_1 = None
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:24 in unsafe_alloc_storage, code: tensor.untyped_storage().resize_(tensor.numel() * tensor.itemsize)
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         resize_storage_bytes_ = torch.ops.inductor.resize_storage_bytes_.default(clone, 64)
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward, code: x5 = self.x1[:]
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         slice_1: "f32[4, 4]" = torch.ops.aten.slice.Tensor(clone, 0, 0, 9223372036854775807)
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:51 in forward, code: torch._foreach_copy_([x5], [x3])
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         _foreach_copy = torch.ops.aten._foreach_copy.default([slice_1], [primals_2]);  slice_1 = primals_2 = None
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         getitem: "f32[4, 4]" = _foreach_copy[0];  _foreach_copy = None
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         slice_scatter: "f32[4, 4]" = torch.ops.aten.slice_scatter.default(clone, getitem, 0, 0, 9223372036854775807);  clone = getitem = None
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         slice_2: "f32[4, 4]" = torch.ops.aten.slice.Tensor(slice_scatter, 0, 0, 9223372036854775807)
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward, code: out = x5 * x5
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         mul: "f32[4, 4]" = torch.ops.aten.mul.Tensor(slice_2, slice_2);  slice_2 = None
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:28 in unsafe_free_storage, code: tensor.untyped_storage().resize_(0)
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         resize_storage_bytes__1 = torch.ops.inductor.resize_storage_bytes_.default(slice_scatter, 0)
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         return [slice_scatter, mul, slice_scatter]
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,264] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO] 
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO] TRACED GRAPH
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]  ===== Backward graph 0 =====
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]  /data/users/willfeng/pytorch_yf225/torch/fx/_lazy_graph_module.py class GraphModule(torch.nn.Module):
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]     def forward(self, slice_scatter: "f32[4, 4]", tangents_1: "f32[4, 4]", tangents_2: "f32[4, 4]"):
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:51 in forward, code: torch._foreach_copy_([x5], [x3])
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         slice_2: "f32[4, 4]" = torch.ops.aten.slice.Tensor(slice_scatter, 0, 0, 9223372036854775807)
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:28 in unsafe_free_storage, code: tensor.untyped_storage().resize_(0)
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         resize_storage_bytes__1 = torch.ops.inductor.resize_storage_bytes_.default(slice_scatter, 0);  slice_scatter = None
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward, code: out = x5 * x5
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         mul_1: "f32[4, 4]" = torch.ops.aten.mul.Tensor(tangents_2, slice_2);  tangents_2 = slice_2 = None
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:52 in forward, code: out = x5 * x5
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         add: "f32[4, 4]" = torch.ops.aten.add.Tensor(mul_1, mul_1);  mul_1 = None
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward, code: x5 = self.x1[:]
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         slice_backward: "f32[4, 4]" = torch.ops.aten.slice_backward.default(add, [4, 4], 0, 0, 9223372036854775807, 1);  add = None
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         # File: /data/users/willfeng/pytorch_yf225/test_resize_foreach_copy.py:49 in forward, code: x5 = self.x1[:]
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         add_1: "f32[4, 4]" = torch.ops.aten.add.Tensor(tangents_1, slice_backward);  tangents_1 = slice_backward = None
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         return [add_1, None]
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO]         
[2024-02-26 13:48:38,265] [0/0] torch._functorch._aot_autograd.jit_compile_runtime_wrappers.__aot_graphs: [INFO] 
[2024-02-26 13:48:38,269] [0/0] torch._dynamo.output_graph: [INFO] Step 2: done compiler function aot_eager
[2024-02-26 13:48:38,272] [0/0] torch.fx.experimental.symbolic_shapes: [INFO] produce_guards
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x3'].size()[0] 4 None
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x3'].size()[1] 4 None
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x3'].stride()[0] 4 None
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x3'].stride()[1] 1 None
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] track_symint L['x3'].storage_offset() 0 None
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x3'].size()[0] == 4
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x3'].size()[1] == 4
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x3'].stride()[0] == 4
[2024-02-26 13:48:38,273] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x3'].stride()[1] == 1
[2024-02-26 13:48:38,274] [0/0] torch.fx.experimental.symbolic_shapes: [DEBUG] Skipping guard L['x3'].storage_offset() == 0
[2024-02-26 13:48:38,274] [0/0] torch._dynamo.guards.__guards: [DEBUG] GUARDS:
[2024-02-26 13:48:38,274] [0/0] torch._dynamo.guards.__guards: [DEBUG] hasattr(L['x3'], '_dynamo_dynamic_indices') == False          # torch._foreach_copy_([x5], [x3])  # test_resize_foreach_copy.py:51 in forward
[2024-02-26 13:48:38,275] [0/0] torch._dynamo.guards.__guards: [DEBUG] ___check_obj_id(L['self'], 140046114884688)                   # unsafe_alloc_storage(self.x1)  # test_resize_foreach_copy.py:48 in forward
[2024-02-26 13:48:38,275] [0/0] torch._dynamo.guards.__guards: [DEBUG] ___check_type_id(L['self'].training, 8883136)                 # unsafe_alloc_storage(self.x1)  # test_resize_foreach_copy.py:48 in forward
[2024-02-26 13:48:38,276] [0/0] torch._dynamo.guards.__guards: [DEBUG] L['self'].training == True                                    # unsafe_alloc_storage(self.x1)  # test_resize_foreach_copy.py:48 in forward
[2024-02-26 13:48:38,276] [0/0] torch._dynamo.guards.__guards: [DEBUG] utils_device.CURRENT_DEVICE == None                           # _dynamo/output_graph.py:395 in init_ambient_guards
[2024-02-26 13:48:38,276] [0/0] torch._dynamo.guards.__guards: [DEBUG] ___check_current_backend(140043661096656)                     # _dynamo/output_graph.py:401 in init_ambient_guards
[2024-02-26 13:48:38,277] [0/0] torch._dynamo.guards.__guards: [DEBUG] check_tensor(L['x3'], Tensor, DispatchKeySet(CPU, BackendSelect, ADInplaceOrView, AutogradCPU), torch.float32, device=None, requires_grad=True, size=[4, 4], stride=[4, 1])  # torch._foreach_copy_([x5], [x3])  # test_resize_foreach_copy.py:51 in forward
