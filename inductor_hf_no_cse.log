WARNING:__main__:Running smaller batch size=4 for AlbertForMaskedLM, orig batch_size=8
cuda train AlbertForMaskedLM                   1.252x p=0.00
WARNING:__main__:Running smaller batch size=4 for AlbertForQuestionAnswering, orig batch_size=8
cuda train AlbertForQuestionAnswering          1.256x p=0.00
WARNING:__main__:Running smaller batch size=4 for AllenaiLongformerBase, orig batch_size=8
cuda train AllenaiLongformerBase               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <resume in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1848, in forward
    outputs = self.longformer(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1750, in forward
    encoder_outputs = self.encoder(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1294, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1326, in <resume in forward>
    layer_outputs = layer_module(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1249, in forward
    self_attn_outputs = self.attention(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1185, in forward
    self_outputs = self.self(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 574, in forward
    attn_scores = self._sliding_chunks_query_key_matmul(
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 839, in _sliding_chunks_query_key_matmul
    query = self._chunk(query, window_overlap, getattr(self.config, "onnx_export", False))
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 840, in <resume in _sliding_chunks_query_key_matmul>
    key = self._chunk(key, window_overlap, getattr(self.config, "onnx_export", False))
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 840, in <resume in _sliding_chunks_query_key_matmul>
    key = self._chunk(key, window_overlap, getattr(self.config, "onnx_export", False))
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 3047, in forward
    return compiled_fn(full_args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1187, in g
    return f(*args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2072, in runtime_wrapper
    all_outs = call_func_with_args(
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1212, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1187, in g
    return f(*args)
  File "/scratch/voz/work/pytorch/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2314, in forward
    fw_outs = call_func_with_args(
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1212, in call_func_with_args
    out = normalize_as_list(f(args))
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 264, in run
    return model(new_inputs)
  File "/tmp/torchinductor_voz/4n/c4n5tgn3o3uyg7scbhra6zb2gehef2tcz5tl5tvatmd7p2qlhp7c.py", line 182, in call
    buf4 = empty_strided((48, 3, 513, 512), (787968, 262656, 512, 1), device='cuda', dtype=torch.float32)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 146.00 MiB (GPU 0; 39.56 GiB total capacity; 25.64 GiB already allocated; 120.56 MiB free; 38.15 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR
WARNING:__main__:Running smaller batch size=4 for BartForCausalLM, orig batch_size=8
cuda train BartForCausalLM                     1.124x p=0.00
WARNING:__main__:Running smaller batch size=2 for BartForConditionalGeneration, orig batch_size=4
cuda train BartForConditionalGeneration        1.052x p=0.00
WARNING:__main__:Running smaller batch size=16 for BertForMaskedLM, orig batch_size=32
cuda train BertForMaskedLM                     1.139x p=0.00
WARNING:__main__:Running smaller batch size=16 for BertForQuestionAnswering, orig batch_size=32
cuda train BertForQuestionAnswering            1.194x p=0.00
WARNING:__main__:Running smaller batch size=4 for BlenderbotForCausalLM, orig batch_size=32
cuda train BlenderbotForCausalLM               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 492, in <resume in forward_and_backward_pass>
    self.grad_scaler.scale(loss).backward()
  File "/scratch/voz/work/pytorch/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/scratch/voz/work/pytorch/torch/autograd/__init__.py", line 204, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/scratch/voz/work/pytorch/torch/autograd/function.py", line 274, in apply
    return user_fn(self, *args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2528, in backward
    out = call_compiled_backward()
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2499, in call_compiled_backward
    CompiledFunction.compiled_bw = aot_config.bw_compiler(
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/common.py", line 40, in _wrapped_bw_compiler
    return eval_frame.disable(eval_frame.disable(bw_compiler)(*args, **kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 519, in bw_compiler
    return inner_compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 617, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_inductor/debug.py", line 239, in inner
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 198, in compile_fx_inner
    compiled_fn = cudagraphify(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 284, in cudagraphify
    return cudagraphify_fn(model, inputs, static_input_idxs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 347, in cudagraphify_impl
    model(list(static_inputs))
  File "/tmp/torchinductor_voz/3a/c3ap2nfpyra7enfmocza2nxqv5rfkr7viivwpc3ussymvemqxag6.py", line 734, in call
    buf26 = empty_strided((128, 128, 128), (16384, 128, 1), device='cuda', dtype=torch.float32)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 39.56 GiB total capacity; 36.40 GiB already allocated; 4.56 MiB free; 38.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForCausalLM, orig batch_size=256
cuda train BlenderbotSmallForCausalLM          0.996x p=0.01
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForConditionalGeneration, orig batch_size=128
cuda train BlenderbotSmallForConditionalGeneration  1.034x p=0.00
WARNING:__main__:Running smaller batch size=16 for CamemBert, orig batch_size=32
cuda train CamemBert                           1.140x p=0.00
WARNING:__main__:Running smaller batch size=4 for DebertaForMaskedLM, orig batch_size=32
cuda train DebertaForMaskedLM                  ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <resume in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1072, in forward
    outputs = self.deberta(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 372, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 412, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 110, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 269, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 331, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 530, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 318, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1848, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 604, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 564, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 389, in wrapper
    self.output.compile_subgraph(self, reason=reason)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 605, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 651, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 730, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 726, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 1083, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/inductor.py", line 9, in inductor
    return compile_fx(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 534, in compile_fx
    return aot_autograd(
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/common.py", line 59, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 3033, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2686, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1792, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1958, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1274, in aot_dispatch_base
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 502, in fw_compiler
    return inner_compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 617, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_inductor/debug.py", line 239, in inner
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 202, in compile_fx_inner
    device_index=next(iter(graph.device_idxs)),
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
StopIteration: 


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
WARNING:__main__:Running smaller batch size=8 for DebertaForQuestionAnswering, orig batch_size=32
cuda train DebertaForQuestionAnswering         ERROR common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <resume in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1401, in forward
    outputs = self.deberta(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 372, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 412, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 110, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 269, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 331, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 530, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 318, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1848, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 604, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 564, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 389, in wrapper
    self.output.compile_subgraph(self, reason=reason)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 605, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 651, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 730, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 726, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 1083, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/inductor.py", line 9, in inductor
    return compile_fx(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 534, in compile_fx
    return aot_autograd(
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/common.py", line 59, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 3033, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2686, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1792, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1958, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1274, in aot_dispatch_base
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 502, in fw_compiler
    return inner_compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 617, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_inductor/debug.py", line 239, in inner
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 202, in compile_fx_inner
    device_index=next(iter(graph.device_idxs)),
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
StopIteration: 


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
WARNING:__main__:Running smaller batch size=1 for DebertaV2ForMaskedLM, orig batch_size=8
cuda train DebertaV2ForMaskedLM                ERROR :common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <resume in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1171, in forward
    outputs = self.deberta(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 372, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 412, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 110, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 269, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 331, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 530, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 318, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1848, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 604, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 564, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 389, in wrapper
    self.output.compile_subgraph(self, reason=reason)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 605, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 651, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 730, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 726, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 1083, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/inductor.py", line 9, in inductor
    return compile_fx(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 534, in compile_fx
    return aot_autograd(
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/common.py", line 59, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 3033, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2686, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1792, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1958, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1274, in aot_dispatch_base
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 502, in fw_compiler
    return inner_compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 617, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_inductor/debug.py", line 239, in inner
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 202, in compile_fx_inner
    device_index=next(iter(graph.device_idxs)),
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
StopIteration: 


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
WARNING:__main__:Running smaller batch size=2 for DebertaV2ForQuestionAnswering, orig batch_size=8
cuda train DebertaV2ForQuestionAnswering       ERROR :common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1376, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 235, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <resume in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 1501, in forward
    outputs = self.deberta(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 372, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 412, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 110, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 269, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 331, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 530, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 318, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1848, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 604, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 564, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 389, in wrapper
    self.output.compile_subgraph(self, reason=reason)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 605, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 651, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 730, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 726, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 1083, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/inductor.py", line 9, in inductor
    return compile_fx(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 534, in compile_fx
    return aot_autograd(
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/common.py", line 59, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 3033, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2686, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1792, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1958, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1274, in aot_dispatch_base
    compiled_fw = aot_config.fw_compiler(fw_module, flat_args)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 502, in fw_compiler
    return inner_compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 617, in debug_wrapper
    compiled_fn = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_inductor/debug.py", line 239, in inner
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/contextlib.py", line 79, in inner
    return func(*args, **kwds)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 202, in compile_fx_inner
    device_index=next(iter(graph.device_idxs)),
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
StopIteration: 


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
WARNING:__main__:Running smaller batch size=128 for DistilBertForMaskedLM, orig batch_size=256
WARNING:__main__:Sequence Length not defined for DistilBertForMaskedLM. Choosing 128 arbitrarily
cuda train DistilBertForMaskedLM               1.016x p=0.00
WARNING:__main__:Running smaller batch size=256 for DistilBertForQuestionAnswering, orig batch_size=512
WARNING:__main__:Sequence Length not defined for DistilBertForQuestionAnswering. Choosing 128 arbitrarily
cuda train DistilBertForQuestionAnswering      1.128x p=0.00
WARNING:__main__:Running smaller batch size=16 for DistillGPT2, orig batch_size=32
cuda train DistillGPT2                         1.345x p=0.00
If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=32 for ElectraForCausalLM, orig batch_size=64
cuda train ElectraForCausalLM                  1.302x p=0.00
WARNING:__main__:Running smaller batch size=64 for ElectraForQuestionAnswering, orig batch_size=128
cuda train ElectraForQuestionAnswering         1.344x p=0.00
WARNING:__main__:Running smaller batch size=4 for GPT2ForSequenceClassification, orig batch_size=8
cuda train GPT2ForSequenceClassification       1.549x p=0.00
WARNING:__main__:Running smaller batch size=16 for GoogleFnet, orig batch_size=32
cuda train GoogleFnet                          1.427x p=0.00
WARNING:__main__:Running smaller batch size=16 for LayoutLMForMaskedLM, orig batch_size=32
cuda train LayoutLMForMaskedLM                 1.130x p=0.00
WARNING:__main__:Running smaller batch size=16 for LayoutLMForSequenceClassification, orig batch_size=32
cuda train LayoutLMForSequenceClassification   1.206x p=0.00
WARNING:__main__:Running smaller batch size=16 for M2M100ForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for M2M100ForConditionalGeneration. Choosing 128 arbitrarily
cuda train M2M100ForConditionalGeneration      1.005x SAME
WARNING:__main__:Running smaller batch size=4 for MBartForCausalLM, orig batch_size=8
cuda train MBartForCausalLM                    1.119x p=0.00
WARNING:__main__:Running smaller batch size=2 for MBartForConditionalGeneration, orig batch_size=4
cuda train MBartForConditionalGeneration       1.050x p=0.00
WARNING:__main__:Running smaller batch size=16 for MT5ForConditionalGeneration, orig batch_size=32
WARNING:__main__:Sequence Length not defined for MT5ForConditionalGeneration. Choosing 128 arbitrarily
cuda train MT5ForConditionalGeneration         1.146x p=0.00
If you want to use `MegatronBertForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=4 for MegatronBertForCausalLM, orig batch_size=16
cuda train MegatronBertForCausalLM             1.035x p=0.00
WARNING:__main__:Running smaller batch size=8 for MegatronBertForQuestionAnswering, orig batch_size=16
cuda train MegatronBertForQuestionAnswering    1.121x p=0.00
WARNING:__main__:Running smaller batch size=64 for MobileBertForMaskedLM, orig batch_size=256
cuda train MobileBertForMaskedLM               0.919x p=0.00
WARNING:__main__:Running smaller batch size=128 for MobileBertForQuestionAnswering, orig batch_size=256
cuda train MobileBertForQuestionAnswering      0.973x SAME
WARNING:__main__:Running smaller batch size=2 for OPTForCausalLM, orig batch_size=4
cuda train OPTForCausalLM                     ERROR  Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 593, in <module>
    huggingface_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 589, in huggingface_main
    main(HuggingfaceRunner())
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2308, in run
    runner.run_one_model(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1473, in run_one_model
    status = self.run_performance_test(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1446, in run_performance_test
    results.append(experiment(model, example_inputs, **experiment_kwargs))
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 618, in speedup_experiment
    timings[rep, 0], expected_output = timed(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 426, in timed
    result = model_iter_fn(model, example_inputs, collect_outputs=collect_outputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in forward_and_backward_pass
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 930, in forward
    outputs = self.model.decoder(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 696, in forward
    layer_outputs = decoder_layer(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 326, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 221, in forward
    attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB (GPU 0; 39.56 GiB total capacity; 22.26 GiB already allocated; 50.56 MiB free; 38.25 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR
WARNING:__main__:Running smaller batch size=8 for PLBartForCausalLM, orig batch_size=16
cuda train PLBartForCausalLM                   1.216x p=0.00
WARNING:__main__:Running smaller batch size=4 for PLBartForConditionalGeneration, orig batch_size=8
cuda train PLBartForConditionalGeneration      1.200x p=0.00
WARNING:__main__:Running smaller batch size=32 for PegasusForCausalLM, orig batch_size=128
WARNING:__main__:Sequence Length not defined for PegasusForCausalLM. Choosing 128 arbitrarily
cuda train PegasusForCausalLM                  0.995x SAME
WARNING:__main__:Running smaller batch size=32 for PegasusForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for PegasusForConditionalGeneration. Choosing 128 arbitrarily
cuda train PegasusForConditionalGeneration     1.018x p=0.01
If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=16 for RobertaForCausalLM, orig batch_size=32
cuda train RobertaForCausalLM                  1.208x p=0.00
WARNING:__main__:Running smaller batch size=16 for RobertaForQuestionAnswering, orig batch_size=32
cuda train RobertaForQuestionAnswering         1.195x p=0.00
WARNING:__main__:Running smaller batch size=256 for Speech2Text2ForCausalLM, orig batch_size=1024
WARNING:__main__:Sequence Length not defined for Speech2Text2ForCausalLM. Choosing 128 arbitrarily
cuda train Speech2Text2ForCausalLM             1.098x p=0.00
WARNING:__main__:Running smaller batch size=4 for T5ForConditionalGeneration, orig batch_size=8
cuda train T5ForConditionalGeneration          1.161x p=0.00
WARNING:__main__:Running smaller batch size=4 for T5Small, orig batch_size=8
cuda train T5Small                             1.157x p=0.00
WARNING:__main__:Running smaller batch size=32 for TrOCRForCausalLM, orig batch_size=64
cuda train TrOCRForCausalLM                    ERROR Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 593, in <module>
    huggingface_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 589, in huggingface_main
    main(HuggingfaceRunner())
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1919, in main
    return maybe_fresh_cache(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 988, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2308, in run
    runner.run_one_model(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1473, in run_one_model
    status = self.run_performance_test(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1446, in run_performance_test
    results.append(experiment(model, example_inputs, **experiment_kwargs))
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 618, in speedup_experiment
    timings[rep, 0], expected_output = timed(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 426, in timed
    result = model_iter_fn(model, example_inputs, collect_outputs=collect_outputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 492, in forward_and_backward_pass
    self.grad_scaler.scale(loss).backward()
  File "/scratch/voz/work/pytorch/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/scratch/voz/work/pytorch/torch/autograd/__init__.py", line 204, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.54 GiB (GPU 0; 39.56 GiB total capacity; 28.13 GiB already allocated; 640.56 MiB free; 37.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR
WARNING:__main__:Running smaller batch size=8 for XGLMForCausalLM, orig batch_size=32
WARNING:__main__:Sequence Length not defined for XGLMForCausalLM. Choosing 128 arbitrarily
cuda train XGLMForCausalLM                     1.059x p=0.00
WARNING:__main__:Running smaller batch size=8 for XLNetLMHeadModel, orig batch_size=16
cuda train XLNetLMHeadModel                    1.594x p=0.00
WARNING:__main__:Running smaller batch size=16 for YituTechConvBert, orig batch_size=32
cuda train YituTechConvBert                    1.129x p=0.00
speedup             gmean=1.13x mean=1.134x
abs_latency         gmean=nanx mean=119.419x
compilation_latency mean=36.017 seconds
compression_ratio   mean=0.850x
eager_peak_mem      gmean=nanx mean=13.388x
dynamo_peak_mem     gmean=nanx mean=15.988x
calls_captured      gmean=nanx mean=637.921x
unique_graphs       gmean=nanx mean=5.211x
graph_breaks        gmean=nanx mean=7.658x
unique_graph_breaks gmean=nanx mean=4.342x
