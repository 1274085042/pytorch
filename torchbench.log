cuda train BERT_pytorch                        TIME SPENT 0.000024TIME SPENT 0.000015TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000831TIME SPENT 0.000019TIME SPENT 0.000012TIME SPENT 0.000482PASS
TIMING: entire_frame_compile:19.15028 backend_compile:14.51345
STATS: call_* op count: 964FakeTensor.__torch_dispatch__:33510 | ProxyTorchDispatchMode.__torch_dispatch__:16047
Dynamo produced 2 graph(s) covering 964 ops
cuda train Background_Matting                  TIME SPENT 0.000064TIME SPENT 0.000069TIME SPENT 0.000013TIME SPENT 0.000044TIME SPENT 0.000111TIME SPENT 0.000032TIME SPENT 0.000026TIME SPENT 0.000623PASS
TIMING: entire_frame_compile:17.12678 backend_compile:10.4795
STATS: call_* op count: 527FakeTensor.__torch_dispatch__:29855 | ProxyTorchDispatchMode.__torch_dispatch__:7609
Dynamo produced 2 graph(s) covering 527 ops
cuda train LearningToPaint                     TIME SPENT 0.000033TIME SPENT 0.000044TIME SPENT 0.000009TIME SPENT 0.000022TIME SPENT 0.000120TIME SPENT 0.000014TIME SPENT 0.000012TIME SPENT 0.000279PASS
TIMING: entire_frame_compile:7.50278 backend_compile:4.03292
STATS: call_* op count: 202FakeTensor.__torch_dispatch__:11196 | ProxyTorchDispatchMode.__torch_dispatch__:2550
Dynamo produced 2 graph(s) covering 202 ops
WARNING:root:Super_SloMo failed to load
Eager model failed to run
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1148, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in forward_and_backward_pass
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/model_wrapper.py", line 34, in forward
    fCoeff = model.getFlowCoeff(trainFrameIndex, I0.device)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/slomo_model.py", line 324, in getFlowCoeff
    C11 = C00 = - (1 - (t[ind])) * (t[ind])
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2286, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 309, in load_model
    self.validate_model(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1150, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

cuda train alexnet                             TIME SPENT 0.000031TIME SPENT 0.000017TIME SPENT 0.000005TIME SPENT 0.000017TIME SPENT 0.000289TIME SPENT 0.000016TIME SPENT 0.000012TIME SPENT 0.000102PASS
TIMING: entire_frame_compile:3.56692 backend_compile:0.79938
STATS: call_* op count: 54FakeTensor.__torch_dispatch__:1929 | ProxyTorchDispatchMode.__torch_dispatch__:547
Dynamo produced 2 graph(s) covering 54 ops
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in <module>
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1918, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 990, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2286, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 273, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/attention_is_all_you_need_pytorch/__init__.py", line 97, in __init__
    train_data, test_data = prepare_dataloaders(self.opt, self.device)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/attention_is_all_you_need_pytorch/train.py", line 333, in prepare_dataloaders
    data = pickle.load(open(opt.data_pkl, 'rb'))
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/dill/_dill.py", line 272, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/dill/_dill.py", line 419, in load
    obj = StockUnpickler.load(self)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/dill/_dill.py", line 409, in find_class
    return StockUnpickler.find_class(self, module, name)
ModuleNotFoundError: No module named 'spacy.lemmatizer'
ERROR
cuda train dcgan                               TIME SPENT 0.000026TIME SPENT 0.000018TIME SPENT 0.000003TIME SPENT 0.000015TIME SPENT 0.000072TIME SPENT 0.000013TIME SPENT 0.000010TIME SPENT 0.000061PASS
TIMING: entire_frame_compile:2.41636 backend_compile:0.80847
STATS: call_* op count: 35FakeTensor.__torch_dispatch__:1785 | ProxyTorchDispatchMode.__torch_dispatch__:417
Dynamo produced 2 graph(s) covering 35 ops
cuda train densenet121                         TIME SPENT 0.000039TIME SPENT 0.000021TIME SPENT 0.000006TIME SPENT 0.000020TIME SPENT 0.002150TIME SPENT 0.000015TIME SPENT 0.000015TIME SPENT 0.001311PASS
TIMING: entire_frame_compile:57.63074 backend_compile:38.18047
STATS: call_* op count: 1159FakeTensor.__torch_dispatch__:66930 | ProxyTorchDispatchMode.__torch_dispatch__:19772
Dynamo produced 2 graph(s) covering 1159 ops
cuda train dlrm                                TIME SPENT 0.000035TIME SPENT 0.000027TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000020TIME SPENT 0.000014PASS
TIMING: entire_frame_compile:1.28826 backend_compile:0.66086
STATS: call_* op count: 40FakeTensor.__torch_dispatch__:1840 | ProxyTorchDispatchMode.__torch_dispatch__:1005
Dynamo produced 1 graph(s) covering 40 ops
cuda train drq                                 TIME SPENT 0.000029TIME SPENT 0.000020TIME SPENT 0.000005TIME SPENT 0.000016TIME SPENT 0.000628TIME SPENT 0.000054TIME SPENT 0.000011TIME SPENT 0.000018TIME SPENT 0.000089PASS
TIMING: entire_frame_compile:5.95919 backend_compile:3.00105
STATS: call_* op count: 97FakeTensor.__torch_dispatch__:3397 | ProxyTorchDispatchMode.__torch_dispatch__:1454
Dynamo produced 5 graph(s) covering 97 ops
cuda train fastNLP_Bert                        TIME SPENT 0.000030TIME SPENT 0.000021TIME SPENT 0.000013TIME SPENT 0.000018TIME SPENT 0.000009TIME SPENT 0.000007TIME SPENT 0.000012TIME SPENT 0.000023TIME SPENT 0.000024TIME SPENT 0.003729TIME SPENT 0.000019TIME SPENT 0.000016TIME SPENT 0.000015TIME SPENT 0.000021TIME SPENT 0.000031TIME SPENT 0.000507[2023-01-27 01:03:21,033] torch._dynamo.utils: [ERROR] RMSE (res-fp64): 0.05833, (ref-fp64): 0.00068 and shape=torch.Size([4, 474])
[2023-01-27 01:03:21,033] torch._dynamo.utils: [ERROR] Accuracy failed for key name pred_start
FAIL
TIMING: entire_frame_compile:18.62836 backend_compile:13.36945
STATS: call_* op count: 1031FakeTensor.__torch_dispatch__:35341 | ProxyTorchDispatchMode.__torch_dispatch__:14321
Dynamo produced 7 graph(s) covering 1031 ops
cuda train hf_Albert                           TIME SPENT 0.000021TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.001099TIME SPENT 0.000012TIME SPENT 0.000090[2023-01-27 01:03:49,229] torch._dynamo.utils: [ERROR] RMSE (res-fp64): 0.11245, (ref-fp64): 0.00019 and shape=torch.Size([4, 512, 30000])
[2023-01-27 01:03:49,229] torch._dynamo.utils: [ERROR] Accuracy failed for key name logits
FAIL
TIMING: entire_frame_compile:15.06524 backend_compile:12.23323
STATS: call_* op count: 642FakeTensor.__torch_dispatch__:21867 | ProxyTorchDispatchMode.__torch_dispatch__:14131
Dynamo produced 2 graph(s) covering 642 ops
cuda train hf_Bart                             ERROR:common:

from user code:
   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 139, in forward
    return super().forward(positions + self.offset)
  File "/scratch/voz/work/pytorch/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1162, in run_node
    return node.target(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 914, in __torch_dispatch__
    return decomposition_table[func](*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_decomp/decompositions.py", line 1054, in embedding
    return weight[indices]
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 936, in __torch_dispatch__
    op_impl_out = op_impl(self, func, *args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 434, in index_tensor
    return run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 421, in run_and_return_new_tensor_of_input_device
    out = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 284, in __call__
    return self._op(*args, **kwargs or {})
  File "/scratch/voz/work/pytorch/torch/_meta_registrations.py", line 1007, in meta_index_Tensor
    check(
  File "/scratch/voz/work/pytorch/torch/_prims_common/__init__.py", line 1551, in check
    raise exc_type(s())
RuntimeError: tensors used as indices must be long, int, byte or bool tensors

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1121, in get_fake_value
    return wrap_fake_exception(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 783, in wrap_fake_exception
    return fn()
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1122, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1174, in run_node
    raise RuntimeError(
RuntimeError: Failed running call_function <function embedding at 0x7fbed46afd90>(*(FakeTensor(FakeTensor(..., device='meta', size=(s0, s1)), cuda:0), FakeTensor(Parameter(FakeTensor(..., device='meta', size=(1026, 768), requires_grad=True)), cuda:0), None, None, 2.0, False, False), **{}):
tensors used as indices must be long, int, byte or bool tensors
(scroll up for backtrace)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 332, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 403, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 261, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 160, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 323, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 310, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1717, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/misc.py", line 670, in call_function
    return self.obj.call_method(tx, self.name, args, kwargs).add_options(self)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/misc.py", line 77, in call_method
    ).call_function(tx, [self.objvar] + args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 259, in call_function
    return super(UserFunctionVariable, self).call_function(tx, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 92, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/torch.py", line 471, in call_function
    tensor_variable = wrap_fx_proxy(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 761, in wrap_fx_proxy
    return wrap_fx_proxy_cls(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 796, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1141, in get_fake_value
    raise TorchRuntimeError() from e
torch._dynamo.exc.TorchRuntimeError: 

from user code:
   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 139, in forward
    return super().forward(positions + self.offset)
  File "/scratch/voz/work/pytorch/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:0.5869
STATS: call_* op count: 0FakeTensor.__torch_dispatch__:62
Dynamo produced 0 graph(s) covering 0 ops
cuda train hf_Bert                             TIME SPENT 0.000023TIME SPENT 0.000016TIME SPENT 0.000003TIME SPENT 0.000014TIME SPENT 0.004436TIME SPENT 0.000012TIME SPENT 0.000414PASS
TIMING: entire_frame_compile:17.4832 backend_compile:12.5568
STATS: call_* op count: 970FakeTensor.__torch_dispatch__:34827 | ProxyTorchDispatchMode.__torch_dispatch__:13339
Dynamo produced 2 graph(s) covering 970 ops
cuda train hf_BigBird                          WARNING:torch.fx.experimental.symbolic_shapes:Failing guard allocated at: 
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in <module>
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1918, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 990, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2320, in run
    runner.run_one_model(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1510, in run_one_model
    status = self.check_accuracy(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 332, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 403, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 261, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 160, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 323, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 310, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1717, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 138, in impl
    self.push(fn_var.call_function(self, self.popn(nargs), {}))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builtin.py", line 288, in call_function
    return wrap_fx_proxy(tx, proxy, **options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 761, in wrap_fx_proxy
    return wrap_fx_proxy_cls(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 796, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1121, in get_fake_value
    return wrap_fake_exception(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 783, in wrap_fake_exception
    return fn()
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1122, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1162, in run_node
    return node.target(*args, **kwargs)

ERROR:common:s2 (could be from ['self.bert.embeddings.token_type_ids.size()[1]']) not in defaultdict(<class 'list'>, {s0: [TensorPropertySource(base=LocalSource(local_name='input_ids'), prop=<TensorProperty.SIZE: 0>, idx=0)], s1: [TensorPropertySource(base=LocalSource(local_name='input_ids'), prop=<TensorProperty.SIZE: 0>, idx=1), TensorPropertySource(base=LocalSource(local_name='input_ids'), prop=<TensorProperty.STRIDE: 1>, idx=0)]})

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 332, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 403, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 261, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 160, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 363, in _compile
    check_fn = CheckFunctionManager(
  File "/scratch/voz/work/pytorch/torch/_dynamo/guards.py", line 548, in __init__
    guard.create(local_builder, global_builder)
  File "/scratch/voz/work/pytorch/torch/_guards.py", line 163, in create
    return self.create_fn(self.source.select(local_builder, global_builder), self)
  File "/scratch/voz/work/pytorch/torch/_dynamo/guards.py", line 402, in SHAPE_ENV
    code = output_graph.shape_env.codegen_guards(
  File "/scratch/voz/work/pytorch/torch/fx/experimental/symbolic_shapes.py", line 909, in codegen_guards
    exprs.append(ShapeGuardPrinter(symbol_to_source, source_ref).doprint(g))
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/sympy-1.11.1-py3.10.egg/sympy/printing/printer.py", line 292, in doprint
    return self._str(self._print(expr))
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/sympy-1.11.1-py3.10.egg/sympy/printing/printer.py", line 331, in _print
    return printmethod(expr, **kwargs)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/sympy-1.11.1-py3.10.egg/sympy/printing/str.py", line 777, in _print_Relational
    return '%s(%s, %s)' % (charmap[expr.rel_op], self._print(expr.lhs),
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/sympy-1.11.1-py3.10.egg/sympy/printing/printer.py", line 331, in _print
    return printmethod(expr, **kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/experimental/symbolic_shapes.py", line 613, in _print_Symbol
    assert expr in self.symbol_to_source, (
AssertionError: s2 (could be from ['self.bert.embeddings.token_type_ids.size()[1]']) not in defaultdict(<class 'list'>, {s0: [TensorPropertySource(base=LocalSource(local_name='input_ids'), prop=<TensorProperty.SIZE: 0>, idx=0)], s1: [TensorPropertySource(base=LocalSource(local_name='input_ids'), prop=<TensorProperty.SIZE: 0>, idx=1), TensorPropertySource(base=LocalSource(local_name='input_ids'), prop=<TensorProperty.STRIDE: 1>, idx=0)]})

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:0.29831
STATS: call_* op count: 0FakeTensor.__torch_dispatch__:15
Dynamo produced 0 graph(s) covering 0 ops
cuda train hf_DistilBert                       TIME SPENT 0.000028TIME SPENT 0.000014TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000033TIME SPENT 0.000012TIME SPENT 0.000255PASS
TIMING: entire_frame_compile:9.16559 backend_compile:6.48274
STATS: call_* op count: 433FakeTensor.__torch_dispatch__:17257 | ProxyTorchDispatchMode.__torch_dispatch__:6901
Dynamo produced 2 graph(s) covering 433 ops
cuda train hf_GPT2                             TIME SPENT 0.000021TIME SPENT 0.000014TIME SPENT 0.000013TIME SPENT 0.000023TIME SPENT 0.000014TIME SPENT 0.000015TIME SPENT 0.000006TIME SPENT 0.000019TIME SPENT 0.000007TIME SPENT 0.000017TIME SPENT 0.000006TIME SPENT 0.000021TIME SPENT 0.000007TIME SPENT 0.000020TIME SPENT 0.000006TIME SPENT 0.000017TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000013TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000023TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000022TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000020TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000019TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000014TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000015TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000013TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000022TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000020TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000029TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000046TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000022TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000015TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000030TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000016TIME SPENT 0.000008TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000015TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000022TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000022TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000022TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000019TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000022TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000020TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000012TIME SPENT 0.000008TIME SPENT 0.000008TIME SPENT 0.000010TIME SPENT 0.000009TIME SPENT 0.000009TIME SPENT 0.000024TIME SPENT 0.000008TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000033TIME SPENT 0.000007TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000010TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000020TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000009TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000024TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000016TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000009TIME SPENT 0.000008TIME SPENT 0.000009TIME SPENT 0.000009TIME SPENT 0.000005TIME SPENT 0.000007TIME SPENT 0.000008TIME SPENT 0.000005TIME SPENT 0.000027TIME SPENT 0.000009TIME SPENT 0.000008TIME SPENT 0.000006TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000008TIME SPENT 0.000008TIME SPENT 0.000011TIME SPENT 0.000012TIME SPENT 0.000022TIME SPENT 0.000017TIME SPENT 0.000031TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000017TIME SPENT 0.000012TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000010TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000029TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000027TIME SPENT 0.000012TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000161TIME SPENT 0.000011TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000075TIME SPENT 0.000062TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000015TIME SPENT 0.000008TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000028TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000027TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000071TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000070TIME SPENT 0.000010TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000027TIME SPENT 0.000009TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000024TIME SPENT 0.000009TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000070TIME SPENT 0.000010TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000081TIME SPENT 0.000012TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000014TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000034TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000023TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000068TIME SPENT 0.000011TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000071TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000029TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000026TIME SPENT 0.000009TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000028TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000068TIME SPENT 0.000011TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000070TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000013TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000026TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000023TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000067TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000100TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000009TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000032TIME SPENT 0.000010TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000022TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000066TIME SPENT 0.000010TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000070TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000028TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000036TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000066TIME SPENT 0.000012TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000070TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000028TIME SPENT 0.000009TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000023TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000078TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000070TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000009TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000033TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000023TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000066TIME SPENT 0.000011TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000070TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000013TIME SPENT 0.000008TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000042TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000022TIME SPENT 0.000011TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000067TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000078TIME SPENT 0.000010TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000027TIME SPENT 0.000009TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000023TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000068TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000070TIME SPENT 0.000153TIME SPENT 0.000145TIME SPENT 0.000150TIME SPENT 0.000018TIME SPENT 0.000384[2023-01-27 01:05:52,689] torch._dynamo.utils: [ERROR] RMSE (res-fp64): 0.20085, (ref-fp64): 0.00044 and shape=torch.Size([2, 512, 50257])
[2023-01-27 01:05:52,689] torch._dynamo.utils: [ERROR] Accuracy failed for key name logits
FAIL
TIMING: entire_frame_compile:25.28474 backend_compile:12.73485
STATS: call_* op count: 1221FakeTensor.__torch_dispatch__:27425 | ProxyTorchDispatchMode.__torch_dispatch__:9556
Dynamo produced 62 graph(s) covering 1221 ops
cuda train hf_Longformer                       ERROR:common:

from user code:
   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1747, in <graph break in forward>
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 475, in forward
    position_embeddings = self.position_embeddings(position_ids)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1167, in run_node
    return nnmodule(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/scratch/voz/work/pytorch/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 914, in __torch_dispatch__
    return decomposition_table[func](*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_decomp/decompositions.py", line 1054, in embedding
    return weight[indices]
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 936, in __torch_dispatch__
    op_impl_out = op_impl(self, func, *args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 434, in index_tensor
    return run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 421, in run_and_return_new_tensor_of_input_device
    out = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 284, in __call__
    return self._op(*args, **kwargs or {})
  File "/scratch/voz/work/pytorch/torch/_meta_registrations.py", line 1007, in meta_index_Tensor
    check(
  File "/scratch/voz/work/pytorch/torch/_prims_common/__init__.py", line 1551, in check
    raise exc_type(s())
RuntimeError: tensors used as indices must be long, int, byte or bool tensors

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1121, in get_fake_value
    return wrap_fake_exception(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 783, in wrap_fake_exception
    return fn()
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1122, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1174, in run_node
    raise RuntimeError(
RuntimeError: Failed running call_module self_embeddings_position_embeddings(*(FakeTensor(FakeTensor(..., device='meta', size=(s0, s1)), cuda:0),), **{}):
tensors used as indices must be long, int, byte or bool tensors
(scroll up for backtrace)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1849, in forward
    outputs = self.longformer(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1732, in forward
    padding_len, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds = self._pad_to_window_size(
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 332, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 403, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 261, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 160, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 323, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 310, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1717, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 203, in call_function
    return wrap_fx_proxy(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 761, in wrap_fx_proxy
    return wrap_fx_proxy_cls(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 796, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1141, in get_fake_value
    raise TorchRuntimeError() from e
torch._dynamo.exc.TorchRuntimeError: 

from user code:
   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1747, in <graph break in forward>
    embedding_output = self.embeddings(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 475, in forward
    position_embeddings = self.position_embeddings(position_ids)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:0.40519 backend_compile:0.0395
STATS: call_* op count: 5FakeTensor.__torch_dispatch__:2521 | ProxyTorchDispatchMode.__torch_dispatch__:2
Dynamo produced 1 graph(s) covering 5 ops
cuda train hf_Reformer                         TIME SPENT 0.000059TIME SPENT 0.000017TIME SPENT 0.000003TIME SPENT 0.000028TIME SPENT 0.000021TIME SPENT 0.000021TIME SPENT 0.000014TIME SPENT 0.000009TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000013TIME SPENT 0.000024TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000032TIME SPENT 0.000103TIME SPENT 0.000014TIME SPENT 0.000013TIME SPENT 0.000010TIME SPENT 0.000007TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000026TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000008TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000013TIME SPENT 0.000009TIME SPENT 0.000013TIME SPENT 0.000003TIME SPENT 0.000013TIME SPENT 0.000019TIME SPENT 0.000005TIME SPENT 0.000041TIME SPENT 0.000019TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000016TIME SPENT 0.000016TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000038TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000014TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000012TIME SPENT 0.000006TIME SPENT 0.000025TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000023TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000016TIME SPENT 0.000015TIME SPENT 0.000018TIME SPENT 0.000009TIME SPENT 0.000019TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000024TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000014TIME SPENT 0.000030TIME SPENT 0.000015TIME SPENT 0.000039TIME SPENT 0.000007TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000009TIME SPENT 0.000026TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000010TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000012TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000012TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000023TIME SPENT 0.000007TIME SPENT 0.000030TIME SPENT 0.000023TIME SPENT 0.000020TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000016TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000039TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000019TIME SPENT 0.000017TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000013TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000030TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000013TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000023TIME SPENT 0.000006TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000020TIME SPENT 0.000009TIME SPENT 0.000003TIME SPENT 0.000019TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000026TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000008TIME SPENT 0.000003TIME SPENT 0.000035TIME SPENT 0.000015TIME SPENT 0.000041TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000008TIME SPENT 0.000012TIME SPENT 0.000051TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000009TIME SPENT 0.000008TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000022TIME SPENT 0.000015TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000007TIME SPENT 0.000008TIME SPENT 0.000159TIME SPENT 0.000018TIME SPENT 0.000012TIME SPENT 0.000011TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.001033TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000073TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000069TIME SPENT 0.000013TIME SPENT 0.000006TIME SPENT 0.000022TIME SPENT 0.000017TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000017TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000015TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000036TIME SPENT 0.000007TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000014TIME SPENT 0.000010TIME SPENT 0.000012TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000012TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000010TIME SPENT 0.000017TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000092TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000064TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000066TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000011TIME SPENT 0.000014TIME SPENT 0.000008TIME SPENT 0.000003TIME SPENT 0.000014TIME SPENT 0.000014TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000022TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000007TIME SPENT 0.000003TIME SPENT 0.000102TIME SPENT 0.000023TIME SPENT 0.000014TIME SPENT 0.000033TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000017TIME SPENT 0.000008TIME SPENT 0.000023TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000008TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000022TIME SPENT 0.000019TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000086TIME SPENT 0.000004TIME SPENT 0.000019TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000061TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000064TIME SPENT 0.000012TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000016TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000016TIME SPENT 0.000025TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000022TIME SPENT 0.000004TIME SPENT 0.000011TIME SPENT 0.000036TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000013TIME SPENT 0.000010TIME SPENT 0.000013TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000018TIME SPENT 0.000017TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000011TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000106TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000064TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000066TIME SPENT 0.000007TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000014TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000013TIME SPENT 0.000013TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000022TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000011TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000103TIME SPENT 0.000023TIME SPENT 0.000014TIME SPENT 0.000032TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000017TIME SPENT 0.000007TIME SPENT 0.000020TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000008TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000023TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000089TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000062TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000063TIME SPENT 0.000014TIME SPENT 0.000004TIME SPENT 0.000025TIME SPENT 0.000020TIME SPENT 0.000018TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000016TIME SPENT 0.000014TIME SPENT 0.000003TIME SPENT 0.000012TIME SPENT 0.000015TIME SPENT 0.000004TIME SPENT 0.000011TIME SPENT 0.000034TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000014TIME SPENT 0.000009TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000010TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000021TIME SPENT 0.000087TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000064TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000066TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000010TIME SPENT 0.000015TIME SPENT 0.000007TIME SPENT 0.000002TIME SPENT 0.000013TIME SPENT 0.000014TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000021TIME SPENT 0.000017TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000010TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000102TIME SPENT 0.000023TIME SPENT 0.000014TIME SPENT 0.000031TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000017TIME SPENT 0.000007TIME SPENT 0.000020TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000008TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000022TIME SPENT 0.000022TIME SPENT 0.000014TIME SPENT 0.000010TIME SPENT 0.000180PASS
TIMING: entire_frame_compile:34.67499 backend_compile:5.81449
STATS: call_* op count: 673FakeTensor.__torch_dispatch__:17755 | ProxyTorchDispatchMode.__torch_dispatch__:1596
Dynamo produced 62 graph(s) covering 673 ops
cuda train hf_T5                               ERROR:common:

from user code:
   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 521, in forward
    position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 434, in compute_bias
    values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1167, in run_node
    return nnmodule(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/scratch/voz/work/pytorch/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 914, in __torch_dispatch__
    return decomposition_table[func](*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_decomp/decompositions.py", line 1054, in embedding
    return weight[indices]
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 936, in __torch_dispatch__
    op_impl_out = op_impl(self, func, *args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 434, in index_tensor
    return run_and_return_new_tensor_of_input_device(fake_mode, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 421, in run_and_return_new_tensor_of_input_device
    out = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 284, in __call__
    return self._op(*args, **kwargs or {})
  File "/scratch/voz/work/pytorch/torch/_meta_registrations.py", line 1007, in meta_index_Tensor
    check(
  File "/scratch/voz/work/pytorch/torch/_prims_common/__init__.py", line 1551, in check
    raise exc_type(s())
RuntimeError: tensors used as indices must be long, int, byte or bool tensors

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1121, in get_fake_value
    return wrap_fake_exception(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 783, in wrap_fake_exception
    return fn()
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1122, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1174, in run_node
    raise RuntimeError(
RuntimeError: Failed running call_module self_model_encoder_block_0_layer_0_SelfAttention_relative_attention_bias(*(FakeTensor(FakeTensor(..., device='meta', size=(1, s1)), cuda:0),), **{}):
tensors used as indices must be long, int, byte or bool tensors
(scroll up for backtrace)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 332, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 403, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 261, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 160, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 323, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 310, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1717, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function
    return super().call_function(tx, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 259, in call_function
    return super(UserFunctionVariable, self).call_function(tx, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 92, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 203, in call_function
    return wrap_fx_proxy(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 761, in wrap_fx_proxy
    return wrap_fx_proxy_cls(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 796, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1141, in get_fake_value
    raise TorchRuntimeError() from e
torch._dynamo.exc.TorchRuntimeError: 

from user code:
   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 521, in forward
    position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py", line 434, in compute_bias
    values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:0.27896
STATS: call_* op count: 0FakeTensor.__torch_dispatch__:698
Dynamo produced 0 graph(s) covering 0 ops
cuda train maml_omniglot                       TIME SPENT 0.000033TIME SPENT 0.000026TIME SPENT 0.000006TIME SPENT 0.000015TIME SPENT 0.000491TIME SPENT 0.000012TIME SPENT 0.000009TIME SPENT 0.000056PASS
TIMING: entire_frame_compile:4.82484 backend_compile:1.42003
STATS: call_* op count: 42FakeTensor.__torch_dispatch__:2226 | ProxyTorchDispatchMode.__torch_dispatch__:498
Dynamo produced 2 graph(s) covering 42 ops
cuda train mnasnet1_0                          TIME SPENT 0.000039TIME SPENT 0.000021TIME SPENT 0.000006TIME SPENT 0.000023TIME SPENT 0.000115TIME SPENT 0.000017TIME SPENT 0.000016TIME SPENT 0.000551PASS
TIMING: entire_frame_compile:14.24658 backend_compile:8.71134
STATS: call_* op count: 468FakeTensor.__torch_dispatch__:27086 | ProxyTorchDispatchMode.__torch_dispatch__:5815
Dynamo produced 2 graph(s) covering 468 ops
cuda train mobilenet_v2                        TIME SPENT 0.000036TIME SPENT 0.000024TIME SPENT 0.000006TIME SPENT 0.000020TIME SPENT 0.000098TIME SPENT 0.000016TIME SPENT 0.000015TIME SPENT 0.000543PASS
TIMING: entire_frame_compile:31.64891 backend_compile:12.09237
STATS: call_* op count: 469FakeTensor.__torch_dispatch__:27076 | ProxyTorchDispatchMode.__torch_dispatch__:5866
Dynamo produced 2 graph(s) covering 469 ops
cuda train mobilenet_v2_quantized_qat          WARNING:common:fp64 golden ref were not generated for mobilenet_v2_quantized_qat. Setting accuracy check to cosine
ERROR:common:output with shape [1] doesn't match the broadcast shape [32]
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/fx/graph_module.py", line 660, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/graph_module.py", line 279, in __call__
    raise e
  File "/scratch/voz/work/pytorch/torch/fx/graph_module.py", line 269, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.8", line 4, in forward
    def forward(self, x : torch.Tensor) -> torch.Tensor:
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2489, in forward
    return compiled_fn(full_args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 996, in g
    return f(*args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2058, in debug_compiled_function
    return compiled_function(*args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1973, in compiled_function
    original_inpt.copy_(updated_inpt)
RuntimeError: output with shape [1] doesn't match the broadcast shape [32]
TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:24.05808 backend_compile:19.20717
STATS: call_* op count: 203FakeTensor.__torch_dispatch__:52829 | ProxyTorchDispatchMode.__torch_dispatch__:17910
Dynamo produced 1 graph(s) covering 203 ops
cuda train mobilenet_v3_large                  TIME SPENT 0.000034TIME SPENT 0.000023TIME SPENT 0.000006TIME SPENT 0.000021TIME SPENT 0.000100TIME SPENT 0.000014TIME SPENT 0.000016TIME SPENT 0.000649PASS
TIMING: entire_frame_compile:17.9424 backend_compile:11.64825
STATS: call_* op count: 535FakeTensor.__torch_dispatch__:28043 | ProxyTorchDispatchMode.__torch_dispatch__:6450
Dynamo produced 2 graph(s) covering 535 ops
cuda train moco                                TIME SPENT 0.000023TIME SPENT 0.000018TIME SPENT 0.000005TIME SPENT 0.000016TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000023TIME SPENT 0.000009TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000033TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000022TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000018TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000018TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000024TIME SPENT 0.000009TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000020TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000019TIME SPENT 0.000008TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000016TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000018TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000024TIME SPENT 0.000008TIME SPENT 0.000022TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000020TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000023TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000017TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000017TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000014TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000013TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000018TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000013TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000017TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000014TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000022TIME SPENT 0.000009TIME SPENT 0.000025TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000024TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000007TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000009TIME SPENT 0.000009TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000018TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000015TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000013TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000014TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000006TIME SPENT 0.000012TIME SPENT 0.000013TIME SPENT 0.000015TIME SPENT 0.000013TIME SPENT 0.000031TIME SPENT 0.000015TIME SPENT 0.000014TIME SPENT 0.000015TIME SPENT 0.000013TIME SPENT 0.000014TIME SPENT 0.000013TIME SPENT 0.000018TIME SPENT 0.000016TIME SPENT 0.000018TIME SPENT 0.000021TIME SPENT 0.000017TIME SPENT 0.000012TIME SPENT 0.000013TIME SPENT 0.000014TIME SPENT 0.000012TIME SPENT 0.000017TIME SPENT 0.000014TIME SPENT 0.000011TIME SPENT 0.000011TIME SPENT 0.000013TIME SPENT 0.000008TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000013TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000029TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000016TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000013TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000012TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000011TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000019TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000021TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000023TIME SPENT 0.000009TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000016TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000017TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000020TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000014TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004[2023-01-27 01:10:45,666] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)
   function: '<graph break in _momentum_update_key_encoder>' (/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py:50)
   reasons:  ___tuple_iterator_len(___stack0) == 160
to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.
ERROR:common:

from user code:
   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 172, in concat_all_gather
    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 944, in __torch_dispatch__
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 284, in __call__
    return self._op(*args, **kwargs or {})
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 377, in _get_dispatch
    final_key = resolve_key(self, key)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 106, in resolve_key
    raise NotImplementedError(f"could not find kernel for {op} at dispatch key {k}")
NotImplementedError: could not find kernel for c10d.allgather_.default at dispatch key DispatchKey.Meta

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1162, in run_node
    return node.target(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/distributed/distributed_c10d.py", line 1421, in wrapper
    return func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/distributed/distributed_c10d.py", line 2418, in all_gather
    work = default_pg.allgather([tensor_list], [tensor])
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 949, in __torch_dispatch__
    return run_fallback_kernel(self, func, args, kwargs, not_implemented_error)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 1128, in run_fallback_kernel
    args = tree_map(to_real_tensor, args)
  File "/scratch/voz/work/pytorch/torch/utils/_pytree.py", line 196, in tree_map
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File "/scratch/voz/work/pytorch/torch/utils/_pytree.py", line 196, in <listcomp>
    return tree_unflatten([fn(i) for i in flat_args], spec)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 1121, in to_real_tensor
    out = torch.zeros_like(e, device=e.fake_device)
RuntimeError: Cannot call strides() on tensor with symbolic sizes/strides

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1121, in get_fake_value
    return wrap_fake_exception(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 783, in wrap_fake_exception
    return fn()
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1122, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1174, in run_node
    raise RuntimeError(
RuntimeError: Failed running call_function <function all_gather at 0x7f8ce738edd0>(*([FakeTensor(FakeTensor(..., device='meta', size=(s0, s1, s2, s2)), cuda:0)], FakeTensor(FakeTensor(..., device='meta', size=(s0, s1, s2, s2)), cuda:0)), **{'async_op': False}):
Cannot call strides() on tensor with symbolic sizes/strides
(scroll up for backtrace)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/nn/parallel/distributed.py", line 1158, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/scratch/voz/work/pytorch/torch/nn/parallel/distributed.py", line 1111, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 130, in forward
    self._momentum_update_key_encoder()  # update the key encoder
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 133, in <graph break in forward>
    im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)
  File "/scratch/voz/work/pytorch/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 76, in _batch_shuffle_ddp
    x_gather = concat_all_gather(x)
  File "/scratch/voz/work/pytorch/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 329, in catch_errors
    return hijacked_callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 403, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 261, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 160, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 323, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 310, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1717, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1048, in CALL_FUNCTION_KW
    self.call_function(fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/torch.py", line 471, in call_function
    tensor_variable = wrap_fx_proxy(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 761, in wrap_fx_proxy
    return wrap_fx_proxy_cls(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 796, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1141, in get_fake_value
    raise TorchRuntimeError() from e
torch._dynamo.exc.TorchRuntimeError: 

from user code:
   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 172, in concat_all_gather
    torch.distributed.all_gather(tensors_gather, tensor, async_op=False)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:34.3909 backend_compile:9.76269
STATS: call_* op count: 507FakeTensor.__torch_dispatch__:177061 | ProxyTorchDispatchMode.__torch_dispatch__:6523
Dynamo produced 68 graph(s) covering 507 ops
cuda train nvidia_deeprecommender              TIME SPENT 0.000023TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000013TIME SPENT 0.000222TIME SPENT 0.000014TIME SPENT 0.000011TIME SPENT 0.000047PASS
TIMING: entire_frame_compile:1.1507 backend_compile:0.33469
STATS: call_* op count: 37FakeTensor.__torch_dispatch__:1554 | ProxyTorchDispatchMode.__torch_dispatch__:445
Dynamo produced 2 graph(s) covering 37 ops
cuda train pytorch_CycleGAN_and_pix2pix        TIME SPENT 0.000031TIME SPENT 0.000023TIME SPENT 0.000003TIME SPENT 0.000017TIME SPENT 0.000031TIME SPENT 0.000016TIME SPENT 0.000012TIME SPENT 0.000261PASS
TIMING: entire_frame_compile:7.45882 backend_compile:5.4863
STATS: call_* op count: 187FakeTensor.__torch_dispatch__:10704 | ProxyTorchDispatchMode.__torch_dispatch__:3029
Dynamo produced 2 graph(s) covering 187 ops
cuda train pytorch_stargan                     TIME SPENT 0.000022TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000016TIME SPENT 0.000094TIME SPENT 0.000012TIME SPENT 0.000009TIME SPENT 0.000662PASS
TIMING: entire_frame_compile:9.00544 backend_compile:6.58849
STATS: call_* op count: 164FakeTensor.__torch_dispatch__:13283 | ProxyTorchDispatchMode.__torch_dispatch__:3758
Dynamo produced 2 graph(s) covering 164 ops
downloading en-ud-v2.zip
  0%|          | 0.00/688k [00:00<?, ?B/s]100%|██████████| 688k/688k [00:00<00:00, 171MB/s]
extracting
cuda train pytorch_struct                      TIME SPENT 0.000024TIME SPENT 0.000012TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000039TIME SPENT 0.000023TIME SPENT 0.000013TIME SPENT 0.000092PASS
TIMING: entire_frame_compile:1.68944 backend_compile:0.60848
STATS: call_* op count: 99FakeTensor.__torch_dispatch__:4569 | ProxyTorchDispatchMode.__torch_dispatch__:1554
Dynamo produced 2 graph(s) covering 99 ops
cuda train pytorch_unet                        TIME SPENT 0.000026TIME SPENT 0.000018TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.001461TIME SPENT 0.000029TIME SPENT 0.000021TIME SPENT 0.000361PASS
TIMING: entire_frame_compile:28.9385 backend_compile:17.58366
STATS: call_* op count: 418FakeTensor.__torch_dispatch__:21851 | ProxyTorchDispatchMode.__torch_dispatch__:7802
Dynamo produced 3 graph(s) covering 418 ops
cuda train resnet18                            TIME SPENT 0.000035TIME SPENT 0.000023TIME SPENT 0.000006TIME SPENT 0.000022TIME SPENT 0.000102TIME SPENT 0.000015TIME SPENT 0.000015TIME SPENT 0.000278PASS
TIMING: entire_frame_compile:6.08565 backend_compile:3.47153
STATS: call_* op count: 193FakeTensor.__torch_dispatch__:10724 | ProxyTorchDispatchMode.__torch_dispatch__:2423
Dynamo produced 2 graph(s) covering 193 ops
cuda train resnet50                            TIME SPENT 0.000032TIME SPENT 0.000024TIME SPENT 0.000005TIME SPENT 0.000023TIME SPENT 0.000124TIME SPENT 0.000014TIME SPENT 0.000015TIME SPENT 0.000583PASS
TIMING: entire_frame_compile:13.67096 backend_compile:9.06113
STATS: call_* op count: 497FakeTensor.__torch_dispatch__:27824 | ProxyTorchDispatchMode.__torch_dispatch__:6201
Dynamo produced 2 graph(s) covering 497 ops
cuda train resnet50_quantized_qat              WARNING:common:fp64 golden ref were not generated for resnet50_quantized_qat. Setting accuracy check to cosine
ERROR:common:output with shape [1] doesn't match the broadcast shape [64]
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/fx/graph_module.py", line 660, in call_wrapped
    return self._wrapped_call(self, *args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/graph_module.py", line 279, in __call__
    raise e
  File "/scratch/voz/work/pytorch/torch/fx/graph_module.py", line 269, in __call__
    return super(self.cls, obj).__call__(*args, **kwargs)  # type: ignore[misc]
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "<eval_with_key>.8", line 4, in forward
    def forward(self, x : torch.Tensor) -> torch.Tensor:
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2489, in forward
    return compiled_fn(full_args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 996, in g
    return f(*args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2058, in debug_compiled_function
    return compiled_function(*args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1973, in compiled_function
    original_inpt.copy_(updated_inpt)
RuntimeError: output with shape [1] doesn't match the broadcast shape [64]
TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:19.63546 backend_compile:15.90588
STATS: call_* op count: 163FakeTensor.__torch_dispatch__:50929 | ProxyTorchDispatchMode.__torch_dispatch__:18316
Dynamo produced 1 graph(s) covering 163 ops
cuda train resnext50_32x4d                     TIME SPENT 0.000035TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000020TIME SPENT 0.000132TIME SPENT 0.000015TIME SPENT 0.000014TIME SPENT 0.000571PASS
TIMING: entire_frame_compile:13.41568 backend_compile:8.94972
STATS: call_* op count: 497FakeTensor.__torch_dispatch__:27824 | ProxyTorchDispatchMode.__torch_dispatch__:6201
Dynamo produced 2 graph(s) covering 497 ops
cuda train shufflenet_v2_x1_0                  TIME SPENT 0.000035TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000021TIME SPENT 0.000127TIME SPENT 0.000017TIME SPENT 0.000016TIME SPENT 0.000573PASS
TIMING: entire_frame_compile:16.30582 backend_compile:10.65717
STATS: call_* op count: 707FakeTensor.__torch_dispatch__:30416 | ProxyTorchDispatchMode.__torch_dispatch__:7513
Dynamo produced 2 graph(s) covering 707 ops
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in <module>
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1918, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 990, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2286, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 273, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/soft_actor_critic/__init__.py", line 131, in __init__
    self.train_env = load_gym(self.args.env_id, self.args.seed)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/soft_actor_critic/envs.py", line 237, in load_gym
    env.seed(seed)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  [Previous line repeated 1 more time]
AttributeError: 'PendulumEnv' object has no attribute 'seed'
ERROR
cuda train speech_transformer                  TIME SPENT 0.000028[2023-01-27 01:15:29,912] torch._dynamo.variables.builtin: [WARNING] incorrect arg count <bound method BuiltinVariable._call_min_max of BuiltinVariable(max)> missing a required argument: 'b' and no constant handler
TIME SPENT 0.000037TIME SPENT 0.000011TIME SPENT 0.000019TIME SPENT 0.000014TIME SPENT 0.000011TIME SPENT 0.000037TIME SPENT 0.000025TIME SPENT 0.000005TIME SPENT 0.000036TIME SPENT 0.000015TIME SPENT 0.000011TIME SPENT 0.000005TIME SPENT 0.000010TIME SPENT 0.000021TIME SPENT 0.000007TIME SPENT 0.000011TIME SPENT 0.000008TIME SPENT 0.000009TIME SPENT 0.000010TIME SPENT 0.000058TIME SPENT 0.000027TIME SPENT 0.000018TIME SPENT 0.000008TIME SPENT 0.000015TIME SPENT 0.000009TIME SPENT 0.000018TIME SPENT 0.000008TIME SPENT 0.000022TIME SPENT 0.000020TIME SPENT 0.000006TIME SPENT 0.000020TIME SPENT 0.000027TIME SPENT 0.000010TIME SPENT 0.000013TIME SPENT 0.000003TIME SPENT 0.000009TIME SPENT 0.000010TIME SPENT 0.000009TIME SPENT 0.000058TIME SPENT 0.000022TIME SPENT 0.000014TIME SPENT 0.000677PASS
TIMING: entire_frame_compile:23.03419 backend_compile:16.15084
STATS: call_* op count: 1405FakeTensor.__torch_dispatch__:44277 | ProxyTorchDispatchMode.__torch_dispatch__:17659
Dynamo produced 12 graph(s) covering 1405 ops
cuda train squeezenet1_1                       TIME SPENT 0.000034TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000020TIME SPENT 0.000251TIME SPENT 0.000015TIME SPENT 0.000013TIME SPENT 0.000235PASS
TIMING: entire_frame_compile:6.24052 backend_compile:2.8104
STATS: call_* op count: 170FakeTensor.__torch_dispatch__:5527 | ProxyTorchDispatchMode.__torch_dispatch__:1613
Dynamo produced 2 graph(s) covering 170 ops
NUMS 4.0
cuda train tacotron2                           ERROR:common:

from user code:
   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 363, in decode
    self.attention_context, self.attention_weights = self.attention_layer(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 83, in forward
    attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1162, in run_node
    return node.target(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 944, in __torch_dispatch__
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 284, in __call__
    return self._op(*args, **kwargs or {})
  File "/scratch/voz/work/pytorch/torch/_meta_registrations.py", line 1482, in meta_bmm
    return common_meta_baddbmm_bmm(self, mat2, True)
  File "/scratch/voz/work/pytorch/torch/_meta_registrations.py", line 1460, in common_meta_baddbmm_bmm
    check(
  File "/scratch/voz/work/pytorch/torch/_prims_common/__init__.py", line 1551, in check
    raise exc_type(s())
RuntimeError: Expected size for first two dimensions of batch2 tensor to be: [s0, 1] but got: [s0, s6].

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1121, in get_fake_value
    return wrap_fake_exception(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 783, in wrap_fake_exception
    return fn()
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1122, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1174, in run_node
    raise RuntimeError(
RuntimeError: Failed running call_function <built-in method bmm of type object at 0x7fb9cf23ba40>(*(FakeTensor(FakeTensor(..., device='meta', size=(s0, 1, 1), grad_fn=<UnsqueezeBackward0>), cuda:0), FakeTensor(FakeTensor(..., device='meta', size=(s0, s6, 768 - s3),
           grad_fn=<AsStridedBackward0>), cuda:0)), **{}):
Expected size for first two dimensions of batch2 tensor to be: [s0, 1] but got: [s0, s6].
(scroll up for backtrace)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1315, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 211, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1192, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 364, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in <graph break in forward_and_backward_pass>
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 507, in forward
    encoder_outputs = self.encoder(embedded_inputs, text_lengths)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 509, in <graph break in forward>
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 399, in forward
    decoder_inputs = self.parse_decoder_inputs(decoder_inputs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 404, in <graph break in forward>
    memory, mask=~get_mask_from_lengths(memory_lengths))
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 403, in <graph break in forward>
    self.initialize_decoder_states(
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 332, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 403, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 103, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 261, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 160, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 323, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 341, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 310, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1717, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/misc.py", line 670, in call_function
    return self.obj.call_method(tx, self.name, args, kwargs).add_options(self)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 605, in call_method
    return super().call_method(tx, name, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/user_defined.py", line 228, in call_method
    ).call_function(tx, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 289, in call_function
    return super().call_function(tx, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 259, in call_function
    return super(UserFunctionVariable, self).call_function(tx, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/functions.py", line 92, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/nn_module.py", line 229, in call_function
    return tx.inline_user_function_return(
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 496, in inline_user_function_return
    result = InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1795, in inline_call
    return cls.inline_call_(parent, func, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1851, in inline_call_
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 563, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 526, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 332, in wrapper
    return inner_fn(self, inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 999, in CALL_FUNCTION
    self.call_function(fn, args, {})
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 460, in call_function
    self.push(fn.call_function(self, args, kwargs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/torch.py", line 471, in call_function
    tensor_variable = wrap_fx_proxy(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 761, in wrap_fx_proxy
    return wrap_fx_proxy_cls(
  File "/scratch/voz/work/pytorch/torch/_dynamo/variables/builder.py", line 796, in wrap_fx_proxy_cls
    example_value = get_fake_value(proxy.node, tx)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 1141, in get_fake_value
    raise TorchRuntimeError() from e
torch._dynamo.exc.TorchRuntimeError: 

from user code:
   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 363, in decode
    self.attention_context, self.attention_weights = self.attention_layer(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 83, in forward
    attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)

Set torch._dynamo.config.verbose=True for more information


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:4.10725 backend_compile:1.43583
STATS: call_* op count: 77FakeTensor.__torch_dispatch__:2923 | ProxyTorchDispatchMode.__torch_dispatch__:921
Dynamo produced 9 graph(s) covering 77 ops
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in <module>
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1918, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 990, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2286, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 247, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/timm_efficientdet/__init__.py", line 12, in <module>
    from effdet import create_model, create_loader
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/__init__.py", line 2, in <module>
    from .bench import DetBenchPredict, DetBenchTrain, unwrap_bench
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/bench.py", line 70, in <module>
    def _batch_detection(
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1343, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1343, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1343, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1343, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1343, in script
    fn = torch._C._jit_script_compile(
RuntimeError: 
object has no attribute nms:
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 41
        _log_api_usage_once(nms)
    _assert_has_ops()
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
           ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
'nms' is being compiled since it was called from '_batched_nms_vanilla'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 109
    for class_id in torch.unique(idxs):
        curr_indices = torch.where(idxs == class_id)[0]
        curr_keep_indices = nms(boxes[curr_indices], scores[curr_indices], iou_threshold)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        keep_mask[curr_indices[curr_keep_indices]] = True
    keep_indices = torch.where(keep_mask)[0]
'_batched_nms_vanilla' is being compiled since it was called from 'batched_nms'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 73
    # https://github.com/pytorch/vision/issues/1311#issuecomment-781329339
    if boxes.numel() > (4000 if boxes.device.type == "cpu" else 20000) and not torchvision._is_tracing():
        return _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)
               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    else:
        return _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)
'batched_nms' is being compiled since it was called from 'generate_detections'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/anchors.py", line 140
        scores[top_detection_idx] = soft_scores
    else:
        top_detection_idx = batched_nms(boxes, scores, classes, iou_threshold=0.5)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE

    # keep only top max_det_per_image scoring predictions
'generate_detections' is being compiled since it was called from '_batch_detection'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/bench.py", line 82
        img_scale_i = None if img_scale is None else img_scale[i]
        img_size_i = None if img_size is None else img_size[i]
        detections = generate_detections(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            class_out[i], box_out[i], anchor_boxes, indices[i], classes[i],
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            img_scale_i, img_size_i, max_det_per_image=max_det_per_image, soft_nms=soft_nms)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        batch_detections.append(detections)
    return torch.stack(batch_detections, dim=0)

ERROR
cuda train timm_efficientnet                   TIME SPENT 0.000027TIME SPENT 0.000019TIME SPENT 0.000004TIME SPENT 0.000019TIME SPENT 0.003383TIME SPENT 0.000013TIME SPENT 0.000013TIME SPENT 0.000757PASS
TIMING: entire_frame_compile:24.1949 backend_compile:16.50565
STATS: call_* op count: 739FakeTensor.__torch_dispatch__:32067 | ProxyTorchDispatchMode.__torch_dispatch__:8384
Dynamo produced 2 graph(s) covering 739 ops
cuda train timm_regnet                         TIME SPENT 0.000030TIME SPENT 0.000019TIME SPENT 0.000004TIME SPENT 0.000024TIME SPENT 0.005493TIME SPENT 0.000015TIME SPENT 0.000015TIME SPENT 0.001453PASS
TIMING: entire_frame_compile:33.18879 backend_compile:22.09231
STATS: call_* op count: 1378FakeTensor.__torch_dispatch__:53753 | ProxyTorchDispatchMode.__torch_dispatch__:13931
Dynamo produced 2 graph(s) covering 1378 ops
cuda train timm_resnest                        TIME SPENT 0.000028TIME SPENT 0.000017TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.000169TIME SPENT 0.000012TIME SPENT 0.000011TIME SPENT 0.000340PASS
TIMING: entire_frame_compile:8.76005 backend_compile:5.00567
STATS: call_* op count: 346FakeTensor.__torch_dispatch__:14505 | ProxyTorchDispatchMode.__torch_dispatch__:3725
Dynamo produced 2 graph(s) covering 346 ops
cuda train timm_vision_transformer             TIME SPENT 0.000034TIME SPENT 0.000021TIME SPENT 0.000005TIME SPENT 0.000024TIME SPENT 0.000036TIME SPENT 0.000016TIME SPENT 0.000014TIME SPENT 0.000389PASS
TIMING: entire_frame_compile:14.41042 backend_compile:10.14917
STATS: call_* op count: 746FakeTensor.__torch_dispatch__:26005 | ProxyTorchDispatchMode.__torch_dispatch__:10408
Dynamo produced 2 graph(s) covering 746 ops
cuda train timm_vovnet                         TIME SPENT 0.000027TIME SPENT 0.000016TIME SPENT 0.000004TIME SPENT 0.000018TIME SPENT 0.003903TIME SPENT 0.000014TIME SPENT 0.000012TIME SPENT 0.000455PASS
TIMING: entire_frame_compile:13.49816 backend_compile:8.01879
STATS: call_* op count: 407FakeTensor.__torch_dispatch__:19272 | ProxyTorchDispatchMode.__torch_dispatch__:4801
Dynamo produced 2 graph(s) covering 407 ops
cuda train tts_angular                         TIME SPENT 0.000011TIME SPENT 0.000019TIME SPENT 0.000022TIME SPENT 0.000015TIME SPENT 0.000003TIME SPENT 0.000011TIME SPENT 0.000007TIME SPENT 0.000003TIME SPENT 0.000007TIME SPENT 0.000009TIME SPENT 0.000010TIME SPENT 0.000014TIME SPENT 0.000021TIME SPENT 0.000012TIME SPENT 0.000011TIME SPENT 0.000057PASS
TIMING: entire_frame_compile:1.47888 backend_compile:0.22796
STATS: call_* op count: 32FakeTensor.__torch_dispatch__:1132 | ProxyTorchDispatchMode.__torch_dispatch__:152
Dynamo produced 2 graph(s) covering 32 ops
cuda train vgg16                               TIME SPENT 0.000033TIME SPENT 0.000024TIME SPENT 0.000006TIME SPENT 0.000019TIME SPENT 0.000322TIME SPENT 0.000019TIME SPENT 0.000014TIME SPENT 0.000171PASS
TIMING: entire_frame_compile:3.43166 backend_compile:1.44278
STATS: call_* op count: 104FakeTensor.__torch_dispatch__:3613 | ProxyTorchDispatchMode.__torch_dispatch__:991
Dynamo produced 2 graph(s) covering 104 ops
WARNING:root:vision_maskrcnn failed to load
Eager model failed to run
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1148, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 367, in forward_and_backward_pass
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/models/detection/generalized_rcnn.py", line 104, in forward
    proposals, proposal_losses = self.rpn(images, features, targets)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1488, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/models/detection/rpn.py", line 372, in forward
    boxes, scores = self.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/models/detection/rpn.py", line 288, in filter_proposals
    keep = box_ops.batched_nms(boxes, scores, lvl, self.nms_thresh)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 75, in batched_nms
    return _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)
  File "/scratch/voz/work/pytorch/torch/jit/_trace.py", line 1212, in wrapper
    return fn(*args, **kwargs)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 94, in _batched_nms_coordinate_trick
    keep = nms(boxes_for_nms, scores, iou_threshold)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 40, in nms
    _assert_has_ops()
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/extension.py", line 48, in _assert_has_ops
    raise RuntimeError(
RuntimeError: Couldn't load custom C++ ops. This can happen if your PyTorch and torchvision versions are incompatible, or if you had errors while compiling torchvision from source. For further information on the compatible versions, check https://github.com/pytorch/vision#installation for the compatibility matrix. Please check your PyTorch version with torch.__version__ and your torchvision version with torchvision.__version__ and verify if they are compatible, and if not please reinstall torchvision so that it matches your PyTorch install.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2286, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 309, in load_model
    self.validate_model(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1150, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

cuda train yolov3                              TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000005TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000019TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000009TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000019TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000019TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000019TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000019TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000019TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000019TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000015TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000007TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000007TIME SPENT 0.000003TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000018TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000007TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000020TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000008TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000007TIME SPENT 0.000013TIME SPENT 0.000012TIME SPENT 0.000006TIME SPENT 0.000010TIME SPENT 0.000011TIME SPENT 0.000011TIME SPENT 0.000010TIME SPENT 0.000006TIME SPENT 0.000008TIME SPENT 0.000012TIME SPENT 0.000008TIME SPENT 0.000011TIME SPENT 0.000006TIME SPENT 0.000010TIME SPENT 0.000008TIME SPENT 0.000010TIME SPENT 0.000011TIME SPENT 0.000015TIME SPENT 0.000008TIME SPENT 0.000008TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000006TIME SPENT 0.000007TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000006TIME SPENT 0.000021TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000007TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000004TIME SPENT 0.000008TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000005TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000019TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000018TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000019TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000018TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002[2023-01-27 01:21:14,661] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)
   function: 'forward' (/scratch/voz/work/pytorch/torch/nn/modules/container.py:215)
   reasons:  ___check_obj_id(self, 140098439199472)
to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.
TIME SPENT 0.000008TIME SPENT 0.000005TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000018TIME SPENT 0.000031TIME SPENT 0.000003TIME SPENT 0.000016TIME SPENT 0.000123TIME SPENT 0.000006TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000021TIME SPENT 0.000004TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000003TIME SPENT 0.000026TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000031TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000035TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000041TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000047TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000052TIME SPENT 0.000012TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000059TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000064TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000071TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000073TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000093TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000092TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000112TIME SPENT 0.000005TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000110TIME SPENT 0.000003TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000114TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000122TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000125TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000130TIME SPENT 0.000004TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000148TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000143TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000149TIME SPENT 0.000004TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000002TIME SPENT 0.000001TIME SPENT 0.000002TIME SPENT 0.000153TIME SPENT 0.000005TIME SPENT 0.000003TIME SPENT 0.000163TIME SPENT 0.000006TIME SPENT 0.000004TIME SPENT 0.000188TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000180TIME SPENT 0.000006TIME SPENT 0.000002TIME SPENT 0.000194TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000200TIME SPENT 0.000005TIME SPENT 0.000002TIME SPENT 0.000204TIME SPENT 0.000041TIME SPENT 0.000009TIME SPENT 0.000810PASS
TIMING: entire_frame_compile:37.96351 backend_compile:16.70452
STATS: call_* op count: 799FakeTensor.__torch_dispatch__:48485 | ProxyTorchDispatchMode.__torch_dispatch__:8626
Dynamo produced 94 graph(s) covering 799 ops
accuracy   pass_rate=70.8%
