WARNING:__main__:Running smaller batch size=4 for AlbertForMaskedLM, orig batch_size=8
cuda train AlbertForMaskedLM 1.253x p=0.00
WARNING:__main__:Running smaller batch size=4 for AlbertForQuestionAnswering, orig batch_size=8
cuda train AlbertForQuestionAnswering 1.254x p=0.00
WARNING:__main__:Running smaller batch size=4 for AllenaiLongformerBase, orig batch_size=8
cuda train AllenaiLongformerBase  ERROR
WARNING:__main__:Running smaller batch size=4 for BartForCausalLM, orig batch_size=8
cuda train BartForCausalLM 1.123x p=0.00
WARNING:__main__:Running smaller batch size=2 for BartForConditionalGeneration, orig batch_size=4
cuda train BartForConditionalGeneration 1.092x p=0.00
WARNING:__main__:Running smaller batch size=16 for BertForMaskedLM, orig batch_size=32
cuda train BertForMaskedLM  1.179x p=0.00
WARNING:__main__:Running smaller batch size=16 for BertForQuestionAnswering, orig batch_size=32
cuda train BertForQuestionAnswering 1.247x p=0.00
WARNING:__main__:Running smaller batch size=4 for BlenderbotForCausalLM, orig batch_size=32
cuda train BlenderbotForCausalLM  ERROR
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForCausalLM, orig batch_size=256
cuda train BlenderbotSmallForCausalLM 0.998x SAME
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForConditionalGeneration, orig batch_size=128
cuda train BlenderbotSmallForConditionalGeneration 1.068x p=0.00
WARNING:__main__:Running smaller batch size=16 for CamemBert, orig batch_size=32
cuda train CamemBert 1.182x p=0.00
WARNING:__main__:Running smaller batch size=4 for DebertaForMaskedLM, orig batch_size=32
cuda train DebertaForMaskedLM  ERROR
WARNING:__main__:Running smaller batch size=8 for DebertaForQuestionAnswering, orig batch_size=32
cuda train DebertaForQuestionAnswering ERROR You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
WARNING:__main__:Running smaller batch size=1 for DebertaV2ForMaskedLM, orig batch_size=8
cuda train DebertaV2ForMaskedLM  ERROR
WARNING:__main__:Running smaller batch size=2 for DebertaV2ForQuestionAnswering, orig batch_size=8
cuda train DebertaV2ForQuestionAnswering  ERROR .suppress_errors = True

ERROR
WARNING:__main__:Running smaller batch size=128 for DistilBertForMaskedLM, orig batch_size=256
WARNING:__main__:Sequence Length not defined for DistilBertForMaskedLM. Choosing 128 arbitrarily
cuda train DistilBertForMaskedLM    1.025x p=0.00
WARNING:__main__:Running smaller batch size=256 for DistilBertForQuestionAnswering, orig batch_size=512
WARNING:__main__:Sequence Length not defined for DistilBertForQuestionAnswering. Choosing 128 arbitrarily
cuda train DistilBertForQuestionAnswering   1.137x p=0.00
WARNING:__main__:Running smaller batch size=16 for DistillGPT2, orig batch_size=32
cuda train DistillGPT2   1.366x p=0.00
If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=32 for ElectraForCausalLM, orig batch_size=64
cuda train ElectraForCausalLM 1.370x p=0.00
WARNING:__main__:Running smaller batch size=64 for ElectraForQuestionAnswering, orig batch_size=128
cuda train ElectraForQuestionAnswering 1.404x p=0.00
WARNING:__main__:Running smaller batch size=4 for GPT2ForSequenceClassification, orig batch_size=8
cuda train GPT2ForSequenceClassification  1.623x p=0.00
WARNING:__main__:Running smaller batch size=16 for GoogleFnet, orig batch_size=32
cuda train GoogleFnet 1.427x p=0.00
WARNING:__main__:Running smaller batch size=16 for LayoutLMForMaskedLM, orig batch_size=32
cuda train LayoutLMForMaskedLM 1.170x p=0.00
WARNING:__main__:Running smaller batch size=16 for LayoutLMForSequenceClassification, orig batch_size=32
cuda train LayoutLMForSequenceClassification 1.262x p=0.00
WARNING:__main__:Running smaller batch size=16 for M2M100ForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for M2M100ForConditionalGeneration. Choosing 128 arbitrarily
cuda train M2M100ForConditionalGeneration 1.014x p=0.05
WARNING:__main__:Running smaller batch size=4 for MBartForCausalLM, orig batch_size=8
cuda train MBartForCausalLM  1.121x p=0.00
WARNING:__main__:Running smaller batch size=2 for MBartForConditionalGeneration, orig batch_size=4
cuda train MBartForConditionalGeneration  1.093x p=0.00
WARNING:__main__:Running smaller batch size=16 for MT5ForConditionalGeneration, orig batch_size=32
WARNING:__main__:Sequence Length not defined for MT5ForConditionalGeneration. Choosing 128 arbitrarily
cuda train MT5ForConditionalGeneration 1.287x p=0.00
If you want to use `MegatronBertForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=4 for MegatronBertForCausalLM, orig batch_size=16
cuda train MegatronBertForCausalLM 1.122x p=0.00
WARNING:__main__:Running smaller batch size=8 for MegatronBertForQuestionAnswering, orig batch_size=16
cuda train MegatronBertForQuestionAnswering 1.183x p=0.00
WARNING:__main__:Running smaller batch size=64 for MobileBertForMaskedLM, orig batch_size=256
cuda train MobileBertForMaskedLM 1.182x p=0.00
WARNING:__main__:Running smaller batch size=128 for MobileBertForQuestionAnswering, orig batch_size=256
cuda train MobileBertForQuestionAnswering 1.203x p=0.00
WARNING:__main__:Running smaller batch size=2 for OPTForCausalLM, orig batch_size=4
cuda train OPTForCausalLM ERROR
WARNING:__main__:Running smaller batch size=8 for PLBartForCausalLM, orig batch_size=16
cuda train PLBartForCausalLM  1.218x p=0.00
WARNING:__main__:Running smaller batch size=4 for PLBartForConditionalGeneration, orig batch_size=8
cuda train PLBartForConditionalGeneration 1.211x p=0.00
WARNING:__main__:Running smaller batch size=32 for PegasusForCausalLM, orig batch_size=128
WARNING:__main__:Sequence Length not defined for PegasusForCausalLM. Choosing 128 arbitrarily
cuda train PegasusForCausalLM 0.996x SAME
WARNING:__main__:Running smaller batch size=32 for PegasusForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for PegasusForConditionalGeneration. Choosing 128 arbitrarily
cuda train PegasusForConditionalGeneration  1.024x p=0.00
If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=16 for RobertaForCausalLM, orig batch_size=32
cuda train RobertaForCausalLM  1.250x p=0.00
WARNING:__main__:Running smaller batch size=16 for RobertaForQuestionAnswering, orig batch_size=32
cuda train RobertaForQuestionAnswering  1.250x p=0.00
WARNING:__main__:Running smaller batch size=256 for Speech2Text2ForCausalLM, orig batch_size=1024
WARNING:__main__:Sequence Length not defined for Speech2Text2ForCausalLM. Choosing 128 arbitrarily
cuda train Speech2Text2ForCausalLM  1.099x p=0.00
WARNING:__main__:Running smaller batch size=4 for T5ForConditionalGeneration, orig batch_size=8
cuda train T5ForConditionalGeneration  1.233x p=0.00
WARNING:__main__:Running smaller batch size=4 for T5Small, orig batch_size=8
cuda train T5Small  1.232x p=0.00
WARNING:__main__:Running smaller batch size=32 for TrOCRForCausalLM, orig batch_size=64
cuda train TrOCRForCausalLM ERROR
WARNING:__main__:Running smaller batch size=8 for XGLMForCausalLM, orig batch_size=32
WARNING:__main__:Sequence Length not defined for XGLMForCausalLM. Choosing 128 arbitrarily
cuda train XGLMForCausalLM 1.062x p=0.00
WARNING:__main__:Running smaller batch size=8 for XLNetLMHeadModel, orig batch_size=16
cuda train XLNetLMHeadModel 1.622x p=0.00
WARNING:__main__:Running smaller batch size=16 for YituTechConvBert, orig batch_size=32
cuda train YituTechConvBert  1.171x p=0.00
speedup             gmean=1.16x mean=1.169x
abs_latency         gmean=nanx mean=115.220x
compilation_latency mean=27.257 seconds
compression_ratio   mean=0.831x
eager_peak_mem      gmean=nanx mean=13.388x
dynamo_peak_mem     gmean=nanx mean=16.378x
calls_captured      gmean=nanx mean=637.921x
unique_graphs       gmean=nanx mean=5.211x
graph_breaks        gmean=nanx mean=7.658x
unique_graph_breaks gmean=nanx mean=4.342x
