WARNING:__main__:Running smaller batch size=4 for AlbertForMaskedLM, orig batch_size=8
cuda train AlbertForMaskedLM                   1.633x p=0.00
TIMING: entire_frame_compile:15.66281 backend_compile:13.12764
STATS: call_* op count: 441 | FakeTensorMode.__torch_dispatch__:51954 | FakeTensor.__torch_dispatch__:9188 | ProxyTorchDispatchMode.__torch_dispatch__:20000
Dynamo produced 1 graphs covering 441 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for AlbertForQuestionAnswering, orig batch_size=8
cuda train AlbertForQuestionAnswering          1.644x p=0.00
TIMING: entire_frame_compile:15.17135 backend_compile:12.69326
STATS: call_* op count: 441 | FakeTensorMode.__torch_dispatch__:51538 | FakeTensor.__torch_dispatch__:8931 | ProxyTorchDispatchMode.__torch_dispatch__:19844
Dynamo produced 1 graphs covering 441 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for AllenaiLongformerBase, orig batch_size=8
cuda train AllenaiLongformerBase               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1402, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 245, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in <resume in forward_and_backward_pass>
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1848, in forward
    outputs = self.longformer(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1750, in forward
    encoder_outputs = self.encoder(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1294, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 392, in catch_errors
    return callback(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 430, in _convert_frame
    result = inner_convert(frame, cache_size, hooks)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 112, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 282, in _convert_frame_assert
    return _compile(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 347, in _compile
    out_code = transform_code_object(code, transform)
  File "/scratch/voz/work/pytorch/torch/_dynamo/bytecode_transformation.py", line 683, in transform_code_object
    transformations(instructions, code_options)
  File "/scratch/voz/work/pytorch/torch/_dynamo/convert_frame.py", line 334, in transform
    tracer.run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1890, in run
    super().run()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 609, in run
    and self.step()
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 569, in step
    getattr(self, inst.opname)(inst)
  File "/scratch/voz/work/pytorch/torch/_dynamo/symbolic_convert.py", line 1977, in RETURN_VALUE
    self.output.compile_subgraph(
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 643, in compile_subgraph
    self.compile_and_call_fx_graph(tx, pass2.graph_output_vars(), root)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 689, in compile_and_call_fx_graph
    compiled_fn = self.call_user_compiler(gm)
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 768, in call_user_compiler
    raise BackendCompilerFailed(self.compiler_fn, e).with_traceback(
  File "/scratch/voz/work/pytorch/torch/_dynamo/output_graph.py", line 764, in call_user_compiler
    compiled_fn = compiler_fn(gm, self.fake_example_inputs())
  File "/scratch/voz/work/pytorch/torch/_dynamo/debug_utils.py", line 1093, in debug_wrapper
    compiled_gm = compiler_fn(gm, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/inductor.py", line 9, in inductor
    return compile_fx(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/compile_fx.py", line 636, in compile_fx
    return aot_autograd(
  File "/scratch/voz/work/pytorch/torch/_dynamo/backends/common.py", line 62, in compiler_fn
    cg = aot_module_simplified(gm, example_inputs, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 3082, in aot_module_simplified
    compiled_fn = create_aot_dispatcher_function(
  File "/scratch/voz/work/pytorch/torch/_dynamo/utils.py", line 166, in time_wrapper
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2725, in create_aot_dispatcher_function
    compiled_fn = compiler_fn(flat_fn, fake_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1826, in aot_wrapper_dedupe
    return compiler_fn(flat_fn, leaf_flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1992, in aot_wrapper_synthetic_base
    return compiler_fn(flat_fn, flat_args, aot_config, fw_metadata=fw_metadata)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 2282, in aot_dispatch_autograd
    fx_g = create_functionalized_graph(
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1152, in create_functionalized_graph
    return make_fx(joint_helper if trace_joint else fwd_helper, decomposition_table=aot_config.decompositions)(*args)
  File "/scratch/voz/work/pytorch/torch/fx/experimental/proxy_tensor.py", line 756, in wrapped
    t = dispatch_trace(wrap_key(func, args, fx_tracer, pre_autograd), tracer=fx_tracer, concrete_args=tuple(phs))
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 245, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/experimental/proxy_tensor.py", line 462, in dispatch_trace
    graph = tracer.trace(root, concrete_args)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 245, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/_symbolic_trace.py", line 778, in trace
    (self.create_arg(fn(*args)),),
  File "/scratch/voz/work/pytorch/torch/fx/_symbolic_trace.py", line 652, in flatten_fn
    tree_out = root_fn(*tree_args)
  File "/scratch/voz/work/pytorch/torch/fx/experimental/proxy_tensor.py", line 479, in wrapped
    out = f(*tensors)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1146, in joint_helper
    return functionalized_f_helper(primals, tangents)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1099, in functionalized_f_helper
    f_outs = fn(*f_args)
  File "/scratch/voz/work/pytorch/torch/_functorch/aot_autograd.py", line 1062, in inner_fn
    backward_out = torch.autograd.grad(
  File "/scratch/voz/work/pytorch/torch/autograd/__init__.py", line 284, in grad
    return handle_torch_function(
  File "/scratch/voz/work/pytorch/torch/overrides.py", line 1536, in handle_torch_function
    result = mode.__torch_function__(public_api, types, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_inductor/overrides.py", line 42, in __torch_function__
    return func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/autograd/__init__.py", line 319, in grad
    result = Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/scratch/voz/work/pytorch/torch/utils/_stats.py", line 20, in wrapper
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/experimental/proxy_tensor.py", line 528, in __torch_dispatch__
    return self.inner_torch_dispatch(func, types, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/experimental/proxy_tensor.py", line 553, in inner_torch_dispatch
    return proxy_call(self, func, self.pre_autograd, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/fx/experimental/proxy_tensor.py", line 364, in proxy_call
    out = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 394, in __call__
    return self._op(*args, **kwargs or {})
  File "/scratch/voz/work/pytorch/torch/utils/_stats.py", line 20, in wrapper
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 1073, in __torch_dispatch__
    return self.dispatch(func, types, args, kwargs)
  File "/scratch/voz/work/pytorch/torch/_subclasses/fake_tensor.py", line 1263, in dispatch
    r = func(*args, **kwargs)
  File "/scratch/voz/work/pytorch/torch/_ops.py", line 394, in __call__
    return self._op(*args, **kwargs or {})
  File "/scratch/voz/work/pytorch/torch/_refs/__init__.py", line 4023, in view
    return _reshape_view_helper(a, *shape, allow_copy=False)
  File "/scratch/voz/work/pytorch/torch/_refs/__init__.py", line 3259, in _reshape_view_helper
    raise ValueError(msg)
torch._dynamo.exc.BackendCompilerFailed: backend='inductor' raised:
ValueError: Cannot view a tensor with shape torch.Size([4, 12, 1024, 513]) and strides (6303744, 513, 6156, 1) as a tensor with shape (48, 4, 256, 513)!


You can suppress this exception and fall back to eager by setting:
    torch._dynamo.config.suppress_errors = True

ERROR
WARNING:__main__:Running smaller batch size=4 for BartForCausalLM, orig batch_size=8
cuda train BartForCausalLM                     1.513x p=0.00
TIMING: entire_frame_compile:19.22376 backend_compile:14.00886
STATS: call_* op count: 484 | FakeTensorMode.__torch_dispatch__:59874 | FakeTensor.__torch_dispatch__:22660 | ProxyTorchDispatchMode.__torch_dispatch__:19295
Dynamo produced 1 graphs covering 484 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=2 for BartForConditionalGeneration, orig batch_size=4
cuda train BartForConditionalGeneration        1.434x p=0.00
TIMING: entire_frame_compile:45.97682 backend_compile:34.08327
STATS: call_* op count: 1260 | FakeTensorMode.__torch_dispatch__:147851 | FakeTensor.__torch_dispatch__:52271 | ProxyTorchDispatchMode.__torch_dispatch__:49834
Dynamo produced 1 graphs covering 1260 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for BertForMaskedLM, orig batch_size=32
cuda train BertForMaskedLM                     1.587x p=0.00
TIMING: entire_frame_compile:20.0229 backend_compile:14.94964
STATS: call_* op count: 372 | FakeTensorMode.__torch_dispatch__:60868 | FakeTensor.__torch_dispatch__:21306 | ProxyTorchDispatchMode.__torch_dispatch__:20184
Dynamo produced 1 graphs covering 372 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for BertForQuestionAnswering, orig batch_size=32
cuda train BertForQuestionAnswering            1.749x p=0.00
TIMING: entire_frame_compile:20.14051 backend_compile:14.78753
STATS: call_* op count: 379 | FakeTensorMode.__torch_dispatch__:60610 | FakeTensor.__torch_dispatch__:21077 | ProxyTorchDispatchMode.__torch_dispatch__:20125
Dynamo produced 1 graphs covering 379 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for BlenderbotForCausalLM, orig batch_size=32
cuda train BlenderbotForCausalLM               ERROR:common:Backend dynamo failed in warmup()
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1402, in warmup
    fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 245, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 487, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in <resume in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 492, in <resume in forward_and_backward_pass>
    self.grad_scaler.scale(loss).backward()
  File "/scratch/voz/work/pytorch/torch/_tensor.py", line 488, in backward
    torch.autograd.backward(
  File "/scratch/voz/work/pytorch/torch/autograd/__init__.py", line 204, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 39.56 GiB total capacity; 37.36 GiB already allocated; 58.56 MiB free; 37.75 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForCausalLM, orig batch_size=256
cuda train BlenderbotSmallForCausalLM          1.227x p=0.00
TIMING: entire_frame_compile:13.53107 backend_compile:10.03524
STATS: call_* op count: 329 | FakeTensorMode.__torch_dispatch__:40491 | FakeTensor.__torch_dispatch__:15284 | ProxyTorchDispatchMode.__torch_dispatch__:13052
Dynamo produced 1 graphs covering 329 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=64 for BlenderbotSmallForConditionalGeneration, orig batch_size=128
cuda train BlenderbotSmallForConditionalGeneration  1.319x p=0.00
TIMING: entire_frame_compile:30.37724 backend_compile:23.10901
STATS: call_* op count: 842 | FakeTensorMode.__torch_dispatch__:99184 | FakeTensor.__torch_dispatch__:35125 | ProxyTorchDispatchMode.__torch_dispatch__:33438
Dynamo produced 1 graphs covering 842 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for CamemBert, orig batch_size=32
cuda train CamemBert                           1.535x p=0.00
TIMING: entire_frame_compile:19.88345 backend_compile:14.81458
STATS: call_* op count: 379 | FakeTensorMode.__torch_dispatch__:60958 | FakeTensor.__torch_dispatch__:21321 | ProxyTorchDispatchMode.__torch_dispatch__:20204
Dynamo produced 1 graphs covering 379 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for DebertaForMaskedLM, orig batch_size=32
cuda train DebertaForMaskedLM                  0.878x p=0.00
TIMING: entire_frame_compile:22.46831 backend_compile:15.71152
STATS: call_* op count: 728 | FakeTensorMode.__torch_dispatch__:65887 | FakeTensor.__torch_dispatch__:20205 | ProxyTorchDispatchMode.__torch_dispatch__:22439
Dynamo produced 95 graphs covering 728 ops with 164 graph breaks (15 unique)
WARNING:__main__:Running smaller batch size=8 for DebertaForQuestionAnswering, orig batch_size=32
cuda train DebertaForQuestionAnswering         1.124x p=0.00
TIMING: entire_frame_compile:22.30012 backend_compile:15.63439
STATS: call_* op count: 735 | FakeTensorMode.__torch_dispatch__:65617 | FakeTensor.__torch_dispatch__:19977 | ProxyTorchDispatchMode.__torch_dispatch__:22372
Dynamo produced 95 graphs covering 735 ops with 164 graph breaks (15 unique)
WARNING:__main__:Running smaller batch size=1 for DebertaV2ForMaskedLM, orig batch_size=8
cuda train DebertaV2ForMaskedLM                   function: 'forward' (/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:226)
   reasons:  ___check_obj_id(L['self'], 140310470141616)
to diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
0.737x p=0.00
TIMING: entire_frame_compile:38.20015 backend_compile:23.98917
STATS: call_* op count: 933 | FakeTensorMode.__torch_dispatch__:133079 | FakeTensor.__torch_dispatch__:44401 | ProxyTorchDispatchMode.__torch_dispatch__:40335
Dynamo produced 179 graphs covering 933 ops with 287 graph breaks (15 unique)
WARNING:__main__:Running smaller batch size=2 for DebertaV2ForQuestionAnswering, orig batch_size=8
cuda train DebertaV2ForQuestionAnswering          function: 'forward' (/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:226)
   reasons:  ___check_obj_id(L['self'], 139680598745936)
to diagnose recompilation issues, see https://pytorch.org/docs/master/compile/troubleshooting.html.
0.795x p=0.00
TIMING: entire_frame_compile:38.59096 backend_compile:24.47338
STATS: call_* op count: 940 | FakeTensorMode.__torch_dispatch__:133033 | FakeTensor.__torch_dispatch__:44272 | ProxyTorchDispatchMode.__torch_dispatch__:40224
Dynamo produced 179 graphs covering 940 ops with 287 graph breaks (15 unique)
WARNING:__main__:Running smaller batch size=128 for DistilBertForMaskedLM, orig batch_size=256
WARNING:__main__:Sequence Length not defined for DistilBertForMaskedLM. Choosing 128 arbitrarily
cuda train DistilBertForMaskedLM               1.212x p=0.00
TIMING: entire_frame_compile:11.3555 backend_compile:9.00985
STATS: call_* op count: 208 | FakeTensorMode.__torch_dispatch__:30933 | FakeTensor.__torch_dispatch__:10792 | ProxyTorchDispatchMode.__torch_dispatch__:10377
Dynamo produced 1 graphs covering 208 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=256 for DistilBertForQuestionAnswering, orig batch_size=512
WARNING:__main__:Sequence Length not defined for DistilBertForQuestionAnswering. Choosing 128 arbitrarily
cuda train DistilBertForQuestionAnswering      1.437x p=0.00
TIMING: entire_frame_compile:10.98981 backend_compile:8.65469
STATS: call_* op count: 216 | FakeTensorMode.__torch_dispatch__:30800 | FakeTensor.__torch_dispatch__:10573 | ProxyTorchDispatchMode.__torch_dispatch__:10361
Dynamo produced 1 graphs covering 216 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for DistillGPT2, orig batch_size=32
cuda train DistillGPT2                         1.646x p=0.00
TIMING: entire_frame_compile:10.77282 backend_compile:8.41183
STATS: call_* op count: 332 | FakeTensorMode.__torch_dispatch__:27369 | FakeTensor.__torch_dispatch__:8839 | ProxyTorchDispatchMode.__torch_dispatch__:9205
Dynamo produced 1 graphs covering 332 ops with 11 graph breaks (6 unique)
If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=32 for ElectraForCausalLM, orig batch_size=64
cuda train ElectraForCausalLM                  1.836x p=0.00
TIMING: entire_frame_compile:20.63358 backend_compile:15.12825
STATS: call_* op count: 377 | FakeTensorMode.__torch_dispatch__:61463 | FakeTensor.__torch_dispatch__:21507 | ProxyTorchDispatchMode.__torch_dispatch__:20362
Dynamo produced 1 graphs covering 377 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=64 for ElectraForQuestionAnswering, orig batch_size=128
cuda train ElectraForQuestionAnswering         2.090x p=0.00
TIMING: entire_frame_compile:19.98063 backend_compile:14.89075
STATS: call_* op count: 380 | FakeTensorMode.__torch_dispatch__:61022 | FakeTensor.__torch_dispatch__:21278 | ProxyTorchDispatchMode.__torch_dispatch__:20231
Dynamo produced 1 graphs covering 380 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for GPT2ForSequenceClassification, orig batch_size=8
cuda train GPT2ForSequenceClassification       2.224x p=0.00
TIMING: entire_frame_compile:18.02444 backend_compile:13.42627
STATS: call_* op count: 646 | FakeTensorMode.__torch_dispatch__:52711 | FakeTensor.__torch_dispatch__:17206 | ProxyTorchDispatchMode.__torch_dispatch__:17752
Dynamo produced 1 graphs covering 646 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for GoogleFnet, orig batch_size=32
WARNING:root:GoogleFnet failed to load
Eager model failed to run
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1172, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 490, in forward_and_backward_pass
    pred = mod(**cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 759, in forward
    outputs = self.fnet(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 597, in forward
    encoder_outputs = self.encoder(
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 302, in forward
    layer_outputs = layer_module(hidden_states)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 261, in forward
    self_fourier_outputs = self.fourier(hidden_states)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 214, in forward
    self_outputs = self.self(hidden_states)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/home/voz/miniconda3/envs/torch5/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 193, in forward
    outputs = self.fourier_transform(hidden_states).real
RuntimeError: cuFFT only supports dimensions whose sizes are powers of two when computing in half precision, but got a signal size of[512, 768]

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2335, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 437, in load_model
    self.validate_model(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1174, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

WARNING:__main__:Running smaller batch size=16 for LayoutLMForMaskedLM, orig batch_size=32
cuda train LayoutLMForMaskedLM                 1.565x p=0.00
TIMING: entire_frame_compile:20.3665 backend_compile:14.98252
STATS: call_* op count: 398 | FakeTensorMode.__torch_dispatch__:62898 | FakeTensor.__torch_dispatch__:22165 | ProxyTorchDispatchMode.__torch_dispatch__:20597
Dynamo produced 1 graphs covering 398 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for LayoutLMForSequenceClassification, orig batch_size=32
cuda train LayoutLMForSequenceClassification   1.767x p=0.00
TIMING: entire_frame_compile:21.18723 backend_compile:14.68751
STATS: call_* op count: 396 | FakeTensorMode.__torch_dispatch__:62285 | FakeTensor.__torch_dispatch__:21984 | ProxyTorchDispatchMode.__torch_dispatch__:20441
Dynamo produced 1 graphs covering 396 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for M2M100ForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for M2M100ForConditionalGeneration. Choosing 128 arbitrarily
cuda train M2M100ForConditionalGeneration      1.266x p=0.00
TIMING: entire_frame_compile:46.03844 backend_compile:34.13855
STATS: call_* op count: 1273 | FakeTensorMode.__torch_dispatch__:152090 | FakeTensor.__torch_dispatch__:53947 | ProxyTorchDispatchMode.__torch_dispatch__:51352
Dynamo produced 1 graphs covering 1273 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for MBartForCausalLM, orig batch_size=8
cuda train MBartForCausalLM                    1.506x p=0.00
TIMING: entire_frame_compile:18.89405 backend_compile:13.77364
STATS: call_* op count: 484 | FakeTensorMode.__torch_dispatch__:60470 | FakeTensor.__torch_dispatch__:23086 | ProxyTorchDispatchMode.__torch_dispatch__:19382
Dynamo produced 1 graphs covering 484 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=2 for MBartForConditionalGeneration, orig batch_size=4
cuda train MBartForConditionalGeneration       1.423x p=0.00
TIMING: entire_frame_compile:45.87699 backend_compile:33.79673
STATS: call_* op count: 1265 | FakeTensorMode.__torch_dispatch__:149264 | FakeTensor.__torch_dispatch__:53254 | ProxyTorchDispatchMode.__torch_dispatch__:50028
Dynamo produced 1 graphs covering 1265 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for MT5ForConditionalGeneration, orig batch_size=32
WARNING:__main__:Sequence Length not defined for MT5ForConditionalGeneration. Choosing 128 arbitrarily
cuda train MT5ForConditionalGeneration         1.765x p=0.00
TIMING: entire_frame_compile:31.7118 backend_compile:23.5425
STATS: call_* op count: 1175 | FakeTensorMode.__torch_dispatch__:102454 | FakeTensor.__torch_dispatch__:27995 | ProxyTorchDispatchMode.__torch_dispatch__:41045
Dynamo produced 1 graphs covering 1175 ops with 11 graph breaks (6 unique)
If you want to use `MegatronBertForCausalLM` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=4 for MegatronBertForCausalLM, orig batch_size=16
cuda train MegatronBertForCausalLM             1.435x p=0.00
TIMING: entire_frame_compile:38.08367 backend_compile:27.44647
STATS: call_* op count: 723 | FakeTensorMode.__torch_dispatch__:119604 | FakeTensor.__torch_dispatch__:42007 | ProxyTorchDispatchMode.__torch_dispatch__:39569
Dynamo produced 1 graphs covering 723 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=8 for MegatronBertForQuestionAnswering, orig batch_size=16
cuda train MegatronBertForQuestionAnswering    1.589x p=0.00
TIMING: entire_frame_compile:37.96541 backend_compile:27.55586
STATS: call_* op count: 726 | FakeTensorMode.__torch_dispatch__:119163 | FakeTensor.__torch_dispatch__:41778 | ProxyTorchDispatchMode.__torch_dispatch__:39438
Dynamo produced 1 graphs covering 726 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=64 for MobileBertForMaskedLM, orig batch_size=256
cuda train MobileBertForMaskedLM               1.761x p=0.00
TIMING: entire_frame_compile:72.49059 backend_compile:44.80969
STATS: call_* op count: 1449 | FakeTensorMode.__torch_dispatch__:215857 | FakeTensor.__torch_dispatch__:26279 | ProxyTorchDispatchMode.__torch_dispatch__:76695
Dynamo produced 1 graphs covering 1449 ops with 5 graph breaks (4 unique)
WARNING:__main__:Running smaller batch size=128 for MobileBertForQuestionAnswering, orig batch_size=256
cuda train MobileBertForQuestionAnswering      1.663x p=0.00
TIMING: entire_frame_compile:71.96428 backend_compile:44.35224
STATS: call_* op count: 1453 | FakeTensorMode.__torch_dispatch__:215722 | FakeTensor.__torch_dispatch__:26267 | ProxyTorchDispatchMode.__torch_dispatch__:76608
Dynamo produced 1 graphs covering 1453 ops with 5 graph breaks (4 unique)
WARNING:__main__:Running smaller batch size=2 for OPTForCausalLM, orig batch_size=4
cuda train OPTForCausalLM                      2.435x p=0.00
TIMING: entire_frame_compile:18.74019 backend_compile:14.45398
STATS: call_* op count: 536 | FakeTensorMode.__torch_dispatch__:56718 | FakeTensor.__torch_dispatch__:20831 | ProxyTorchDispatchMode.__torch_dispatch__:19164
Dynamo produced 1 graphs covering 536 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=8 for PLBartForCausalLM, orig batch_size=16
cuda train PLBartForCausalLM                   1.621x p=0.00
TIMING: entire_frame_compile:11.48454 backend_compile:8.38317
STATS: call_* op count: 256 | FakeTensorMode.__torch_dispatch__:31566 | FakeTensor.__torch_dispatch__:11656 | ProxyTorchDispatchMode.__torch_dispatch__:10187
Dynamo produced 1 graphs covering 256 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for PLBartForConditionalGeneration, orig batch_size=8
cuda train PLBartForConditionalGeneration      1.615x p=0.00
TIMING: entire_frame_compile:24.26253 backend_compile:18.5785
STATS: call_* op count: 660 | FakeTensorMode.__torch_dispatch__:78259 | FakeTensor.__torch_dispatch__:27538 | ProxyTorchDispatchMode.__torch_dispatch__:26402
Dynamo produced 1 graphs covering 660 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForCausalLM, orig batch_size=128
WARNING:__main__:Sequence Length not defined for PegasusForCausalLM. Choosing 128 arbitrarily
cuda train PegasusForCausalLM                  1.172x p=0.00
TIMING: entire_frame_compile:19.5883 backend_compile:13.59912
STATS: call_* op count: 483 | FakeTensorMode.__torch_dispatch__:60097 | FakeTensor.__torch_dispatch__:22834 | ProxyTorchDispatchMode.__torch_dispatch__:19266
Dynamo produced 1 graphs covering 483 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=32 for PegasusForConditionalGeneration, orig batch_size=64
WARNING:__main__:Sequence Length not defined for PegasusForConditionalGeneration. Choosing 128 arbitrarily
cuda train PegasusForConditionalGeneration     1.232x p=0.00
TIMING: entire_frame_compile:39.44976 backend_compile:33.7209
STATS: call_* op count: 1250 | FakeTensorMode.__torch_dispatch__:135970 | FakeTensor.__torch_dispatch__:16003 | ProxyTorchDispatchMode.__torch_dispatch__:49710
Dynamo produced 1 graphs covering 1250 ops with 5 graph breaks (4 unique)
If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`
WARNING:__main__:Running smaller batch size=16 for RobertaForCausalLM, orig batch_size=32
cuda train RobertaForCausalLM                  1.650x p=0.00
TIMING: entire_frame_compile:20.00748 backend_compile:14.93884
STATS: call_* op count: 383 | FakeTensorMode.__torch_dispatch__:61132 | FakeTensor.__torch_dispatch__:21332 | ProxyTorchDispatchMode.__torch_dispatch__:20274
Dynamo produced 1 graphs covering 383 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for RobertaForQuestionAnswering, orig batch_size=32
cuda train RobertaForQuestionAnswering         1.763x p=0.00
TIMING: entire_frame_compile:19.9574 backend_compile:14.61863
STATS: call_* op count: 386 | FakeTensorMode.__torch_dispatch__:60685 | FakeTensor.__torch_dispatch__:21091 | ProxyTorchDispatchMode.__torch_dispatch__:20143
Dynamo produced 1 graphs covering 386 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=256 for Speech2Text2ForCausalLM, orig batch_size=1024
WARNING:__main__:Sequence Length not defined for Speech2Text2ForCausalLM. Choosing 128 arbitrarily
cuda train Speech2Text2ForCausalLM             1.536x p=0.00
TIMING: entire_frame_compile:10.64185 backend_compile:8.04205
STATS: call_* op count: 265 | FakeTensorMode.__torch_dispatch__:30041 | FakeTensor.__torch_dispatch__:11178 | ProxyTorchDispatchMode.__torch_dispatch__:9726
Dynamo produced 1 graphs covering 265 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for T5ForConditionalGeneration, orig batch_size=8
cuda train T5ForConditionalGeneration          1.604x p=0.00
TIMING: entire_frame_compile:22.57905 backend_compile:17.08629
STATS: call_* op count: 812 | FakeTensorMode.__torch_dispatch__:70706 | FakeTensor.__torch_dispatch__:19222 | ProxyTorchDispatchMode.__torch_dispatch__:28006
Dynamo produced 1 graphs covering 812 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=4 for T5Small, orig batch_size=8
cuda train T5Small                             1.607x p=0.00
TIMING: entire_frame_compile:22.60822 backend_compile:17.15405
STATS: call_* op count: 812 | FakeTensorMode.__torch_dispatch__:70706 | FakeTensor.__torch_dispatch__:19222 | ProxyTorchDispatchMode.__torch_dispatch__:28006
Dynamo produced 1 graphs covering 812 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=32 for TrOCRForCausalLM, orig batch_size=64
cuda train TrOCRForCausalLM                    1.252x p=0.00
TIMING: entire_frame_compile:19.05845 backend_compile:13.72866
STATS: call_* op count: 483 | FakeTensorMode.__torch_dispatch__:59872 | FakeTensor.__torch_dispatch__:22661 | ProxyTorchDispatchMode.__torch_dispatch__:19293
Dynamo produced 1 graphs covering 483 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=8 for XGLMForCausalLM, orig batch_size=32
WARNING:__main__:Sequence Length not defined for XGLMForCausalLM. Choosing 128 arbitrarily
cuda train XGLMForCausalLM                     1.309x p=0.00
TIMING: entire_frame_compile:35.7633 backend_compile:27.1385
STATS: call_* op count: 1003 | FakeTensorMode.__torch_dispatch__:119002 | FakeTensor.__torch_dispatch__:43079 | ProxyTorchDispatchMode.__torch_dispatch__:39799
Dynamo produced 1 graphs covering 1003 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=8 for XLNetLMHeadModel, orig batch_size=16
cuda train XLNetLMHeadModel                    1.827x p=0.00
TIMING: entire_frame_compile:53.96496 backend_compile:43.56509
STATS: call_* op count: 770 | FakeTensorMode.__torch_dispatch__:164362 | FakeTensor.__torch_dispatch__:45805 | ProxyTorchDispatchMode.__torch_dispatch__:61047
Dynamo produced 1 graphs covering 770 ops with 11 graph breaks (6 unique)
WARNING:__main__:Running smaller batch size=16 for YituTechConvBert, orig batch_size=32
cuda train YituTechConvBert                    1.437x p=0.00
TIMING: entire_frame_compile:29.74843 backend_compile:22.23213
STATS: call_* op count: 636 | FakeTensorMode.__torch_dispatch__:91141 | FakeTensor.__torch_dispatch__:30534 | ProxyTorchDispatchMode.__torch_dispatch__:30902
Dynamo produced 1 graphs covering 636 ops with 11 graph breaks (6 unique)
speedup             gmean=1.48x mean=1.511x
abs_latency         gmean=nanx mean=77.221x
compilation_latency mean=40.386 seconds
compression_ratio   mean=0.907x
eager_peak_mem      gmean=nanx mean=11.148x
dynamo_peak_mem     gmean=nanx mean=12.638x
calls_captured      gmean=nanx mean=662.767x
unique_graphs       gmean=nanx mean=13.651x
graph_breaks        gmean=nanx mean=30.535x
unique_graph_breaks gmean=nanx mean=6.698x
