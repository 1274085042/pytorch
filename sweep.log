cuda train BERT_pytorch                        [2023-02-17 02:10:03,027] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:10:03,198] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:10:04,519] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:3.4948 backend_compile:0.00055
STATS: call_* op count: 926 | FakeTensorMode.__torch_dispatch__:6372 | FakeTensor.__torch_dispatch__:6028
Dynamo produced 2 graphs covering 926 ops with 4 graph breaks (3 unique)
cuda train Background_Matting                  [2023-02-17 02:11:03,581] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:11:04,918] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:11:05,736] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:3.81775 backend_compile:0.0004
STATS: call_* op count: 527 | FakeTensorMode.__torch_dispatch__:5397 | FakeTensor.__torch_dispatch__:5536
Dynamo produced 2 graphs covering 527 ops with 4 graph breaks (3 unique)
cuda train LearningToPaint                     [2023-02-17 02:11:24,076] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:11:24,252] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:11:24,590] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:1.08655 backend_compile:0.00018
STATS: call_* op count: 201 | FakeTensor.__torch_dispatch__:2045 | FakeTensorMode.__torch_dispatch__:1995
Dynamo produced 2 graphs covering 201 ops with 4 graph breaks (3 unique)
WARNING:root:Super_SloMo failed to load
Eager model failed to run
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1074, in validate_model
    self.model_iter_fn(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 368, in forward_and_backward_pass
    pred = mod(*cloned_inputs)
  File "/scratch/voz/work/pytorch/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/model_wrapper.py", line 34, in forward
    fCoeff = model.getFlowCoeff(trainFrameIndex, I0.device)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/Super_SloMo/slomo_model.py", line 324, in getFlowCoeff
    C11 = C00 = - (1 - (t[ind])) * (t[ind])
RuntimeError: indices should be either on cpu or on the same device as the indexed tensor (cpu)

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2169, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 310, in load_model
    self.validate_model(model, example_inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1076, in validate_model
    raise NotImplementedError("Eager model failed to run") from e
NotImplementedError: Eager model failed to run

cuda train alexnet                             [2023-02-17 02:11:49,957] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:11:50,137] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:11:50,197] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.39627 backend_compile:9e-05
STATS: call_* op count: 54 | FakeTensor.__torch_dispatch__:512 | FakeTensorMode.__torch_dispatch__:364
Dynamo produced 2 graphs covering 54 ops with 4 graph breaks (3 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/benchmarks.py", line 86, in <module>
    torchbench.torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1840, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 916, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2169, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 274, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/attention_is_all_you_need_pytorch/__init__.py", line 97, in __init__
    train_data, test_data = prepare_dataloaders(self.opt, self.device)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/attention_is_all_you_need_pytorch/train.py", line 333, in prepare_dataloaders
    data = pickle.load(open(opt.data_pkl, 'rb'))
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/dill/_dill.py", line 272, in load
    return Unpickler(file, ignore=ignore, **kwds).load()
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/dill/_dill.py", line 419, in load
    obj = StockUnpickler.load(self)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/dill/_dill.py", line 409, in find_class
    return StockUnpickler.find_class(self, module, name)
ModuleNotFoundError: No module named 'spacy.lemmatizer'
ERROR
cuda train dcgan                               [2023-02-17 02:12:12,499] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:12:13,130] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:12:13,197] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.81479 backend_compile:8e-05
STATS: call_* op count: 35 | FakeTensor.__torch_dispatch__:358 | FakeTensorMode.__torch_dispatch__:324
Dynamo produced 2 graphs covering 35 ops with 4 graph breaks (3 unique)
cuda train demucs                              [2023-02-17 02:12:42,192] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:12:42,363] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:12:42,471] torch._dynamo.symbolic_convert: [WARNING] Graph break: TorchDynamo purposely graph breaks on RNN, GRU, LSTMs from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/demucs/__init__.py", line 31, in forward
    return sources, self.model(mix)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/demucs/demucs/model.py", line 209, in forward
    x = self.lstm(x)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/demucs/demucs/model.py", line 27, in forward
    x = self.lstm(x)[0]

[2023-02-17 02:12:42,640] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:1.02777 backend_compile:0.00029
STATS: call_* op count: 215 | FakeTensorMode.__torch_dispatch__:1387 | FakeTensor.__torch_dispatch__:2169
Dynamo produced 5 graphs covering 215 ops with 6 graph breaks (4 unique)
cuda train densenet121                         [2023-02-17 02:12:56,563] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:12:56,738] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:12:58,862] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:6.69522 backend_compile:0.00084
STATS: call_* op count: 1159 | FakeTensor.__torch_dispatch__:11413 | FakeTensorMode.__torch_dispatch__:11304
Dynamo produced 2 graphs covering 1159 ops with 4 graph breaks (3 unique)
cuda train dlrm                                [2023-02-17 02:13:26,158] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:13:26,883] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method NNModuleVariable() zero_grad [ConstantVariable(bool)] {} from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1126, in optimizer_zero_grad
    mod.zero_grad(True)

[2023-02-17 02:13:27,025] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.8848 backend_compile:6e-05
STATS: call_* op count: 36 | FakeTensor.__torch_dispatch__:128 | FakeTensorMode.__torch_dispatch__:293
Dynamo produced 1 graphs covering 36 ops with 4 graph breaks (3 unique)
cuda train drq                                 [2023-02-17 02:13:45,217] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:13:45,411] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:0.51535 backend_compile:0.00012
STATS: call_* op count: 66 | FakeTensorMode.__torch_dispatch__:428 | FakeTensor.__torch_dispatch__:577
Dynamo produced 3 graphs covering 66 ops with 3 graph breaks (2 unique)
cuda train fastNLP_Bert                        [2023-02-17 02:13:58,138] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:13:58,331] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:13:58,366] torch._dynamo.symbolic_convert: [WARNING] Graph break: Tensor.item from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/fastNLP/models/bert.py", line 265, in forward
    sequence_output = self.bert(words)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 137, in forward
    outputs = self.model(words)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 445, in forward
    max_word_piece_length = batch_word_pieces_length.sum(dim=-1).max().item()  # 表示word piece的长度(包括padding)

[2023-02-17 02:13:58,429] torch._dynamo.symbolic_convert: [WARNING] Graph break: Tensor.numpy from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/fastNLP/embeddings/bert_embedding.py", line 462, in <graph break in forward>
    word_indexes = words.cpu().numpy()

[2023-02-17 02:13:59,770] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:3.66191 backend_compile:0.0006
STATS: call_* op count: 833 | FakeTensorMode.__torch_dispatch__:7000 | FakeTensor.__torch_dispatch__:6712
Dynamo produced 6 graphs covering 833 ops with 8 graph breaks (5 unique)
cuda train hf_Albert                           [2023-02-17 02:14:14,107] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:14:14,990] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.42187 backend_compile:0.00022
STATS: call_* op count: 492 | FakeTensorMode.__torch_dispatch__:4701 | FakeTensor.__torch_dispatch__:1774
Dynamo produced 2 graphs covering 492 ops with 3 graph breaks (2 unique)
cuda train hf_Bart                             [2023-02-17 02:14:32,165] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:14:32,829] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:14:33,605] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/framework/huggingface/model_factory.py", line 27, in forward
    return self.model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1369, in forward
    outputs = self.model(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1251, in forward
    decoder_outputs = self.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1123, in forward
    next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

PASS
TIMING: entire_frame_compile:7.0868 backend_compile:0.00089
STATS: call_* op count: 1160 | FakeTensorMode.__torch_dispatch__:14320 | FakeTensor.__torch_dispatch__:10592
Dynamo produced 10 graphs covering 1160 ops with 14 graph breaks (6 unique)
cuda train hf_Bert                             [2023-02-17 02:14:52,008] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:14:52,195] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.79181 backend_compile:0.0006
STATS: call_* op count: 771 | FakeTensorMode.__torch_dispatch__:6859 | FakeTensor.__torch_dispatch__:6765
Dynamo produced 2 graphs covering 771 ops with 3 graph breaks (2 unique)
cuda train hf_BigBird                          [2023-02-17 02:15:11,149] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:15:11,571] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:15:11,700] torch._dynamo.symbolic_convert: [WARNING] Graph break: numpy from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 2462, in forward
    outputs = self.bert(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 2144, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 1637, in forward
    layer_outputs = layer_module(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 1489, in forward
    self_attention_outputs = self.attention(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 1402, in forward
    self_outputs = self.self(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 471, in forward
    context_layer, attention_probs = self.bigbird_block_sparse_attention(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 569, in bigbird_block_sparse_attention
    np.random.seed(seed)

[2023-02-17 02:15:12,104] torch._dynamo.symbolic_convert: [WARNING] Graph break: numpy from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 571, in <graph break in bigbird_block_sparse_attention>
    rand_attn = [
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 572, in <listcomp>
    self._bigbird_block_rand_mask(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 1084, in _bigbird_block_rand_mask
    rand_attn = np.zeros((from_seq_length // from_block_size - 2, num_rand_blocks), dtype=np.int32)

[2023-02-17 02:15:12,174] torch._dynamo.symbolic_convert: [WARNING] Graph break: numpy from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 593, in <graph break in bigbird_block_sparse_attention>
    rand_attn = np.stack(rand_attn, axis=0)

[2023-02-17 02:15:12,237] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function args: NumpyVariable() TorchVariable() ConstantVariable(dtype) from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/big_bird/modeling_big_bird.py", line 594, in <graph break in bigbird_block_sparse_attention>
    rand_attn = torch.tensor(rand_attn, device=query_layer.device, dtype=torch.long)

[2023-02-17 02:15:19,521] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:15:19,525] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:10.37675 backend_compile:0.00269
STATS: call_* op count: 3405 | FakeTensorMode.__torch_dispatch__:26347 | FakeTensor.__torch_dispatch__:8698
Dynamo produced 65 graphs covering 3405 ops with 59 graph breaks (7 unique)
cuda train hf_DistilBert                       [2023-02-17 02:15:34,378] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:15:35,032] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.20465 backend_compile:0.00029
STATS: call_* op count: 413 | FakeTensorMode.__torch_dispatch__:3258 | FakeTensor.__torch_dispatch__:3315
Dynamo produced 2 graphs covering 413 ops with 3 graph breaks (2 unique)
cuda train hf_GPT2                             [2023-02-17 02:15:50,166] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:15:50,353] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.15517 backend_compile:0.00049
STATS: call_* op count: 931 | FakeTensorMode.__torch_dispatch__:5172 | FakeTensor.__torch_dispatch__:4479
Dynamo produced 2 graphs covering 931 ops with 3 graph breaks (2 unique)
cuda train hf_Longformer                       [2023-02-17 02:16:09,219] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:16:09,403] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:16:09,738] torch._dynamo.symbolic_convert: [WARNING] Graph break: Tensor.item from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1849, in forward
    outputs = self.longformer(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1751, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1295, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()

[2023-02-17 02:16:10,152] torch._dynamo.symbolic_convert: [WARNING] Graph break: const method call dict.get from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1250, in forward
    self_attn_outputs = self.attention(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1186, in forward
    self_outputs = self.self(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 575, in forward
    attn_scores = self._sliding_chunks_query_key_matmul(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 840, in _sliding_chunks_query_key_matmul
    query = self._chunk(query, window_overlap, self.config.__dict__.get("onnx_export", False))

[2023-02-17 02:16:10,269] torch._dynamo.symbolic_convert: [WARNING] Graph break: const method call dict.get from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 841, in <graph break in _sliding_chunks_query_key_matmul>
    key = self._chunk(key, window_overlap, self.config.__dict__.get("onnx_export", False))

[2023-02-17 02:16:10,302] torch._dynamo.symbolic_convert: [WARNING] Graph break: data dependent operator: aten._local_scalar_dense.default from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 859, in <graph break in _sliding_chunks_query_key_matmul>
    diagonal_attention_scores = diagonal_chunked_attention_scores.new_zeros(

[2023-02-17 02:16:15,743] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:16:15,748] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:8.57282 backend_compile:0.00457
STATS: call_* op count: 2114 | FakeTensorMode.__torch_dispatch__:17829 | FakeTensor.__torch_dispatch__:10510
Dynamo produced 160 graphs covering 2114 ops with 129 graph breaks (8 unique)
cuda train hf_Reformer                         [2023-02-17 02:16:29,512] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:16:30,386] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:16:30,408] torch._dynamo.symbolic_convert: [WARNING] Graph break: numpy from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2399, in forward
    reformer_outputs = self.reformer(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2063, in forward
    least_common_mult_chunk_length = _get_least_common_mult_chunk_len(self.config)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 92, in _get_least_common_mult_chunk_len
    return np.lcm(config.lsh_attn_chunk_length, config.local_attn_chunk_length)

[2023-02-17 02:16:30,480] torch._dynamo.symbolic_convert: [WARNING] Graph break: autograd.Function with requires_grad from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 2107, in <graph break in forward>
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1733, in forward
    hidden_states = _ReversibleFunction.apply(

[2023-02-17 02:16:30,523] torch._dynamo.symbolic_convert: [WARNING] Graph break: hasattr: TorchVariable(<module 'torch.cuda' from '/scratch/voz/work/pytorch/torch/cuda/__init__.py'>) from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1484, in forward
    self._init_attention_seed()
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1440, in _init_attention_seed
    if hasattr(torch.cuda, "default_generators") and len(torch.cuda.default_generators) > 0:

[2023-02-17 02:16:30,532] torch._dynamo.symbolic_convert: [WARNING] Graph break: inlining disallowed: <function current_device at 0x7f40bba5fac0> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1442, in <graph break in _init_attention_seed>
    device_idx = torch.cuda.current_device()

[2023-02-17 02:16:30,535] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function UserDefinedObjectVariable(seed) [] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1443, in <graph break in _init_attention_seed>
    self.attention_seed = torch.cuda.default_generators[device_idx].seed()

[2023-02-17 02:16:30,542] torch._dynamo.symbolic_convert: [WARNING] Graph break: torch.* op returned non-Tensor Generator call_function <built-in method manual_seed of torch._C.Generator object at 0x7f40bc50be10> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1448, in <graph break in _init_attention_seed>
    torch.manual_seed(self.attention_seed)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 299, in deterministic_torch_manual_seed
    return default_generator.manual_seed(seed)

[2023-02-17 02:16:30,668] torch._dynamo.symbolic_convert: [WARNING] Graph break: hasattr: TorchVariable(<module 'torch.cuda' from '/scratch/voz/work/pytorch/torch/cuda/__init__.py'>) from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1509, in <graph break in forward>
    self._init_feed_forward_seed()
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1457, in _init_feed_forward_seed
    if hasattr(torch.cuda, "default_generators") and len(torch.cuda.default_generators) > 0:

[2023-02-17 02:16:30,684] torch._dynamo.symbolic_convert: [WARNING] Graph break: inlining disallowed: <function current_device at 0x7f40bba5fac0> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1459, in <graph break in _init_feed_forward_seed>
    device_idx = torch.cuda.current_device()

[2023-02-17 02:16:30,688] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function UserDefinedObjectVariable(seed) [] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/reformer/modeling_reformer.py", line 1460, in <graph break in _init_feed_forward_seed>
    self.feed_forward_seed = torch.cuda.default_generators[device_idx].seed()

PASS
TIMING: entire_frame_compile:2.99102 backend_compile:0.00068
STATS: call_* op count: 651 | FakeTensorMode.__torch_dispatch__:3143 | FakeTensor.__torch_dispatch__:2497
Dynamo produced 19 graphs covering 651 ops with 45 graph breaks (8 unique)
cuda train hf_T5                               [2023-02-17 02:16:45,007] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:16:45,360] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.82492 backend_compile:0.00048
STATS: call_* op count: 1057 | FakeTensorMode.__torch_dispatch__:6843 | FakeTensor.__torch_dispatch__:4799
Dynamo produced 2 graphs covering 1057 ops with 3 graph breaks (2 unique)
cuda train maml_omniglot                       [2023-02-17 02:16:59,626] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:16:59,790] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:16:59,860] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.37479 backend_compile:9e-05
STATS: call_* op count: 42 | FakeTensor.__torch_dispatch__:446 | FakeTensorMode.__torch_dispatch__:387
Dynamo produced 2 graphs covering 42 ops with 4 graph breaks (3 unique)
cuda train mnasnet1_0                          [2023-02-17 02:17:11,918] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:17:12,095] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:17:12,845] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.40402 backend_compile:0.00037
STATS: call_* op count: 468 | FakeTensor.__torch_dispatch__:4959 | FakeTensorMode.__torch_dispatch__:4823
Dynamo produced 2 graphs covering 468 ops with 4 graph breaks (3 unique)
cuda train mobilenet_v2                        [2023-02-17 02:17:26,063] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:17:26,248] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:17:27,048] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.59532 backend_compile:0.0004
STATS: call_* op count: 469 | FakeTensor.__torch_dispatch__:4994 | FakeTensorMode.__torch_dispatch__:4865
Dynamo produced 2 graphs covering 469 ops with 4 graph breaks (3 unique)
cuda train mobilenet_v2_quantized_qat          WARNING:common:fp64 golden ref were not generated for mobilenet_v2_quantized_qat. Setting accuracy check to cosine
[2023-02-17 02:17:42,759] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:17:43,250] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:17:44,653] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:3.48878 backend_compile:0.00039
STATS: call_* op count: 519 | FakeTensorMode.__torch_dispatch__:6266 | FakeTensor.__torch_dispatch__:5304
Dynamo produced 2 graphs covering 519 ops with 4 graph breaks (3 unique)
cuda train mobilenet_v3_large                  [2023-02-17 02:17:58,711] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:17:58,890] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:17:59,738] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.83997 backend_compile:0.00041
STATS: call_* op count: 535 | FakeTensor.__torch_dispatch__:5508 | FakeTensorMode.__torch_dispatch__:5075
Dynamo produced 2 graphs covering 535 ops with 4 graph breaks (3 unique)
cuda train moco                                [2023-02-17 02:18:16,565] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:18:16,744] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:18:17,572] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: MoCo._momentum_update_key_encoder  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 130, in forward
    self._momentum_update_key_encoder()  # update the key encoder

[2023-02-17 02:18:38,404] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)
   function: '<graph break in _momentum_update_key_encoder>' (/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py:50)
   reasons:  ___tuple_iterator_len(___stack0) == 160
to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.
[2023-02-17 02:18:38,412] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: MoCo._batch_shuffle_ddp  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 133, in <graph break in forward>
    im_k, idx_unshuffle = self._batch_shuffle_ddp(im_k)

[2023-02-17 02:18:38,420] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: concat_all_gather  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 76, in _batch_shuffle_ddp
    x_gather = concat_all_gather(x)

[2023-02-17 02:18:39,708] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: MoCo._batch_unshuffle_ddp  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 139, in <graph break in forward>
    k = self._batch_unshuffle_ddp(k, idx_unshuffle)

[2023-02-17 02:18:39,733] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: concat_all_gather  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 104, in _batch_unshuffle_ddp
    x_gather = concat_all_gather(x)

[2023-02-17 02:18:39,773] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: MoCo._dequeue_and_enqueue  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 158, in <graph break in forward>
    self._dequeue_and_enqueue(k)

[2023-02-17 02:18:39,781] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: concat_all_gather  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 55, in _dequeue_and_enqueue
    keys = concat_all_gather(keys)

[2023-02-17 02:18:39,785] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(int) [TensorVariable()] {} from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/moco/moco/builder.py", line 59, in <graph break in _dequeue_and_enqueue>
    ptr = int(self.queue_ptr)

[2023-02-17 02:18:39,799] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:24.98508 backend_compile:0.67667
STATS: call_* op count: 1042 | FakeTensor.__torch_dispatch__:40923 | FakeTensorMode.__torch_dispatch__:63266
Dynamo produced 76 graphs covering 1042 ops with 77 graph breaks (10 unique)
cuda train nvidia_deeprecommender              [2023-02-17 02:18:55,497] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:18:55,826] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:18:55,901] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.52854 backend_compile:8e-05
STATS: call_* op count: 37 | FakeTensor.__torch_dispatch__:374 | FakeTensorMode.__torch_dispatch__:314
Dynamo produced 2 graphs covering 37 ops with 4 graph breaks (3 unique)
cuda train pytorch_CycleGAN_and_pix2pix        [2023-02-17 02:19:08,213] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:19:08,395] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:19:08,711] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.91661 backend_compile:0.00016
STATS: call_* op count: 187 | FakeTensorMode.__torch_dispatch__:1801 | FakeTensor.__torch_dispatch__:1540
Dynamo produced 2 graphs covering 187 ops with 4 graph breaks (3 unique)
cuda train pytorch_stargan                     [2023-02-17 02:19:26,613] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:19:26,798] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:19:27,135] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.98158 backend_compile:0.00017
STATS: call_* op count: 160 | FakeTensorMode.__torch_dispatch__:2017 | FakeTensor.__torch_dispatch__:1707
Dynamo produced 2 graphs covering 160 ops with 4 graph breaks (3 unique)
cuda train pytorch_struct                      [2023-02-17 02:19:38,560] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:19:38,738] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:19:38,869] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.54689 backend_compile:0.00011
STATS: call_* op count: 93 | FakeTensor.__torch_dispatch__:833 | FakeTensorMode.__torch_dispatch__:825
Dynamo produced 2 graphs covering 93 ops with 4 graph breaks (3 unique)
cuda train pytorch_unet                        [2023-02-17 02:19:54,922] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:19:56,070] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:19:56,471] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.19081 backend_compile:0.0002
STATS: call_* op count: 219 | FakeTensor.__torch_dispatch__:2337 | FakeTensorMode.__torch_dispatch__:2422
Dynamo produced 2 graphs covering 219 ops with 4 graph breaks (3 unique)
cuda train resnet18                            [2023-02-17 02:20:09,174] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:20:09,352] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:20:09,690] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:1.06856 backend_compile:0.00019
STATS: call_* op count: 193 | FakeTensor.__torch_dispatch__:1951 | FakeTensorMode.__torch_dispatch__:1909
Dynamo produced 2 graphs covering 193 ops with 4 graph breaks (3 unique)
cuda train resnet50                            [2023-02-17 02:20:22,880] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:20:23,058] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:20:23,874] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.50379 backend_compile:0.00036
STATS: call_* op count: 497 | FakeTensor.__torch_dispatch__:5053 | FakeTensorMode.__torch_dispatch__:4958
Dynamo produced 2 graphs covering 497 ops with 4 graph breaks (3 unique)
cuda train resnet50_quantized_qat              WARNING:common:fp64 golden ref were not generated for resnet50_quantized_qat. Setting accuracy check to cosine
[2023-02-17 02:20:38,990] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:20:39,170] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:20:40,529] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:3.02832 backend_compile:0.00042
STATS: call_* op count: 485 | FakeTensorMode.__torch_dispatch__:6326 | FakeTensor.__torch_dispatch__:5309
Dynamo produced 2 graphs covering 485 ops with 4 graph breaks (3 unique)
cuda train resnext50_32x4d                     [2023-02-17 02:20:54,518] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:20:54,696] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:20:55,514] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.50489 backend_compile:0.00039
STATS: call_* op count: 497 | FakeTensor.__torch_dispatch__:5053 | FakeTensorMode.__torch_dispatch__:4958
Dynamo produced 2 graphs covering 497 ops with 4 graph breaks (3 unique)
cuda train shufflenet_v2_x1_0                  [2023-02-17 02:21:09,784] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:21:10,117] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:21:11,061] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:3.04775 backend_compile:0.00039
STATS: call_* op count: 611 | FakeTensor.__torch_dispatch__:5335 | FakeTensorMode.__torch_dispatch__:5536
Dynamo produced 2 graphs covering 611 ops with 4 graph breaks (3 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/benchmarks.py", line 86, in <module>
    torchbench.torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1840, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 916, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2169, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 274, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/soft_actor_critic/__init__.py", line 131, in __init__
    self.train_env = load_gym(self.args.env_id, self.args.seed)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/soft_actor_critic/envs.py", line 237, in load_gym
    env.seed(seed)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/gym/core.py", line 241, in __getattr__
    return getattr(self.env, name)
  [Previous line repeated 1 more time]
AttributeError: 'PendulumEnv' object has no attribute 'seed'
ERROR
cuda train speech_transformer                  [2023-02-17 02:21:31,003] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:21:31,167] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:21:31,186] torch._dynamo.symbolic_convert: [WARNING] Graph break: Dynamic slicing on data-dependent value is not supported from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/speech_transformer/speech_transformer/transformer/transformer.py", line 28, in forward
    encoder_padded_outputs, *_ = self.encoder(padded_input, input_lengths)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/speech_transformer/speech_transformer/transformer/encoder.py", line 48, in forward
    non_pad_mask = get_non_pad_mask(padded_input, input_lengths=input_lengths)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/speech_transformer/speech_transformer/utils/utils.py", line 109, in get_non_pad_mask
    non_pad_mask[i, input_lengths[i]:] = 0

[2023-02-17 02:21:31,745] torch._dynamo.symbolic_convert: [WARNING] Graph break: dynamic Tensor.__getitem__(bool[]) from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/speech_transformer/speech_transformer/transformer/transformer.py", line 30, in <graph break in forward>
    pred, gold, *_ = self.decoder(padded_target, encoder_padded_outputs,
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/speech_transformer/speech_transformer/transformer/decoder.py", line 84, in forward
    ys_in_pad, ys_out_pad = self.preprocess(padded_input)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/speech_transformer/speech_transformer/transformer/decoder.py", line 59, in preprocess
    ys = [y[y != IGNORE_ID] for y in padded_input]  # parse padded ys
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/speech_transformer/speech_transformer/transformer/decoder.py", line 59, in <listcomp>
    ys = [y[y != IGNORE_ID] for y in padded_input]  # parse padded ys

[2023-02-17 02:21:32,757] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:4.50617 backend_compile:0.00088
STATS: call_* op count: 1263 | FakeTensorMode.__torch_dispatch__:8267 | FakeTensor.__torch_dispatch__:8174
Dynamo produced 10 graphs covering 1263 ops with 13 graph breaks (5 unique)
cuda train squeezenet1_1                       [2023-02-17 02:21:46,856] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:21:47,032] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:21:47,184] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.79618 backend_compile:0.00016
STATS: call_* op count: 170 | FakeTensor.__torch_dispatch__:1649 | FakeTensorMode.__torch_dispatch__:1048
Dynamo produced 2 graphs covering 170 ops with 4 graph breaks (3 unique)
NUMS 4.0
cuda train tacotron2                           [2023-02-17 02:22:15,801] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:22:17,205] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:22:17,295] torch._dynamo.symbolic_convert: [WARNING] Graph break: Tensor.numpy from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 507, in forward
    encoder_outputs = self.encoder(embedded_inputs, text_lengths)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 180, in forward
    input_lengths = input_lengths.cpu().numpy()

[2023-02-17 02:22:17,370] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function args: TensorVariable() NumpyVariable() ConstantVariable(bool) from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 181, in <graph break in forward>
    x = nn.utils.rnn.pack_padded_sequence(

[2023-02-17 02:22:17,394] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(print) [ConstantVariable(str), ConstantVariable(float)] {} from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 509, in <graph break in forward>
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 399, in forward
    decoder_inputs = self.parse_decoder_inputs(decoder_inputs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 310, in parse_decoder_inputs
    print("NUMS", decoder_inputs.size(1)/self.n_frames_per_step)

[2023-02-17 02:22:17,449] torch._dynamo.symbolic_convert: [WARNING] Graph break: Tensor.item from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 404, in <graph break in forward>
    memory, mask=~get_mask_from_lengths(memory_lengths))
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/utils.py", line 8, in get_mask_from_lengths
    max_len = torch.max(lengths).item()

[2023-02-17 02:22:17,473] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function UserDefinedObjectVariable(initialize_decoder_states) [TensorVariable()] {'mask': TensorVariable()} from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tacotron2/model.py", line 403, in <graph break in forward>
    self.initialize_decoder_states(

[2023-02-17 02:23:38,496] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

ERROR:common:one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4, 80, 724]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1243, in check_accuracy
    new_result = optimized_model_iter_fn(model_copy, example_inputs)
  File "/scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py", line 209, in _fn
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1119, in run_n_iterations
    self.model_iter_fn(mod, inputs, collect_outputs=False)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 370, in <graph break in forward_and_backward_pass>
    self.grad_scaler.scale(loss).backward()
  File "/scratch/voz/work/pytorch/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/scratch/voz/work/pytorch/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [4, 80, 724]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
NUMS 4.0
TorchDynamo optimized model failed to run because of following error
FAIL
TIMING: entire_frame_compile:82.12674 backend_compile:0.01587
STATS: call_* op count: 26176 | FakeTensorMode.__torch_dispatch__:207398 | FakeTensor.__torch_dispatch__:59028
Dynamo produced 13 graphs covering 26176 ops with 15 graph breaks (8 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/benchmarks.py", line 86, in <module>
    torchbench.torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1840, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 916, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2169, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 248, in load_model
    module = importlib.import_module(f"torchbenchmark.models.{model_name}")
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/timm_efficientdet/__init__.py", line 12, in <module>
    from effdet import create_model, create_loader
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/__init__.py", line 2, in <module>
    from .bench import DetBenchPredict, DetBenchTrain, unwrap_bench
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/bench.py", line 70, in <module>
    def _batch_detection(
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
  File "/scratch/voz/work/pytorch/torch/jit/_recursive.py", line 867, in try_compile_fn
    return torch.jit.script(fn, _rcb=rcb)
  File "/scratch/voz/work/pytorch/torch/jit/_script.py", line 1341, in script
    fn = torch._C._jit_script_compile(
RuntimeError: 
object has no attribute nms:
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 41
        _log_api_usage_once(nms)
    _assert_has_ops()
    return torch.ops.torchvision.nms(boxes, scores, iou_threshold)
           ~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
'nms' is being compiled since it was called from '_batched_nms_vanilla'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 109
    for class_id in torch.unique(idxs):
        curr_indices = torch.where(idxs == class_id)[0]
        curr_keep_indices = nms(boxes[curr_indices], scores[curr_indices], iou_threshold)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        keep_mask[curr_indices[curr_keep_indices]] = True
    keep_indices = torch.where(keep_mask)[0]
'_batched_nms_vanilla' is being compiled since it was called from 'batched_nms'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/ops/boxes.py", line 73
    # https://github.com/pytorch/vision/issues/1311#issuecomment-781329339
    if boxes.numel() > (4000 if boxes.device.type == "cpu" else 20000) and not torchvision._is_tracing():
        return _batched_nms_vanilla(boxes, scores, idxs, iou_threshold)
               ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
    else:
        return _batched_nms_coordinate_trick(boxes, scores, idxs, iou_threshold)
'batched_nms' is being compiled since it was called from 'generate_detections'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/anchors.py", line 140
        scores[top_detection_idx] = soft_scores
    else:
        top_detection_idx = batched_nms(boxes, scores, classes, iou_threshold=0.5)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE

    # keep only top max_det_per_image scoring predictions
'generate_detections' is being compiled since it was called from '_batch_detection'
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/effdet/bench.py", line 82
        img_scale_i = None if img_scale is None else img_scale[i]
        img_size_i = None if img_size is None else img_size[i]
        detections = generate_detections(
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            class_out[i], box_out[i], anchor_boxes, indices[i], classes[i],
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
            img_scale_i, img_size_i, max_det_per_image=max_det_per_image, soft_nms=soft_nms)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
        batch_detections.append(detections)
    return torch.stack(batch_detections, dim=0)

ERROR
cuda train timm_efficientnet                   [2023-02-17 02:24:06,051] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:24:06,278] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:24:07,740] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:4.06244 backend_compile:0.00057
STATS: call_* op count: 739 | FakeTensor.__torch_dispatch__:6595 | FakeTensorMode.__torch_dispatch__:5817
Dynamo produced 2 graphs covering 739 ops with 4 graph breaks (3 unique)
cuda train timm_nfnet                          [2023-02-17 02:24:24,768] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:24:24,986] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:24:26,932] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:4.71481 backend_compile:0.00065
STATS: call_* op count: 1081 | FakeTensorMode.__torch_dispatch__:6928 | FakeTensor.__torch_dispatch__:7085
Dynamo produced 2 graphs covering 1081 ops with 4 graph breaks (3 unique)
cuda train timm_regnet                         [2023-02-17 02:24:43,357] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:24:43,994] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:24:46,699] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:7.78578 backend_compile:0.00095
STATS: call_* op count: 1378 | FakeTensor.__torch_dispatch__:11213 | FakeTensorMode.__torch_dispatch__:10056
Dynamo produced 2 graphs covering 1378 ops with 4 graph breaks (3 unique)
cuda train timm_resnest                        [2023-02-17 02:25:05,961] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:25:06,184] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:25:06,677] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:1.45331 backend_compile:0.00022
STATS: call_* op count: 314 | FakeTensor.__torch_dispatch__:2609 | FakeTensorMode.__torch_dispatch__:2586
Dynamo produced 2 graphs covering 314 ops with 4 graph breaks (3 unique)
cuda train timm_vision_transformer             [2023-02-17 02:25:20,873] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:25:21,093] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:25:21,879] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.43036 backend_compile:0.00043
STATS: call_* op count: 677 | FakeTensor.__torch_dispatch__:4792 | FakeTensorMode.__torch_dispatch__:4739
Dynamo produced 2 graphs covering 677 ops with 4 graph breaks (3 unique)
cuda train timm_vovnet                         [2023-02-17 02:25:37,188] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:25:37,406] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:25:38,413] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:2.31398 backend_compile:0.00034
STATS: call_* op count: 407 | FakeTensor.__torch_dispatch__:3581 | FakeTensorMode.__torch_dispatch__:3588
Dynamo produced 2 graphs covering 407 ops with 4 graph breaks (3 unique)
cuda train tts_angular                         [2023-02-17 02:25:49,682] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:25:50,268] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:25:50,279] torch._dynamo.symbolic_convert: [WARNING] Graph break: TorchDynamo purposely graph breaks on RNN, GRU, LSTMs from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tts_angular/model.py", line 59, in forward
    d = self.layers(x)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/tts_angular/model.py", line 17, in forward
    self.lstm.flatten_parameters()

[2023-02-17 02:25:50,303] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:0.769 backend_compile:9e-05
STATS: call_* op count: 32 | FakeTensor.__torch_dispatch__:421 | FakeTensorMode.__torch_dispatch__:188
Dynamo produced 2 graphs covering 32 ops with 5 graph breaks (4 unique)
cuda train vgg16                               [2023-02-17 02:26:03,423] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:26:04,470] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:26:04,563] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:1.42452 backend_compile:0.00012
STATS: call_* op count: 104 | FakeTensor.__torch_dispatch__:1016 | FakeTensorMode.__torch_dispatch__:680
Dynamo produced 2 graphs covering 104 ops with 4 graph breaks (3 unique)
Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/benchmarks.py", line 86, in <module>
    torchbench.torchbench_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 381, in torchbench_main
    main(TorchBenchmarkRunner(), original_dir)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1840, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 916, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2169, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 274, in load_model
    benchmark = benchmark_cls(
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/util/model.py", line 13, in __call__
    obj = type.__call__(cls, *args, **kwargs)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/vision_maskrcnn/__init__.py", line 68, in __init__
    dataset = CocoDetection(root=os.path.join(DATA_DIR, COCO_DATA[COCO_DATA_KEY][0]),
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/datasets/coco.py", line 33, in __init__
    super().__init__(root, transforms, transform, target_transform)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/datasets/vision.py", line 39, in __init__
    if isinstance(root, torch._six.string_classes):
AttributeError: module 'torch' has no attribute '_six'
ERROR
cuda train yolov3                              [2023-02-17 02:26:35,096] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 365, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:26:35,312] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 366, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:26:37,711] torch._dynamo.symbolic_convert: [WARNING] Graph break: setattr(UserDefinedObjectVariable) <function Module.__setattr__ at 0x7fc9fd799f30> from user code at   File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/yolov3/yolo_models.py", line 238, in forward
    return self.forward_once(x)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/yolov3/yolo_models.py", line 290, in forward_once
    yolo_out.append(module(x, out))
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/yolov3/yolo_models.py", line 188, in forward
    self.create_grids((nx, ny), p.device)
  File "/scratch/voz/work/torchbenchmark/torchbenchmark/models/yolov3/yolo_models.py", line 150, in create_grids
    self.nx, self.ny = ng  # x and y grid size

[2023-02-17 02:26:40,782] torch._dynamo.convert_frame: [WARNING] torch._dynamo hit config.cache_size_limit (64)
   function: 'forward' (/scratch/voz/work/pytorch/torch/nn/modules/container.py:215)
   reasons:  ___check_obj_id(self, 140508172909824)
to diagnose recompilation issues, see https://pytorch.org/docs/master/dynamo/troubleshooting.html.
[2023-02-17 02:26:40,940] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function reduce_to_scalar_loss in skip_files /scratch/voz/work/pytorch/torch/_dynamo/testing.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/torchbench.py", line 358, in compute_loss
    return reduce_to_scalar_loss(pred)

PASS
TIMING: entire_frame_compile:8.43962 backend_compile:0.0026
STATS: call_* op count: 690 | FakeTensor.__torch_dispatch__:12299 | FakeTensorMode.__torch_dispatch__:17225
Dynamo produced 94 graphs covering 690 ops with 5 graph breaks (4 unique)
cuda train AlbertForMaskedLM                   [2023-02-17 02:27:03,075] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:27:04,482] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.06701 backend_compile:0.00026
STATS: call_* op count: 495 | FakeTensorMode.__torch_dispatch__:4767 | FakeTensor.__torch_dispatch__:1775
Dynamo produced 2 graphs covering 495 ops with 3 graph breaks (2 unique)
cuda train AlbertForQuestionAnswering          [2023-02-17 02:27:22,858] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:27:23,048] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:1.80731 backend_compile:0.00021
STATS: call_* op count: 489 | FakeTensorMode.__torch_dispatch__:4746 | FakeTensor.__torch_dispatch__:1678
Dynamo produced 2 graphs covering 489 ops with 3 graph breaks (2 unique)
cuda train AllenaiLongformerBase               [2023-02-17 02:27:39,399] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:27:39,592] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:27:39,931] torch._dynamo.symbolic_convert: [WARNING] Graph break: Tensor.item from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1849, in forward
    outputs = self.longformer(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1751, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1295, in forward
    is_global_attn = is_index_global_attn.flatten().any().item()

[2023-02-17 02:27:40,356] torch._dynamo.symbolic_convert: [WARNING] Graph break: const method call dict.get from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1250, in forward
    self_attn_outputs = self.attention(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 1186, in forward
    self_outputs = self.self(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 575, in forward
    attn_scores = self._sliding_chunks_query_key_matmul(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 840, in _sliding_chunks_query_key_matmul
    query = self._chunk(query, window_overlap, self.config.__dict__.get("onnx_export", False))

[2023-02-17 02:27:40,483] torch._dynamo.symbolic_convert: [WARNING] Graph break: const method call dict.get from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 841, in <graph break in _sliding_chunks_query_key_matmul>
    key = self._chunk(key, window_overlap, self.config.__dict__.get("onnx_export", False))

[2023-02-17 02:27:40,518] torch._dynamo.symbolic_convert: [WARNING] Graph break: data dependent operator: aten._local_scalar_dense.default from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/longformer/modeling_longformer.py", line 859, in <graph break in _sliding_chunks_query_key_matmul>
    diagonal_attention_scores = diagonal_chunked_attention_scores.new_zeros(

[2023-02-17 02:27:45,966] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:27:45,970] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:8.79413 backend_compile:0.00448
STATS: call_* op count: 2117 | FakeTensorMode.__torch_dispatch__:19076 | FakeTensor.__torch_dispatch__:10583
Dynamo produced 160 graphs covering 2117 ops with 129 graph breaks (8 unique)
cuda train BartForCausalLM                     [2023-02-17 02:28:05,434] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:28:06,863] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:28:07,154] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1869, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1123, in forward
    next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

[2023-02-17 02:28:08,503] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:28:08,530] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:5.51219 backend_compile:0.00087
STATS: call_* op count: 874 | FakeTensorMode.__torch_dispatch__:7621 | FakeTensor.__torch_dispatch__:7574
Dynamo produced 15 graphs covering 874 ops with 10 graph breaks (7 unique)
cuda train BartForConditionalGeneration        [2023-02-17 02:28:35,493] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:28:35,706] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:10.64329 backend_compile:0.00123
STATS: call_* op count: 2280 | FakeTensorMode.__torch_dispatch__:15807 | FakeTensor.__torch_dispatch__:16123
Dynamo produced 2 graphs covering 2280 ops with 3 graph breaks (2 unique)
cuda train BertForMaskedLM                     [2023-02-17 02:28:59,694] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:29:00,116] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.93703 backend_compile:0.00048
STATS: call_* op count: 774 | FakeTensorMode.__torch_dispatch__:6925 | FakeTensor.__torch_dispatch__:6766
Dynamo produced 2 graphs covering 774 ops with 3 graph breaks (2 unique)
cuda train BertForQuestionAnswering            [2023-02-17 02:29:18,003] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:29:18,334] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.82536 backend_compile:0.00051
STATS: call_* op count: 775 | FakeTensorMode.__torch_dispatch__:6919 | FakeTensor.__torch_dispatch__:6669
Dynamo produced 2 graphs covering 775 ops with 3 graph breaks (2 unique)
cuda train BlenderbotForCausalLM               PASS
TIMING:
STATS: call_* op count: 0
Dynamo produced 0 graphs covering 0 ops with 0 graph breaks (0 unique)
cuda train BlenderbotSmallForCausalLM          [2023-02-17 02:30:31,701] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:30:31,895] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:30:32,058] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py", line 1537, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/blenderbot_small/modeling_blenderbot_small.py", line 1051, in forward
    next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

[2023-02-17 02:30:33,090] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:30:33,108] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:2.83767 backend_compile:0.00057
STATS: call_* op count: 591 | FakeTensorMode.__torch_dispatch__:5359 | FakeTensor.__torch_dispatch__:5158
Dynamo produced 11 graphs covering 591 ops with 10 graph breaks (7 unique)
cuda train BlenderbotSmallForConditionalGeneration  [2023-02-17 02:30:48,874] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:30:49,064] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.53511 backend_compile:0.00094
STATS: call_* op count: 1526 | FakeTensorMode.__torch_dispatch__:10598 | FakeTensor.__torch_dispatch__:10827
Dynamo produced 2 graphs covering 1526 ops with 3 graph breaks (2 unique)
cuda train CamemBert                           [2023-02-17 02:31:08,862] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:31:09,051] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.85489 backend_compile:0.00053
STATS: call_* op count: 781 | FakeTensorMode.__torch_dispatch__:6938 | FakeTensor.__torch_dispatch__:6766
Dynamo produced 2 graphs covering 781 ops with 3 graph breaks (2 unique)
cuda train DebertaForMaskedLM                  [2023-02-17 02:31:26,178] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:31:27,997] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:31:28,108] torch._dynamo.symbolic_convert: [WARNING] Graph break: autograd.Function with requires_grad from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1089, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 999, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 492, in forward
    hidden_states = layer_module(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 397, in forward
    attention_output = self.attention(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 330, in forward
    self_output = self.self(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 692, in forward
    attention_probs = XSoftmax.apply(attention_scores, attention_mask, -1)

[2023-02-17 02:31:28,518] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function args: TensorVariable() GetAttrVariable(AutogradFunctionContextVariable(), dim)  from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 136, in <graph break in forward>
    output = torch.softmax(output, self.dim)

[2023-02-17 02:31:28,524] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method AutogradFunctionContextVariable() save_for_backward [TensorVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 138, in <graph break in forward>
    self.save_for_backward(output)

[2023-02-17 02:31:30,699] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:31:30,713] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:6.25967 backend_compile:0.0017
STATS: call_* op count: 1042 | FakeTensorMode.__torch_dispatch__:9438 | FakeTensor.__torch_dispatch__:6196
Dynamo produced 54 graphs covering 1042 ops with 50 graph breaks (10 unique)
cuda train DebertaForQuestionAnswering         [2023-02-17 02:31:45,659] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:31:45,850] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:31:45,962] torch._dynamo.symbolic_convert: [WARNING] Graph break: autograd.Function with requires_grad from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 1425, in forward
    outputs = self.deberta(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 999, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 492, in forward
    hidden_states = layer_module(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 397, in forward
    attention_output = self.attention(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 330, in forward
    self_output = self.self(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 692, in forward
    attention_probs = XSoftmax.apply(attention_scores, attention_mask, -1)

[2023-02-17 02:31:46,381] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function args: TensorVariable() GetAttrVariable(AutogradFunctionContextVariable(), dim)  from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 136, in <graph break in forward>
    output = torch.softmax(output, self.dim)

[2023-02-17 02:31:46,387] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method AutogradFunctionContextVariable() save_for_backward [TensorVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta/modeling_deberta.py", line 138, in <graph break in forward>
    self.save_for_backward(output)

[2023-02-17 02:31:48,585] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:31:48,598] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:4.64221 backend_compile:0.00173
STATS: call_* op count: 1043 | FakeTensorMode.__torch_dispatch__:9432 | FakeTensor.__torch_dispatch__:6099
Dynamo produced 54 graphs covering 1043 ops with 50 graph breaks (10 unique)
Traceback (most recent call last):
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1093, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 170, in <module>
    @torch._dynamo.mark_unguarded
AttributeError: module 'torch._dynamo' has no attribute 'mark_unguarded'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/benchmarks.py", line 82, in <module>
    huggingface.huggingface_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 591, in huggingface_main
    main(HuggingfaceRunner())
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1840, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 916, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2169, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 382, in load_model
    model_cls = get_module_cls_by_model_name(model_name)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 175, in get_module_cls_by_model_name
    return getattr(module, model_cls_name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1084, in __getattr__
    value = getattr(module, name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1083, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1095, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.deberta_v2.modeling_deberta_v2 because of the following error (look up to see its traceback):
module 'torch._dynamo' has no attribute 'mark_unguarded'
ERROR
Traceback (most recent call last):
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1093, in _get_module
    return importlib.import_module("." + module_name, self.__name__)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/importlib/__init__.py", line 126, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "<frozen importlib._bootstrap>", line 1050, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1027, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1006, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 688, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 883, in exec_module
  File "<frozen importlib._bootstrap>", line 241, in _call_with_frames_removed
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py", line 170, in <module>
    @torch._dynamo.mark_unguarded
AttributeError: module 'torch._dynamo' has no attribute 'mark_unguarded'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/benchmarks.py", line 82, in <module>
    huggingface.huggingface_main()
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 591, in huggingface_main
    main(HuggingfaceRunner())
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1840, in main
    return maybe_fresh_cache(run, args.cold_start_latency and args.only)(
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 916, in inner
    return fn(*args, **kwargs)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 2169, in run
    ) = runner.load_model(device, model_name, batch_size=batch_size)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 382, in load_model
    model_cls = get_module_cls_by_model_name(model_name)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 175, in get_module_cls_by_model_name
    return getattr(module, model_cls_name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1084, in __getattr__
    value = getattr(module, name)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1083, in __getattr__
    module = self._get_module(self._class_to_module[name])
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/import_utils.py", line 1095, in _get_module
    raise RuntimeError(
RuntimeError: Failed to import transformers.models.deberta_v2.modeling_deberta_v2 because of the following error (look up to see its traceback):
module 'torch._dynamo' has no attribute 'mark_unguarded'
ERROR
WARNING:huggingface:Sequence Length not defined for DistilBertForMaskedLM. Choosing 128 arbitrarily
cuda train DistilBertForMaskedLM               [2023-02-17 02:32:17,333] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:32:17,514] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:1.82354 backend_compile:0.0003
STATS: call_* op count: 416 | FakeTensorMode.__torch_dispatch__:3309 | FakeTensor.__torch_dispatch__:3316
Dynamo produced 2 graphs covering 416 ops with 3 graph breaks (2 unique)
WARNING:huggingface:Sequence Length not defined for DistilBertForQuestionAnswering. Choosing 128 arbitrarily
cuda train DistilBertForQuestionAnswering      [2023-02-17 02:32:30,192] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:32:31,710] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.13514 backend_compile:0.00034
STATS: call_* op count: 418 | FakeTensorMode.__torch_dispatch__:3303 | FakeTensor.__torch_dispatch__:3219
Dynamo produced 2 graphs covering 418 ops with 3 graph breaks (2 unique)
cuda train DistillGPT2                         [2023-02-17 02:32:45,547] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:32:45,736] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:1.70751 backend_compile:0.00029
STATS: call_* op count: 482 | FakeTensorMode.__torch_dispatch__:2703 | FakeTensor.__torch_dispatch__:2308
Dynamo produced 2 graphs covering 482 ops with 3 graph breaks (2 unique)
If you want to use `ElectraForCausalLM` as a standalone, add `is_decoder=True.`
cuda train ElectraForCausalLM                  [2023-02-17 02:32:57,539] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:32:57,723] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:32:59,097] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method UserDefinedObjectVariable(ClassInstantier) __contains__ [ConstantVariable(str)] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 1645, in forward
    prediction_scores = self.generator_lm_head(self.generator_predictions(sequence_output))
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/electra/modeling_electra.py", line 659, in forward
    hidden_states = get_activation("gelu")(hidden_states)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/activations.py", line 172, in get_activation
    if activation_string in ACT2FN:

[2023-02-17 02:32:59,155] torch._dynamo.symbolic_convert: [WARNING] Graph break: Patched init cannot be inlined. from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/activations.py", line 47, in __init__
    super().__init__()

PASS
TIMING: entire_frame_compile:3.82761 backend_compile:0.00061
STATS: call_* op count: 783 | FakeTensorMode.__torch_dispatch__:7050 | FakeTensor.__torch_dispatch__:6869
Dynamo produced 5 graphs covering 783 ops with 6 graph breaks (4 unique)
cuda train ElectraForQuestionAnswering         [2023-02-17 02:33:11,968] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:33:12,151] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.68769 backend_compile:0.00049
STATS: call_* op count: 780 | FakeTensorMode.__torch_dispatch__:6981 | FakeTensor.__torch_dispatch__:6736
Dynamo produced 2 graphs covering 780 ops with 3 graph breaks (2 unique)
cuda train GPT2ForSequenceClassification       [2023-02-17 02:33:28,550] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:33:28,734] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.13025 backend_compile:0.00053
STATS: call_* op count: 941 | FakeTensorMode.__torch_dispatch__:5225 | FakeTensor.__torch_dispatch__:4639
Dynamo produced 3 graphs covering 941 ops with 4 graph breaks (3 unique)
cuda train GoogleFnet                          [2023-02-17 02:33:44,978] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:33:45,192] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:33:45,245] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function UserDefinedObjectVariable(partial) [TensorVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 763, in forward
    outputs = self.fnet(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 600, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 304, in forward
    layer_outputs = layer_module(hidden_states)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 263, in forward
    self_fourier_outputs = self.fourier(hidden_states)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 216, in forward
    self_outputs = self.self(hidden_states)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/fnet/modeling_fnet.py", line 195, in forward
    outputs = self.fourier_transform(hidden_states).real

[2023-02-17 02:33:46,042] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:33:46,046] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:2.09022 backend_compile:0.00088
STATS: call_* op count: 425 | FakeTensorMode.__torch_dispatch__:3023 | FakeTensor.__torch_dispatch__:3839
Dynamo produced 29 graphs covering 425 ops with 44 graph breaks (6 unique)
cuda train LayoutLMForMaskedLM                 [2023-02-17 02:33:59,669] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:33:59,854] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.1137 backend_compile:0.00052
STATS: call_* op count: 808 | FakeTensorMode.__torch_dispatch__:7788 | FakeTensor.__torch_dispatch__:7339
Dynamo produced 2 graphs covering 808 ops with 3 graph breaks (2 unique)
cuda train LayoutLMForSequenceClassification   [2023-02-17 02:34:16,227] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:34:16,412] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.00883 backend_compile:0.00052
STATS: call_* op count: 804 | FakeTensorMode.__torch_dispatch__:7713 | FakeTensor.__torch_dispatch__:7299
Dynamo produced 3 graphs covering 804 ops with 4 graph breaks (3 unique)
WARNING:huggingface:Sequence Length not defined for M2M100ForConditionalGeneration. Choosing 128 arbitrarily
cuda train M2M100ForConditionalGeneration      [2023-02-17 02:34:48,599] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:34:50,137] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:34:50,167] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: M2M100SinusoidalPositionalEmbedding.forward  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1326, in forward
    outputs = self.model(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1199, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 780, in forward
    embed_pos = self.embed_positions(input_ids, inputs_embeds)

[2023-02-17 02:34:51,329] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: M2M100SinusoidalPositionalEmbedding.forward  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1217, in <graph break in forward>
    decoder_outputs = self.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/m2m_100/modeling_m2m_100.py", line 1014, in forward
    positions = self.embed_positions(input_ids, inputs_embeds, past_key_values_length)

PASS
TIMING: entire_frame_compile:12.53879 backend_compile:0.00176
STATS: call_* op count: 2284 | FakeTensorMode.__torch_dispatch__:16883 | FakeTensor.__torch_dispatch__:18114
Dynamo produced 20 graphs covering 2284 ops with 16 graph breaks (6 unique)
cuda train MBartForCausalLM                    [2023-02-17 02:35:17,464] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:35:17,659] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:35:17,938] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 1853, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/mbart/modeling_mbart.py", line 1118, in forward
    next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

[2023-02-17 02:35:19,228] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:35:19,254] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:4.13748 backend_compile:0.00081
STATS: call_* op count: 877 | FakeTensorMode.__torch_dispatch__:7639 | FakeTensor.__torch_dispatch__:7628
Dynamo produced 15 graphs covering 877 ops with 10 graph breaks (7 unique)
cuda train MBartForConditionalGeneration       [2023-02-17 02:35:44,686] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:35:44,867] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:10.76803 backend_compile:0.00133
STATS: call_* op count: 2293 | FakeTensorMode.__torch_dispatch__:15876 | FakeTensor.__torch_dispatch__:16248
Dynamo produced 2 graphs covering 2293 ops with 3 graph breaks (2 unique)
WARNING:huggingface:Sequence Length not defined for MT5ForConditionalGeneration. Choosing 128 arbitrarily
cuda train MT5ForConditionalGeneration         [2023-02-17 02:36:11,900] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:36:12,088] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:5.15422 backend_compile:0.00067
STATS: call_* op count: 1553 | FakeTensorMode.__torch_dispatch__:9938 | FakeTensor.__torch_dispatch__:6937
Dynamo produced 2 graphs covering 1553 ops with 3 graph breaks (2 unique)
If you want to use `MegatronBertForCausalLM` as a standalone, add `is_decoder=True.`
cuda train MegatronBertForCausalLM             [2023-02-17 02:36:34,673] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:36:34,859] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:8.02017 backend_compile:0.00099
STATS: call_* op count: 1509 | FakeTensorMode.__torch_dispatch__:13553 | FakeTensor.__torch_dispatch__:13198
Dynamo produced 2 graphs covering 1509 ops with 3 graph breaks (2 unique)
cuda train MegatronBertForQuestionAnswering    [2023-02-17 02:36:59,968] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:37:00,968] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:8.87215 backend_compile:0.00098
STATS: call_* op count: 1506 | FakeTensorMode.__torch_dispatch__:13525 | FakeTensor.__torch_dispatch__:13101
Dynamo produced 2 graphs covering 1506 ops with 3 graph breaks (2 unique)
cuda train MobileBertForMaskedLM               [2023-02-17 02:37:21,983] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:37:22,172] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method NNModuleVariable() zero_grad [ConstantVariable(bool)] {} from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1126, in optimizer_zero_grad
    mod.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.81516 backend_compile:0.00059
STATS: call_* op count: 1447 | FakeTensorMode.__torch_dispatch__:20953 | FakeTensor.__torch_dispatch__:6335
Dynamo produced 1 graphs covering 1447 ops with 3 graph breaks (2 unique)
cuda train MobileBertForQuestionAnswering      [2023-02-17 02:37:41,475] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:37:41,657] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method NNModuleVariable() zero_grad [ConstantVariable(bool)] {} from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1126, in optimizer_zero_grad
    mod.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.76576 backend_compile:0.00054
STATS: call_* op count: 1451 | FakeTensorMode.__torch_dispatch__:20981 | FakeTensor.__torch_dispatch__:6321
Dynamo produced 1 graphs covering 1451 ops with 3 graph breaks (2 unique)
cuda train OPTForCausalLM                      [2023-02-17 02:38:03,281] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:38:04,261] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:38:04,432] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 934, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/opt/modeling_opt.py", line 710, in forward
    next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)

[2023-02-17 02:38:05,873] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:38:05,898] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:4.56583 backend_compile:0.00082
STATS: call_* op count: 925 | FakeTensorMode.__torch_dispatch__:6957 | FakeTensor.__torch_dispatch__:7107
Dynamo produced 15 graphs covering 925 ops with 10 graph breaks (7 unique)
cuda train PLBartForCausalLM                   [2023-02-17 02:38:20,285] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:38:21,037] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:38:21,198] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py", line 1698, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py", line 1095, in forward
    next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

[2023-02-17 02:38:22,020] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:38:22,035] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:2.81017 backend_compile:0.00047
STATS: call_* op count: 454 | FakeTensorMode.__torch_dispatch__:4249 | FakeTensor.__torch_dispatch__:3950
Dynamo produced 9 graphs covering 454 ops with 10 graph breaks (7 unique)
cuda train PLBartForConditionalGeneration      [2023-02-17 02:38:38,542] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:38:38,735] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:38:39,624] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py", line 1327, in forward
    outputs = self.model(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py", line 1211, in forward
    decoder_outputs = self.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/plbart/modeling_plbart.py", line 1095, in forward
    next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

PASS
TIMING: entire_frame_compile:5.97338 backend_compile:0.00089
STATS: call_* op count: 1176 | FakeTensorMode.__torch_dispatch__:11899 | FakeTensor.__torch_dispatch__:10008
Dynamo produced 11 graphs covering 1176 ops with 13 graph breaks (6 unique)
WARNING:huggingface:Sequence Length not defined for PegasusForCausalLM. Choosing 128 arbitrarily
cuda train PegasusForCausalLM                  [2023-02-17 02:39:01,580] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:39:01,776] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:39:01,824] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: PegasusSinusoidalPositionalEmbedding.forward  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1667, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1035, in forward
    positions = self.embed_positions(input_shape, past_key_values_length)

[2023-02-17 02:39:03,237] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:39:03,263] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:3.99367 backend_compile:0.00086
STATS: call_* op count: 868 | FakeTensorMode.__torch_dispatch__:7253 | FakeTensor.__torch_dispatch__:7462
Dynamo produced 17 graphs covering 868 ops with 11 graph breaks (7 unique)
WARNING:huggingface:Sequence Length not defined for PegasusForConditionalGeneration. Choosing 128 arbitrarily
cuda train PegasusForConditionalGeneration     [2023-02-17 02:39:31,577] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:39:32,328] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method NNModuleVariable() zero_grad [ConstantVariable(bool)] {} from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1126, in optimizer_zero_grad
    mod.zero_grad(True)

[2023-02-17 02:39:32,358] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: PegasusSinusoidalPositionalEmbedding.forward  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1405, in forward
    outputs = self.model(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1242, in forward
    encoder_outputs = self.encoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 763, in forward
    embed_pos = self.embed_positions(input_shape)

[2023-02-17 02:39:33,504] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: PegasusSinusoidalPositionalEmbedding.forward  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1260, in <graph break in forward>
    decoder_outputs = self.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/pegasus/modeling_pegasus.py", line 1035, in forward
    positions = self.embed_positions(input_shape, past_key_values_length)

PASS
TIMING: entire_frame_compile:3.90055 backend_compile:0.00044
STATS: call_* op count: 1244 | FakeTensorMode.__torch_dispatch__:10247 | FakeTensor.__torch_dispatch__:2425
Dynamo produced 7 graphs covering 1244 ops with 14 graph breaks (5 unique)
If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`
cuda train RobertaForCausalLM                  [2023-02-17 02:39:48,071] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:39:48,249] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.75443 backend_compile:0.00052
STATS: call_* op count: 785 | FakeTensorMode.__torch_dispatch__:6957 | FakeTensor.__torch_dispatch__:6766
Dynamo produced 2 graphs covering 785 ops with 3 graph breaks (2 unique)
cuda train RobertaForQuestionAnswering         [2023-02-17 02:40:04,021] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:40:04,209] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.64356 backend_compile:0.00052
STATS: call_* op count: 782 | FakeTensorMode.__torch_dispatch__:6929 | FakeTensor.__torch_dispatch__:6669
Dynamo produced 2 graphs covering 782 ops with 3 graph breaks (2 unique)
WARNING:huggingface:Sequence Length not defined for Speech2Text2ForCausalLM. Choosing 128 arbitrarily
cuda train Speech2Text2ForCausalLM             [2023-02-17 02:40:18,900] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:40:19,085] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:40:19,132] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Speech2Text2SinusoidalPositionalEmbedding.forward  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py", line 912, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/speech_to_text_2/modeling_speech_to_text_2.py", line 623, in forward
    positions = self.embed_positions(input_ids, past_key_values_length=past_key_values_length)

[2023-02-17 02:40:19,976] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:40:19,991] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:2.12209 backend_compile:0.0005
STATS: call_* op count: 455 | FakeTensorMode.__torch_dispatch__:3888 | FakeTensor.__torch_dispatch__:3786
Dynamo produced 11 graphs covering 455 ops with 11 graph breaks (7 unique)
cuda train T5ForConditionalGeneration          [2023-02-17 02:40:32,611] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:40:33,371] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.22715 backend_compile:0.00052
STATS: call_* op count: 1060 | FakeTensorMode.__torch_dispatch__:6927 | FakeTensor.__torch_dispatch__:4800
Dynamo produced 2 graphs covering 1060 ops with 3 graph breaks (2 unique)
cuda train T5Small                             [2023-02-17 02:40:48,733] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:40:48,919] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.63431 backend_compile:0.00049
STATS: call_* op count: 1060 | FakeTensorMode.__torch_dispatch__:6927 | FakeTensor.__torch_dispatch__:4800
Dynamo produced 2 graphs covering 1060 ops with 3 graph breaks (2 unique)
cuda train TrOCRForCausalLM                    [2023-02-17 02:41:08,413] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:41:08,607] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:41:08,886] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function BuiltinVariable(iadd) [ConstantVariable(tuple), TupleVariable()] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py", line 960, in forward
    outputs = self.model.decoder(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/trocr/modeling_trocr.py", line 736, in forward
    next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)

[2023-02-17 02:41:10,170] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:41:10,195] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:4.11644 backend_compile:0.00085
STATS: call_* op count: 873 | FakeTensorMode.__torch_dispatch__:7617 | FakeTensor.__torch_dispatch__:7574
Dynamo produced 15 graphs covering 873 ops with 10 graph breaks (7 unique)
WARNING:huggingface:Sequence Length not defined for XGLMForCausalLM. Choosing 128 arbitrarily
cuda train XGLMForCausalLM                     [2023-02-17 02:41:37,843] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:41:38,604] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:41:38,651] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: XGLMSinusoidalPositionalEmbedding.forward  | decorate_context /scratch/voz/work/pytorch/torch/utils/_contextlib.py from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 892, in forward
    outputs = self.model(
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/xglm/modeling_xglm.py", line 713, in forward
    positions = self.embed_positions(input_ids, inputs_embeds, past_key_values_length)

[2023-02-17 02:41:41,163] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setitem__' of 'collections.OrderedDict' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 283, in __setitem__
    super().__setitem__(key, value)

[2023-02-17 02:41:41,208] torch._dynamo.symbolic_convert: [WARNING] Graph break: non-function or method super: <slot wrapper '__setattr__' of 'object' objects> from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/utils/generic.py", line 285, in <graph break in __setitem__>
    super().__setattr__(key, value)

PASS
TIMING: entire_frame_compile:8.21995 backend_compile:0.00154
STATS: call_* op count: 1772 | FakeTensorMode.__torch_dispatch__:12654 | FakeTensor.__torch_dispatch__:13748
Dynamo produced 29 graphs covering 1772 ops with 11 graph breaks (7 unique)
cuda train XLNetLMHeadModel                    [2023-02-17 02:42:03,903] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:42:04,085] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:8.51711 backend_compile:0.00096
STATS: call_* op count: 1540 | FakeTensorMode.__torch_dispatch__:19885 | FakeTensor.__torch_dispatch__:13045
Dynamo produced 2 graphs covering 1540 ops with 3 graph breaks (2 unique)
cuda train YituTechConvBert                    [2023-02-17 02:42:27,027] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 488, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:42:27,214] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/huggingface.py", line 489, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:42:29,276] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method UserDefinedObjectVariable(ClassInstantier) __contains__ [ConstantVariable(str)] {} from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/convbert/modeling_convbert.py", line 941, in forward
    prediction_scores = self.generator_predictions(generator_sequence_output)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/models/convbert/modeling_convbert.py", line 874, in forward
    hidden_states = get_activation("gelu")(hidden_states)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/activations.py", line 172, in get_activation
    if activation_string in ACT2FN:

[2023-02-17 02:42:29,358] torch._dynamo.symbolic_convert: [WARNING] Graph break: Patched init cannot be inlined. from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/transformers/activations.py", line 47, in __init__
    super().__init__()

PASS
TIMING: entire_frame_compile:5.49034 backend_compile:0.00075
STATS: call_* op count: 1206 | FakeTensorMode.__torch_dispatch__:10782 | FakeTensor.__torch_dispatch__:9622
Dynamo produced 5 graphs covering 1206 ops with 6 graph breaks (4 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train adv_inception_v3                    [2023-02-17 02:42:50,891] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:42:51,107] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.85841 backend_compile:0.00068
STATS: call_* op count: 883 | FakeTensor.__torch_dispatch__:8908 | FakeTensorMode.__torch_dispatch__:8603
Dynamo produced 3 graphs covering 883 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train beit_base_patch16_224               [2023-02-17 02:43:10,298] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:43:10,522] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.3109 backend_compile:0.00054
STATS: call_* op count: 868 | FakeTensor.__torch_dispatch__:6190 | FakeTensorMode.__torch_dispatch__:5926
Dynamo produced 3 graphs covering 868 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train botnet26t_256                       [2023-02-17 02:43:28,193] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:43:28,415] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.37266 backend_compile:0.00036
STATS: call_* op count: 540 | FakeTensor.__torch_dispatch__:3048 | FakeTensorMode.__torch_dispatch__:3645
Dynamo produced 3 graphs covering 540 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train cait_m36_384                        [2023-02-17 02:43:52,098] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:43:52,661] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:15.60103 backend_compile:0.0018
STATS: call_* op count: 2727 | FakeTensor.__torch_dispatch__:21683 | FakeTensorMode.__torch_dispatch__:20642
Dynamo produced 3 graphs covering 2727 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train coat_lite_mini                      [2023-02-17 02:44:21,826] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:44:22,850] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.54252 backend_compile:0.00046
STATS: call_* op count: 765 | FakeTensor.__torch_dispatch__:4930 | FakeTensorMode.__torch_dispatch__:5525
Dynamo produced 3 graphs covering 765 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train convit_base                         WARNING:common:fp64 golden ref were not generated for convit_base. Setting accuracy check to cosine
[2023-02-17 02:44:39,972] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:44:40,661] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

[2023-02-17 02:44:40,718] torch._dynamo.symbolic_convert: [WARNING] Graph break: dynamic shape operator: aten.repeat_interleave.Tensor from user code at   File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/timm/models/convit.py", line 333, in forward
    x = self.forward_features(x)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/timm/models/convit.py", line 323, in forward_features
    x = blk(x)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/timm/models/convit.py", line 214, in forward
    x = x + self.drop_path(self.attn(self.norm1(x)))
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/timm/models/convit.py", line 86, in forward
    self.rel_indices = self.get_rel_indices(N)
  File "/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/timm/models/convit.py", line 138, in get_rel_indices
    indy = ind.repeat_interleave(img_size, dim=0).repeat_interleave(img_size, dim=1)

PASS
TIMING: entire_frame_compile:4.57482 backend_compile:0.00176
STATS: call_* op count: 1249 | FakeTensor.__torch_dispatch__:6300 | FakeTensorMode.__torch_dispatch__:8880
Dynamo produced 56 graphs covering 1249 ops with 18 graph breaks (3 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train convmixer_768_32                    [2023-02-17 02:44:58,752] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:44:59,313] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method NNModuleVariable() zero_grad [ConstantVariable(bool)] {} from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1126, in optimizer_zero_grad
    mod.zero_grad(True)

PASS
TIMING: entire_frame_compile:1.77433 backend_compile:0.00012
STATS: call_* op count: 232 | FakeTensor.__torch_dispatch__:1123 | FakeTensorMode.__torch_dispatch__:4191
Dynamo produced 2 graphs covering 232 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train convnext_base                       [2023-02-17 02:45:15,717] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:45:15,938] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:5.69414 backend_compile:0.00078
STATS: call_* op count: 1184 | FakeTensor.__torch_dispatch__:10623 | FakeTensorMode.__torch_dispatch__:7696
Dynamo produced 3 graphs covering 1184 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train crossvit_9_240                      [2023-02-17 02:45:35,483] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:45:35,705] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.62223 backend_compile:0.0007
STATS: call_* op count: 1137 | FakeTensor.__torch_dispatch__:8455 | FakeTensorMode.__torch_dispatch__:8855
Dynamo produced 3 graphs covering 1137 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train cspdarknet53                        [2023-02-17 02:45:55,291] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:45:55,510] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.19096 backend_compile:0.00061
STATS: call_* op count: 846 | FakeTensor.__torch_dispatch__:6169 | FakeTensorMode.__torch_dispatch__:6094
Dynamo produced 3 graphs covering 846 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train deit_base_distilled_patch16_224     [2023-02-17 02:46:15,399] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:46:15,620] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.54878 backend_compile:0.00041
STATS: call_* op count: 688 | FakeTensor.__torch_dispatch__:4883 | FakeTensorMode.__torch_dispatch__:4892
Dynamo produced 3 graphs covering 688 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train dla102                              [2023-02-17 02:46:32,624] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:46:33,642] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.42197 backend_compile:0.00074
STATS: call_* op count: 1051 | FakeTensor.__torch_dispatch__:9945 | FakeTensorMode.__torch_dispatch__:9643
Dynamo produced 3 graphs covering 1051 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train dm_nfnet_f0                         [2023-02-17 02:46:54,598] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:46:55,282] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:5.15637 backend_compile:0.00065
STATS: call_* op count: 1083 | FakeTensorMode.__torch_dispatch__:6955 | FakeTensor.__torch_dispatch__:7086
Dynamo produced 3 graphs covering 1083 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train dpn107                              [2023-02-17 02:47:16,646] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:47:16,869] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:7.41409 backend_compile:0.00094
STATS: call_* op count: 1412 | FakeTensor.__torch_dispatch__:10033 | FakeTensorMode.__torch_dispatch__:11439
Dynamo produced 3 graphs covering 1412 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train eca_botnext26ts_256                 [2023-02-17 02:47:37,852] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:47:38,865] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.28673 backend_compile:0.00039
STATS: call_* op count: 580 | FakeTensor.__torch_dispatch__:3262 | FakeTensorMode.__torch_dispatch__:3847
Dynamo produced 3 graphs covering 580 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train eca_halonext26ts                    [2023-02-17 02:47:54,718] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:47:55,406] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.07495 backend_compile:0.00039
STATS: call_* op count: 610 | FakeTensor.__torch_dispatch__:3358 | FakeTensorMode.__torch_dispatch__:3958
Dynamo produced 3 graphs covering 610 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train ese_vovnet19b_dw                    [2023-02-17 02:48:10,769] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:48:11,352] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.26778 backend_compile:0.00029
STATS: call_* op count: 344 | FakeTensor.__torch_dispatch__:2842 | FakeTensorMode.__torch_dispatch__:2619
Dynamo produced 3 graphs covering 344 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train fbnetc_100                          [2023-02-17 02:48:26,509] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:48:26,729] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.03814 backend_compile:0.00059
STATS: call_* op count: 775 | FakeTensor.__torch_dispatch__:5922 | FakeTensorMode.__torch_dispatch__:5795
Dynamo produced 3 graphs covering 775 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train fbnetv3_b                           [2023-02-17 02:48:44,472] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:48:45,472] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:7.8288 backend_compile:0.00092
STATS: call_* op count: 1280 | FakeTensor.__torch_dispatch__:10352 | FakeTensorMode.__torch_dispatch__:9431
Dynamo produced 3 graphs covering 1280 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train gernet_l                            [2023-02-17 02:49:07,015] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:49:07,235] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.67478 backend_compile:0.00056
STATS: call_* op count: 753 | FakeTensor.__torch_dispatch__:5202 | FakeTensorMode.__torch_dispatch__:5136
Dynamo produced 3 graphs covering 753 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train ghostnet_100                        [2023-02-17 02:49:24,532] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:49:24,750] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.69216 backend_compile:0.0006
STATS: call_* op count: 870 | FakeTensor.__torch_dispatch__:8529 | FakeTensorMode.__torch_dispatch__:8424
Dynamo produced 3 graphs covering 870 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train gluon_inception_v3                  [2023-02-17 02:49:43,734] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:49:43,954] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.89967 backend_compile:0.00063
STATS: call_* op count: 883 | FakeTensor.__torch_dispatch__:8908 | FakeTensorMode.__torch_dispatch__:8603
Dynamo produced 3 graphs covering 883 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train gluon_xception65                    [2023-02-17 02:50:04,087] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:50:04,782] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:7.48325 backend_compile:0.00087
STATS: call_* op count: 1150 | FakeTensor.__torch_dispatch__:12480 | FakeTensorMode.__torch_dispatch__:11869
Dynamo produced 3 graphs covering 1150 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train gmixer_24_224                       [2023-02-17 02:50:25,893] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:50:26,114] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.87109 backend_compile:0.00074
STATS: call_* op count: 1221 | FakeTensor.__torch_dispatch__:9270 | FakeTensorMode.__torch_dispatch__:8607
Dynamo produced 3 graphs covering 1221 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train gmlp_s16_224                        [2023-02-17 02:50:44,631] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:50:44,857] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.79617 backend_compile:0.00074
STATS: call_* op count: 1101 | FakeTensor.__torch_dispatch__:9648 | FakeTensorMode.__torch_dispatch__:8109
Dynamo produced 3 graphs covering 1101 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train hrnet_w18                           [2023-02-17 02:51:06,419] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:51:07,111] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method NNModuleVariable() zero_grad [ConstantVariable(bool)] {} from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1126, in optimizer_zero_grad
    mod.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.86908 backend_compile:0.0004
STATS: call_* op count: 1371 | FakeTensor.__torch_dispatch__:4290 | FakeTensorMode.__torch_dispatch__:19678
Dynamo produced 2 graphs covering 1371 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train inception_v3                        [2023-02-17 02:51:28,712] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:51:28,937] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.86725 backend_compile:0.00062
STATS: call_* op count: 883 | FakeTensor.__torch_dispatch__:8908 | FakeTensorMode.__torch_dispatch__:8603
Dynamo produced 3 graphs covering 883 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train jx_nest_base                        [2023-02-17 02:51:49,532] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:51:49,761] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:5.56569 backend_compile:0.00076
STATS: call_* op count: 1470 | FakeTensor.__torch_dispatch__:9606 | FakeTensorMode.__torch_dispatch__:9204
Dynamo produced 3 graphs covering 1470 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train lcnet_050                           [2023-02-17 02:52:08,818] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:52:09,508] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.46361 backend_compile:0.00031
STATS: call_* op count: 352 | FakeTensor.__torch_dispatch__:2869 | FakeTensorMode.__torch_dispatch__:2719
Dynamo produced 3 graphs covering 352 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train levit_128                           [2023-02-17 02:52:24,704] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:52:25,280] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.4729 backend_compile:0.0006
STATS: call_* op count: 954 | FakeTensor.__torch_dispatch__:6535 | FakeTensorMode.__torch_dispatch__:8961
Dynamo produced 3 graphs covering 954 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train mixer_b16_224                       [2023-02-17 02:52:43,871] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:52:44,087] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.30041 backend_compile:0.0004
STATS: call_* op count: 525 | FakeTensor.__torch_dispatch__:4734 | FakeTensorMode.__torch_dispatch__:3927
Dynamo produced 3 graphs covering 525 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train mixnet_l                            [2023-02-17 02:53:00,282] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:53:01,304] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:7.16463 backend_compile:0.00085
STATS: call_* op count: 1285 | FakeTensor.__torch_dispatch__:9484 | FakeTensorMode.__torch_dispatch__:8469
Dynamo produced 3 graphs covering 1285 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train mnasnet_100                         [2023-02-17 02:53:21,305] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:53:21,522] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.21042 backend_compile:0.00045
STATS: call_* op count: 618 | FakeTensor.__torch_dispatch__:4752 | FakeTensorMode.__torch_dispatch__:4649
Dynamo produced 3 graphs covering 618 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train mobilenetv2_100                     [2023-02-17 02:53:38,012] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:53:38,234] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.23023 backend_compile:0.00047
STATS: call_* op count: 618 | FakeTensor.__torch_dispatch__:4787 | FakeTensorMode.__torch_dispatch__:4684
Dynamo produced 3 graphs covering 618 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train mobilenetv3_large_100               [2023-02-17 02:53:54,577] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:53:54,795] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.35685 backend_compile:0.00051
STATS: call_* op count: 661 | FakeTensor.__torch_dispatch__:5321 | FakeTensorMode.__torch_dispatch__:4895
Dynamo produced 3 graphs covering 661 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train mobilevit_s                         [2023-02-17 02:54:12,201] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:54:12,563] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.28461 backend_compile:0.00057
STATS: call_* op count: 947 | FakeTensor.__torch_dispatch__:6686 | FakeTensorMode.__torch_dispatch__:6719
Dynamo produced 3 graphs covering 947 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train nfnet_l0                            [2023-02-17 02:54:31,086] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:54:31,603] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.61702 backend_compile:0.00066
STATS: call_* op count: 990 | FakeTensor.__torch_dispatch__:6770 | FakeTensorMode.__torch_dispatch__:6778
Dynamo produced 3 graphs covering 990 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train pit_b_224                           [2023-02-17 02:54:49,511] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:54:49,729] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.87868 backend_compile:0.00045
STATS: call_* op count: 768 | FakeTensor.__torch_dispatch__:5422 | FakeTensorMode.__torch_dispatch__:5473
Dynamo produced 3 graphs covering 768 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train pnasnet5large                       [2023-02-17 02:55:10,102] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:55:11,007] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_method NNModuleVariable() zero_grad [ConstantVariable(bool)] {} from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1126, in optimizer_zero_grad
    mod.zero_grad(True)

PASS
TIMING: entire_frame_compile:5.33549 backend_compile:0.00028
STATS: call_* op count: 956 | FakeTensor.__torch_dispatch__:3439 | FakeTensorMode.__torch_dispatch__:13557
Dynamo produced 2 graphs covering 956 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train poolformer_m36                      [2023-02-17 02:55:30,535] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:55:31,219] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.75553 backend_compile:0.00094
STATS: call_* op count: 1441 | FakeTensor.__torch_dispatch__:11473 | FakeTensorMode.__torch_dispatch__:7750
Dynamo produced 3 graphs covering 1441 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train regnety_002                         [2023-02-17 02:55:51,103] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:55:51,322] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.49174 backend_compile:0.00056
STATS: call_* op count: 738 | FakeTensor.__torch_dispatch__:5670 | FakeTensorMode.__torch_dispatch__:5039
Dynamo produced 3 graphs covering 738 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train repvgg_a2                           [2023-02-17 02:56:08,618] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:56:08,834] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.55541 backend_compile:0.00055
STATS: call_* op count: 731 | FakeTensor.__torch_dispatch__:5018 | FakeTensorMode.__torch_dispatch__:5076
Dynamo produced 3 graphs covering 731 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train res2net101_26w_4s                   [2023-02-17 02:56:28,131] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:56:28,484] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:10.62947 backend_compile:0.00119
STATS: call_* op count: 1829 | FakeTensor.__torch_dispatch__:16052 | FakeTensorMode.__torch_dispatch__:16260
Dynamo produced 3 graphs covering 1829 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train res2net50_14w_8s                    [2023-02-17 02:56:54,561] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:56:54,784] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:8.94189 backend_compile:0.00106
STATS: call_* op count: 1599 | FakeTensor.__torch_dispatch__:14078 | FakeTensorMode.__torch_dispatch__:14263
Dynamo produced 3 graphs covering 1599 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train res2next50                          [2023-02-17 02:57:17,956] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:57:18,863] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:5.25494 backend_compile:0.00063
STATS: call_* op count: 911 | FakeTensor.__torch_dispatch__:8062 | FakeTensorMode.__torch_dispatch__:8151
Dynamo produced 3 graphs covering 911 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train resmlp_12_224                       [2023-02-17 02:57:36,342] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:57:36,702] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.37342 backend_compile:0.00044
STATS: call_* op count: 501 | FakeTensor.__torch_dispatch__:4549 | FakeTensorMode.__torch_dispatch__:3565
Dynamo produced 3 graphs covering 501 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train resnest101e                         [2023-02-17 02:57:54,129] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:57:55,183] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:11.68854 backend_compile:0.00117
STATS: call_* op count: 2056 | FakeTensor.__torch_dispatch__:16240 | FakeTensorMode.__torch_dispatch__:15965
Dynamo produced 3 graphs covering 2056 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train rexnet_100                          [2023-02-17 02:58:20,088] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:58:20,982] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:5.10409 backend_compile:0.00063
STATS: call_* op count: 856 | FakeTensor.__torch_dispatch__:6960 | FakeTensorMode.__torch_dispatch__:6569
Dynamo produced 3 graphs covering 856 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train sebotnet33ts_256                    [2023-02-17 02:58:38,896] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:58:39,255] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.30534 backend_compile:0.00048
STATS: call_* op count: 758 | FakeTensor.__torch_dispatch__:4564 | FakeTensorMode.__torch_dispatch__:5063
Dynamo produced 3 graphs covering 758 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train selecsls42b                         [2023-02-17 02:58:56,086] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:58:57,148] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.98104 backend_compile:0.00033
STATS: call_* op count: 384 | FakeTensor.__torch_dispatch__:3926 | FakeTensorMode.__torch_dispatch__:3789
Dynamo produced 3 graphs covering 384 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train spnasnet_100                        [2023-02-17 02:59:12,364] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:59:12,581] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:3.93648 backend_compile:0.00062
STATS: call_* op count: 762 | FakeTensor.__torch_dispatch__:5832 | FakeTensorMode.__torch_dispatch__:5705
Dynamo produced 3 graphs covering 762 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train swin_base_patch4_window7_224        [2023-02-17 02:59:32,116] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 02:59:32,780] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:7.43017 backend_compile:0.001
STATS: call_* op count: 2038 | FakeTensor.__torch_dispatch__:10334 | FakeTensorMode.__torch_dispatch__:12010
Dynamo produced 3 graphs covering 2038 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train swsl_resnext101_32x16d              [2023-02-17 02:59:59,617] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:00:00,651] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.5269 backend_compile:0.00074
STATS: call_* op count: 1041 | FakeTensor.__torch_dispatch__:9848 | FakeTensorMode.__torch_dispatch__:9503
Dynamo produced 3 graphs covering 1041 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train tf_efficientnet_b0                  [2023-02-17 03:00:19,767] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:00:19,984] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.11553 backend_compile:0.00058
STATS: call_* op count: 795 | FakeTensor.__torch_dispatch__:6586 | FakeTensorMode.__torch_dispatch__:5750
Dynamo produced 3 graphs covering 795 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train tf_mixnet_l                         [2023-02-17 03:00:38,172] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:00:38,819] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:7.0898 backend_compile:0.00084
STATS: call_* op count: 1300 | FakeTensor.__torch_dispatch__:9454 | FakeTensorMode.__torch_dispatch__:8483
Dynamo produced 3 graphs covering 1300 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train tinynet_a                           [2023-02-17 03:00:59,011] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:00:59,230] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.98109 backend_compile:0.00065
STATS: call_* op count: 937 | FakeTensor.__torch_dispatch__:7802 | FakeTensorMode.__torch_dispatch__:6793
Dynamo produced 3 graphs covering 937 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train tnt_s_patch16_224                   [2023-02-17 03:01:18,501] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:01:18,719] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:6.39494 backend_compile:0.00092
STATS: call_* op count: 1526 | FakeTensor.__torch_dispatch__:11104 | FakeTensorMode.__torch_dispatch__:11286
Dynamo produced 3 graphs covering 1526 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train twins_pcpvt_base                    [2023-02-17 03:01:40,338] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:01:41,006] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:10.31342 backend_compile:0.00119
STATS: call_* op count: 2060 | FakeTensor.__torch_dispatch__:16363 | FakeTensorMode.__torch_dispatch__:15634
Dynamo produced 3 graphs covering 2060 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train visformer_small                     [2023-02-17 03:02:05,310] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:02:05,526] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.27474 backend_compile:0.00035
STATS: call_* op count: 557 | FakeTensor.__torch_dispatch__:3828 | FakeTensorMode.__torch_dispatch__:4000
Dynamo produced 3 graphs covering 557 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train vit_base_patch16_224                [2023-02-17 03:02:23,058] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:02:23,282] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:2.64713 backend_compile:0.00045
STATS: call_* op count: 679 | FakeTensor.__torch_dispatch__:4793 | FakeTensorMode.__torch_dispatch__:4815
Dynamo produced 3 graphs covering 679 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train volo_d1_224                         [2023-02-17 03:02:40,088] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:02:40,302] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:4.28912 backend_compile:0.00065
STATS: call_* op count: 1118 | FakeTensor.__torch_dispatch__:7961 | FakeTensorMode.__torch_dispatch__:7877
Dynamo produced 3 graphs covering 1118 ops with 3 graph breaks (2 unique)
/data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /data/home/voz/miniconda/envs/torch4/lib/python3.10/site-packages/torchvision-0.15.0a0+93df9a5-py3.10-linux-x86_64.egg/torchvision/image.so: undefined symbol: _ZN3c104cuda29c10_cuda_check_implementationEPKcS2_ib
  warn(f"Failed to load image Python extension: {e}")
cuda train xcit_large_24_p8_224                WARNING:common:fp64 golden ref were not generated for xcit_large_24_p8_224. Setting accuracy check to cosine
[2023-02-17 03:03:04,112] torch._dynamo.symbolic_convert: [WARNING] Graph break: call_function clone_inputs in skip_files /scratch/voz/work/pytorch/torch/_dynamo/utils.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 326, in forward_and_backward_pass
    cloned_inputs = clone_inputs(inputs)

[2023-02-17 03:03:04,481] torch._dynamo.symbolic_convert: [WARNING] Graph break: inline in skipfiles: Optimizer.zero_grad  | _fn /scratch/voz/work/pytorch/torch/_dynamo/eval_frame.py from user code at   File "/scratch/voz/work/pytorch/benchmarks/dynamo/timm_models.py", line 327, in <graph break in forward_and_backward_pass>
    self.optimizer_zero_grad(mod)
  File "/scratch/voz/work/pytorch/benchmarks/dynamo/common.py", line 1124, in optimizer_zero_grad
    self.optimizer.zero_grad(True)

PASS
TIMING: entire_frame_compile:13.048 backend_compile:0.00153
STATS: call_* op count: 2449 | FakeTensor.__torch_dispatch__:19565 | FakeTensorMode.__torch_dispatch__:17845
Dynamo produced 3 graphs covering 2449 ops with 3 graph breaks (2 unique)
accuracy   pass_rate=90.20%
accuracy   pass_rate=93.48%
accuracy   pass_rate=100.00%
